[["index.html", "Introduction to Remote Computing Overview Introductory skills Intermediate skills Advanced skills Additional materials", " Introduction to Remote Computing C. Titus Brown, Saranya Canchi, Amanda Charbonneau, Marisa Lim, Abhijna Parigi, Pamela Reynolds, Nick Ulle, and Shannon Joslin. 2021-09-03 Overview Introductory skills Workshop 1: Introduction to the UNIX Command Line Workshop 2: Creating and modifying text files on remote computers Intermediate skills Workshop 3: Connecting to remote computers with ssh Workshop 4: Running programs on remote computers and retrieving the results Workshop 5: Installing software on remote computers with conda Workshop 6: Structuring your projects for current and future you Workshop 7: Automating your analyses and executing long-running analyses on remote computers Workshop 8: Keeping track of your files with version control Advanced skills Workshop 9: Automating your analyses with the snakemake workflow system Workshop 10: Executing large analyses on HPC clusters with slurm Workshop 11: Making use of on-demand “cloud” computers from Amazon Web Services Additional materials Please see Previous offerings if you are interested in videos. "],["introduction-to-the-unix-command-line.html", "1 Introduction to the UNIX Command Line 1.1 Introduction to UNIX 1.2 Navigation 1.3 Viewing &amp; Searching 1.4 File Manipulation 1.5 Some final notes", " 1 Introduction to the UNIX Command Line This two hour workshop will introduce attendees to the UNIX command line, which is the main way to interact with remote computers. We will cover computing concepts, file systems and directory structure, and some of the most important commands for working with remote computers. Today and tomorrow we’ll be using an interactive Web site running on a binder. To start your binder, please click on the “launch” button below; it will take up to a minute to start. NOTE: This lesson was adapted from Data Carpentry’s Introduction to the Command Line for Genomics lesson and the Lab for Data Intensive Biology’s Advanced Beginner/Intermediate Shell workshop, and is based on work done by Shannon Joslin. 1.1 Introduction to UNIX 1.1.1 Learning Goals visualize file/directory structures understand basic shell vocabulary gain exposure to the syntax of shell &amp; shell scripting look at the contents of a directory find features of commands with man commands: pwd, ls, cd, man 1.1.1.1 What is the shell and what is the terminal? The shell is a computer program that uses a command line interface (CLI) to give commands made by your keyboard to your operating system. Most people are used to interacting with a graphic user interface (GUI), where you can use a combination of your mouse and keyboard to carry out commands on your computer. We can use the shell through a terminal program. Everything we can do using our computer GUI, we can do in the shell. We can open programs, run analyses, create documents, delete files and create folders. We should note that folders are called directories at the command line. For all intents and purposes they can be used interchangeably but if you’d like more information please see “The folder metaphor” section of Wikipedia. The ease of getting things done via the shell will increase with your exposure to the program. Go ahead and open a new terminal window in binder by clicking on “Terminal”. When we open up terminal in binder we will see a a line of text. This is a prompt statement. It can tell us useful things such as the name of the directory we are currently in, our username, or what computer we are currently running terminal on. Let’s take a look around. First, we can use the print working directory command see what directory we are currently located in. pwd This gives us the absolute path to the directory where we are located. An absolute path shows the complete series of directories you need to locate either a directory or a file starting from the root directory of your computer. What is the root? A useful way to start thinking about directories and files is through levels. At the highest level of your computer, you have the root directory. Everything that is contained in your computer is located in directories below your root directory. We can also look at the contents of the directory by using the ls (“list”) command: ls This command prints out a list of files and directories that are located in our current working directory. We’ve preloaded some data into the binder, so we have a subdirectory data/ that we can look at. To change the working directory, we need to use the cd (“change directory”) command. Let’s move into the data directory. cd data Let’s have a look around. ls We can see the following files: MiSeq Slide1.jpg hello.sh nano1.png README.md gvng.jpg nano2.png However, this directory contains more than the eye can see! To show hidden files we can use the -a option. ls -a We will see the following: . MiSeq Slide1.jpg hello.sh nano1.png .. README.md gvng.jpg .hidden nano2.png Three new items pop up ., .. and .hidden. Using options with our commands allows us to do a lot! But how did we know to add -a after ls? Most commands offer a --help. Let’s look at the available options that ls has: ls --help Here we see a long list of options. Each option will allow us to do something different. CHALLENGE Try to find the option that allows you to differentiate between directories and executable files when using ls. Hint: look for the word classify. (You can also look at the ls man page if you prefer! We can also combine commands: ls -aFl This combination of options will list all the contents of the directory and differentiate between file types. 1.2 Navigation 1.2.1 Learning Goals paths look at the contents of files perform functions outside of the directory you are in intro to the wildcard expression: * copy, move and remove files create and remove directories understand the structure of commands commands: cat, cp, mv, rm, mkdir Now we have seen how to navigate around our computers and seeing what is located in the directory we are. But some of the beauty of the shell is that we can execute activities in locations that we are not currently in. To do this we can either use an absolute path or a relative path. A relative path is the path to another directory from the the one you are currently in. Navigate into the tmp1 directory located in the .hidden directory. cd .hidden/tmp1 Here we see two files notit.txt and thisinnotit.txt. We can see what is in the directories using the cat command which concatenates and prints the content of the file we list. cat thisinnotit.txt This is not the text file you&#39;re looking for NOTE - you can use TAB to do filename completion, so if you type cat this and then press your Tab key once, it will autocomplete if there is a unique match. If there is more than one match, the first Tab will do nothing, and the second will show all the possible matches. Let’s see what else is in the other tmp directories: ls ../tmp2 and we can see the contents of tmp3 ls ../tmp3 So, even though we are in the tmp1/ directory, we can see what is in other directories by using the relative path to the directory of interest. Note we can also use absolute paths too. You may have noticed the ../ this is how to get to the directory above the one you are currently located in. Note: in this case, we have access to the RStudio file browser, too, which is really nice. But in the future we won’t. So we can use the file browser today, but on Farm we’ll have to get by with just the command line interface and no other interface! CHALLENGE: Use the absolute path to list the files in the tmp2 directory. Wouldn’t it be nice to see the contents of all the tmp directories at once? We can use a regular expression to capture a sequence of characters (like the numbers 1, 2 and 3 at the end of the tmp directories). We can use the wild card character *, which expands to match any amount of characters. ls ../tmp* ../tmp1: notit.txt thisinnotit.txt ../tmp2: anotherfile.txt ../tmp3: closebutnotit.txt youfoundit.txt So, even though we are in the tmp1 directory we can use a relative path. We are quite used to moving, copying and deleting files using a GUI. All of these functions can be carried out at the command line with the following commands: Copy files with the cp command by specifying a file to copy and the location of the copied file. Here we will copy the thisinnotit.txt into the file thisisacopy.txt. cp thisinnotit.txt thisisacopy.txt The syntax for the copy command is cp &lt;source_file&gt; &lt;destination_file&gt;. Using this syntax we can copy files to other directories as well: cp thisinnotit.txt ../tmp2 If we navigate to the tmp2 directory and list the files that are in it we will see the thisinnotit.txt file has been copied to the tmp2 directory. cd ../tmp2 ls -l CHALLENGE: Use the mv command to move the thisinnotit.txt file from tmp2 to tmp3. Once we know how to copy and move files, we can also copy and move directories. We can create new directories with the command mkdir. Let’s make a new directory called tmp4 cd ../ mkdir tmp4 ls -l The shell is quite powerful and can create multiple directories at once. It can create multiple directories in the current working directory: mkdir tmp5 tmp6 ls -l or it can create a series of directories on top of one another: mkdir -p how/deep/does/the/rabbit/hole/go We can use tab complete to get to the go directory. Type cd h then hit tab. If you hit tab enough times your command will eventually read: cd how/deep/does/the/rabbit/hole/go/ You can see that we’ve created a bit of a monster directory structure… CHALLENGE: Navigate to the data directory and use the rm command to remove the how directory and all its contents. This nicely hints at the power of the shell - you can do certain things (in this case, create a nested hierarchy of directories) much more easily in the shell. But that power cuts both ways - you can also mess things up more easily in the shell! 1.3 Viewing &amp; Searching 1.3.1 Learning Goals looking inside files search for keywords within files commands: less, head, tail, grep A big part of data science is making sure what you expect in a particular file is what you have in that file. There are a few ways to look at the contents of a file. We’ve already seen how to print the entirety of a file to the stdout of our cat command. We can also look at files using the less command. Less is a safe way of looking at the contents of a file without the ability to change it. (We’ll talk more about text files and editing them in the second workshop!) Starting from the data/ directory in our home directory cd ~/data/ let’s look at some sequence data in a fastq file format. cd MiSeq less F3D0_S188_L001_R1_001.fastq We can see a bunch of sequence data! Use the up, down, left and right arrows to look through the folder a bit. Then press q to quit less. A lot of the time we want to know if a file contains what we expect. Many of the sequence files in this directory have the file ending .fastq. We expect these files to contain information in a particular format throughout the file with four lines of information for each sequence string. Looking through a million line file using less will take a long time. Rather than manually looking through the file we can print only a portion of the files contents to the terminal: head F3D0_S188_L001_R1_001.fastq @M00967:43:000000000-A3JHG:1:1101:18327:1699 1:N:0:188 NACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGTAGGCGGCCTGCCAAGTCAGCGGTAAAATTGCGGGGCTCAACCCCGTACAGCCGTTGAAACTGCCGGGCTCGAGTGGGCGAGAAGTATGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACCCCGATTGCGAAGGCAGCATACCGGCGCCCTACTGACGCTGAGGCACGAAAGTGCGGGGATCAAACAG + #&gt;&gt;AABABBFFFGGGGGGGGGGGGGGGGHHHHHHHGGGHHHHHGHGGGGGGGHGGGGGGHHHHHHHHHHGGGGGHHHHGHGGGGGGHHBGHGDGGGGGHHHGGGGHHHHHHHHGGGGGHG@DHHGHEGGGGGGBFGGEGGGGGGGG.DFEFFFFFFFDCFFFFFFFFFFFFFFFFFFFFFFFFFFDFDFFFEFFCFF?FDFFFFFFFFAFFFFFFFFFFFBDDFFFFFEFADFFFFFBAFFFA?EFFFBFF @M00967:43:000000000-A3JHG:1:1101:14069:1827 1:N:0:188 TACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGTAGGCGGCCTGCCAAGTCAGCGGTAAAATTGCGGGGCTCAACCCCGTACAGCCGTTGAAACTGCCGGGCTCGAGTGGGCGAGAAGTATGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACCCCGATTGCGAAGGCAGCATACCGGCGCCCTACTGACGCTGAGGCACGAAAGTGCGGGGATCAAACAG + 3AA?ABBDBFFBEGGEGGGGAFFGGGGGHHHCGGGGGGHFGHGGCFDEFGGGHGGGEGF1GGFGHHHHHGGEGGHHHHHFGGGGGGHHHHHGGGGCDDGHHGGGFHHHHHHHHCD@CCHGGGGHEHGGG@GFGGGGGGG@BGGGEGCEBFFFBFFB;9@EFFFEFFFFFFFFFFFFAFBBBFFFFFBBBFFFFBBBFFFFFFFFFFFBBBBBBBFFFFFFFFFDDFAFFFFF.AF9/FBBBBB.EAFFE?F @M00967:43:000000000-A3JHG:1:1101:18044:1900 1:N:0:188 TACGGAGGATGCGAGCGTTGTCCGGAATCACTGGGCGTAAAGGGCGCGTAGGCGGTTTAATAAGTCAGTGGTGAAAACTGAGGGCTCAACCCTCAGCCTGCCACTGATACTGTTAGACTTGAGTATGGAAGAGGAGAATGGAATTCCTAGTGTAGCGGTGAAATGCGTAGATATTAGGAGGAACACCAGTGGCGAAGGCGATTCTCTGGGCCAAGACTGACGCTGAGGCGCGAAAGCGTGGGGAGCAAACA head prints the first ten lines of a file out onto your screen. We can look at the last ten lines of a file using the tail command: tail F3D0_S188_L001_R1_001.fastq We can see that our fastq files look a lot different than the fasta files: head HMP_MOCK.v35.fasta &gt;A.baumannii.1 TGGGGAATATTGGACAATGGGGGGAACCCTGATCCAGCCATGCCGCGTGTGTGAAGAAGGCCTTATGGTTGTAAAGCACTTTAAGCGAGGAGGAGGCTACTTTAGTTAATACCTAGAGATAGTGGACGTTACTCGCAGAATAAGCACCGGCTAACTCTGTGCCAGCAGCCGCGGTAATACAGAGGGTGCGAGCGTTAATCGGATTTACTGGGCGTAAAGCGTGCGTAGGCGGCTTATTAAGTCGGATGTGAAATCCCCGAGCTTAACTTGGGAATTGCATTCGATACTGGTGAGCTAGAGTATGGGAGAGGATGGTAGAATTCCAGGTGTAGCGGTGAAATGCGTAGAGATCTGGAGGAATACCGATGGCGAAGGCAGCCATCTGGCCTAATACTGACGCTGAGGTACGAAAGCATGGGGAGCAAACAGGATTAGATACCCTGGTAGTCCATGCCGTAAACGATGTCTACTAGCCGTTGGGGCCTTTGAGGCTTTAGTGGCGCAGCTAACGCGATAAGTAGACCGCCTGGGGAGTACGGTC &gt;A.odontolyticus.1 TGGGGAATATTGCACAATGGGCGAAAGCCTGATGCAGCGACGCCGCGTGAGGGATGGAGGCCTTCGGGTTGTAAACCTCTTTCGCTCATGGTCAAGCCGCAACTCAAGGTTGTGGTGAGGGTAGTGGGTAAAGAAGCGCCGGCTAACTACGTGCCAGCAGCCGCGGTAATACGTAGGGCGCGAGCGTTGTCCGGAATTATTGGGCGTAAAGGGCTTGTAGGCGGTTGGTCGCGTCTGCCGTGAAATCCTCTGGCTTAACTGGGGGCGTGCGGTGGGTACGGGCTGACTTGAGTGCGGTAGGGGAGACTGGAACTCCTGGTGTAGCGGTGGAATGCGCAGATATCAGGAAGAACACCGGTGGCGAAGGCGGGTCTCTGGGCCGTTACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGGATTAGATACCCTGGTAGTCCACGCTGTAAACGTTGGGCACTAGGTGTGGGGGCCACCCGTGGTTTCTGCGCCGTAGCTAACGCTTTAAGTGCCCCGCCTGGGGAGTACGGCC &gt;B.cereus.1 TAGGGAATCTTCCGCAATGGACGAAAGTCTGACGGAGCAACGCCGCGTGAGTGATGAAGGCTTTCGGGTCGTAAAACTCTGTTGTTAGGGAAGAACAAGTGCTAGTTGAATAAGCTGGCACCTTGACGGTACCTAACCAGAAAGCCACGGCTAACTACGTGCCAGCAGCCGCGGTAATACGTAGGTGGCAAGCGTTATCCGGAATTATTGGGCGTAAAGCGCGCGCAGGTGGTTTCTTAAGTCTGATGTGAAAGCCCACGGCTCAACCGTGGAGGGTCATTGGAAACTGGGAGACTTGAGTGCAGAAGAGGAAAGTGGAATTCCATGTGTAGCGGTGAAATGCGTAGAGATATGGAGGAACACCAGTGGCGAAGGCGACTTTCTGGTCTGTAACTGACACTGAGGCGCGAAAGCGTGGGGAGCAAACAGGATTAGATACCCTGGTAGTCCACGCCGTAAACGATGAGTGCTAAGTGTTAGAGGGTTTCCGCCCTTTAGTGCTGAAGTTAACGCATTAAGCACTCCGCCTGGGGAGTACGGCC &gt;B.vulgatus.1 TGAGGAATATTGGTCAATGGGCGCAGGCCTGAACCAGCCAAGTAGCGTGAAGGATGACTGCCCTATGGGTTGTAAACTTCTTTTATAAAGGAATAAAGTCGGGTATGGATACCCGTTTGCATGTACTTTATGAATAAGGATCGGCTAACTCCGTGCCAGCAGCCGCGGTAATACGGAGGATCCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGAGCGTAGATGGATGTTTAAGTCAGTTGTGAAAGTTTGCGGCTCAACCGTAAAATTGCAGTTGATACTGGATATCTTGAGTGCAGTTGAGGCAGGCGGAATTCGTGGTGTAGCGGTGAAATGCTTAGATATCACGAAGAACTCCGATTGCGAAGGCAGCCTGCTAAGCTGCAACTGACATTGAGGCTCGAAAGTGTGGGTATCAAACAGGATTAGATACCCTGGTAGTCCACACGGTAAACGATGAATACTCGCTGTTTGCGATATACGGCAAGCGGCCAAGCGAAAGCGTTAAGTATTCCACCTGGGGAGTACGCCG &gt;B.vulgatus.2 TGAGGAATATTGGTCAATGGGCGAGAGCCTGAACCAGCCAAGTAGCGTGAAGGATGACTGCCCTATGGGTTGTAAACTTCTTTTATAAAGGAATAAAGTCGGGTATGGATACCCGTTTGCATGTACTTTATGAATAAGGATCGGCTAACTCCGTGCCAGCAGCCGCGGTAATACGGAGGATCCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGAGCGTAGATGGATGTTTAAGTCAGTTGTGAAAGTTTGCGGCTCAACCGTAAAATTGCAGTTGATACTGGATATCTTGAGTGCAGTTGAGGCAGGCGGAATTCGTGGTGTAGCGGTGAAATGCTTAGATATCACGAAGAACTCCGATTGCGAAGGCAGCCTGCTAAGCTGCAACTGACATTGAGGCTCGAAAGTGTGGGTATCAAACAGGATTAGATACCCTGGTAGTCCACACGGTAAACGATGAATACTCGCTGTTTGCGATATACGGCAAGCGGCCAAGCGAAAGCGTTAAGTATTCCACCTGGGGAGTACGCCG Each sequence entry for a fasta formatted file contains only two lines of information for each sequence string. Another useful thing to do is to be able to search the contents of files for a particular string of characters you would like to find. Let’s say you’d like to find the sequence CATTAG in your files. We can use the file pattern searcher grep to look for our favorite sequence: grep CATTAG F3D0_S188_L001_R2_001.fastq We can also use the wildcard regular expression to search CATTAG in all of the fastq files located in our current working directory: grep CATTAG *.fastq CHALLENGE: What line does CATTAG occur on in F3D141_S207_L001_R1_001.fastq? (HINT: Use grep --help to search for grep options related to line number) 1.4 File Manipulation 1.4.1 Learning Goals commands for, basename, echo 1.4.2 Renaming a bunch of files Let’s make sure we’re in the right directory- the one that contains all of our data files. cd ~/data/MiSeq For our first task, let’s pretend that we want to rename all of the fastq files to be .fq files instead (this is a surprisingly useful specific skill, even if you can’t immediately think of why you would want to do that!). Here, we get to use two of my favorite commands - ‘for’ and ‘basename’. for lets you do something to every file in a list. To see it in action: for i in *.fastq do echo $i done This is running the command echo for every value of the variable ‘i’, which is set (one by one) to all the values in the expression *.fastq. If we want to get rid of the extension ‘.fastq’, we can use the basename command: for i in *.fastq do basename $i .fastq done Now, this doesn’t actually rename the files - it just prints out the name, with the suffix ‘.fastq’ removed. To rename the files, we need to capture the new name in a variable: for i in *.fastq do newname=$(basename $i .fastq).fq echo $newname done What $( ... ) does is run the command in the middle, and then replace the $( ) with the output of running the command. Now we have the old name ($i) and the new name ($newname) and we’re ready to write the rename command – for i in *.fastq do newname=$(basename $i .fastq).fq echo mv $i $newname done Question: why did I put echo here? Now that we’re pretty sure it all looks good, let’s run it for realz: for i in *.fastq do newname=$(basename $i .fastq).fq mv $i $newname done and voila, we have renamed all the files! Side note: you may see backquotes used instead of $(...). It does the same thing but is trickier to get right, so we teach $(...) instead of `. 1.5 Some final notes This lesson focused on file and directory exploration because that’s something everyone needs to know, and all these commands will work on pretty much any computer that is running a UNIX compatible shell (including Mac OS X and Windows Subsystem for Linux). We’ll get into a broader range of tasks soon, promise! The binder and this documentation page will stay working for the foreseeable future, so please feel free to come back and revisit some of these commands! We will explore more UNIX commands over the next few workshops! Google (and especially stackoverflow) is your friend! Use Internet search whenever you have questions about what a command does, or what commands to use to achieve a particular tasks. "],["creating-and-modifying-text-files-on-remote-computers.html", "2 Creating and modifying text files on remote computers 2.1 Text files vs other files 2.2 Big Powerful Editors 2.3 Remote vs local, and why editors? 2.4 Editors that run locally on your laptop/desktop 2.5 Thinking about editors as a means to an end 2.6 Other ways to create, edit, filter, and modify files 2.7 Working with CSV files 2.8 A quick primer on compression. 2.9 Concluding thoughts", " 2 Creating and modifying text files on remote computers This two hour workshop will introduce attendees to the concepts and skills needed to create, modify, and search text files on remote computers. We will discuss files and content types, and cover the most common ways to work with remote text files. As with the first workshop introducing the UNIX command line, we’ll be using an interactive Web site running on a binder. To start your binder, please click on the “launch” button below; it will take up to a minute to start. Once it’s launched, go to the “Terminal” tab. 2.1 Text files vs other files Text files are a fairly narrow but very important subset of the kinds of files that we will work with in data science. Text files are, loosely defined, files that are human-readable without any special machine interpretation needed - such as text-only e-mails, CSV files, configuration files, and Python and R scripts. The list above is interesting, because it makes the point that just because a human can “read” the files doesn’t mean that they are intended for humans, necessarily. For example, CSV files can be more or less strictly defined in terms of formatting, and Python and R scripts still need to be valid Python or R code. DNA sequence data files like we saw yesterday are another case in point - it’s pretty rare (and a bad idea) to edit them manually, but you could if you really wanted to. The operational distinction really comes down to this: text files can be created, edited, changed, and otherwise manipulated with text-format based tools, like text editors, grep (which we saw yesterday), and other programs. Text files are a common and standard format that many tools can interact with. In comparison, binary files are files that need special programs to interact with them. Some of them are more standard than others - for example, Word files can be read or written by many programs. Images (JPG and PNG and…) can be manipulated by many programs as well. Zip files are another semi-standard format that can be manipulated by several different programs. The main thing is that you can’t just look at them with standard text-focused tools - and typically this is because binary files are meant to be used for different kinds of data than text. As a side note, one of the most important aspects of text files is that there are some really powerful tools for tracking changes to them, and collaboratively editing them - we’ll cover that in workshop 8, version control! 2.1.1 OK, OK, what does this all mean in practice? Let’s look at a simple text file - 2cities/README.md: cat 2cities/README.md As you may remember, ‘cat’ (short for ‘catenate’) displays the content of the file. This is a file in a format called Markdown, that is a lightly decorated text file with a title and so on. While it can be nicely formatted by an interpreting program (see the way github renders this file!, it can also just be viewed and read with cat. This is different from the other file in 2cities/; take a look at what’s there by running, ls 2cities/ and you should see README.md book.txt.gz In this directory, there is one text file and one binary file. If you want to see if it’s a file type that UNIX recognizes you can run the file command, e.g. file 2cities/README.md will report that it’s ASCII text, while file 2cities/book.txt.gz will report that it’s “gzip compressed data”, which is a compressed data type. What do we do with that? 2.1.2 Working with gzipped files gzip is a common type of file, and all that it means is that it’s been compressed (made smaller) with the gzip program. Look at it’s file size first – ls -lh 2cities/book.txt.gz and you’ll see that it’s about 300k. You can uncompress a gzip file with gunzip; in this case, gunzip 2cities/book.txt.gz will produce 2cities/book.txt CHALLENGE: what two commands will tell you the file type and size of 2cities/book.txt? Yep, it’s almost 3 times bigger when it’s uncompressed! And it’s file type is “UTF-8 Unicode (with BOM) text, with CRLF line terminators” which is a fancy way of saying “text, supporting extended characters (unicode), and with both a carriage return (CR) and a line feed (LF) at the end of each line.” The important thing is that pretty much any text editor should be able to edit this kind of file. Let’s take a quick look at the beginning of the file with head: head 2cities/book.txt yep, looks like text! 2.1.3 Digression: file extensions are often meaningful (but don’t have to be) Couldn’t we have guessed at what these files were based on their names? Yes, the .md extension usually means it’s a text file with Markdown formatting, and the .gz extension typically means it’s a compressed file, and the .txt extension typically means it’s a text file. So you can read book.txt.gz to mean that it’s a text file that’s been compressed. But this isn’t guaranteed - it’s a convention, rather than a requirement. Many programs will actively “sniff” the file type by looking at the content (which is what file does), and you should never blindly trust the file type indicated by the extension. 2.1.4 Let’s edit this file! Let’s start with the nano editor. nano and its sibling pico are simple text editors that let you get started, but are ultimately limited in their functionality. Note: If you’ve ever used the ‘pine’ e-mailer, you’ve used these editors! nano (and all of the editors we’ll use below in the terminal) are “text graphics” editors that give you a visual interface that is not the command line (which is good, trust us) but that also exist only within the terminal program and do not support mouse movements. This is important - you can’t use the mouse to move the cursor or make changes (although you can select things). 2.1.5 Running the editor and exiting/saving To get started, let’s open the file: nano 2cities/book.txt this will put you in an editor window. First things first: you can immediately exit by typing CTRL-X (that’s holding down the CTRL key, and then typing X, lowercase or uppercase - no shift key is needed. If you haven’t changed anything, it will simply exit. Now edit the file with nano again (use the up arrow on the command line to find and rerun the previous command!) – nano 2cities/book.txt Now change something - just type. You should see the new characters added. Use CTRL-X again, and it will ask “Save modified buffer?” If you say “No”, it will not save; if you type ‘y’, it will ask you for the name of the file. Just hit ENTER to overwrite the file you edited here. Now, you should be back at the command line. Run: head 2cities/book.txt and you should see your changes! Note, there’s no ‘undo’ possible once you’ve saved. 2.1.6 Navigating in nano Let’s go back into nano and learn how to move around. Run: nano 2cities/book.txt and use the arrow keys to move up and down and left and right. For big files, this can be tedious! If you look down on the bottom, you can see a bunch of help text telling you what control+keys to use - use CTRL-V to page down, and CTRL-Y to page back up. 2.1.7 Long lines - note! One of my least favorite features of nano is the way it handles long lines (lines that extend off the right of the screen). Try making one - go to the end of a line, and add a bunch of text. What it does is shift the whole line left while you’re typing, and then when you scroll back over to the left, it puts a $ at the last column on the screen to tell you that it’s a long line. Very confusing. But there you are. 2.1.8 Slightly more advanced features ^K will delete the current line, and ^U will put the last deleted line into the current location. (It’s a slightly janky version of cut and paste that many editors use in UNIX, for some reason.) 2.1.9 Getting help! In nano, CTRL-G will put you in “help” mode, and you can now navigate around (CTRL-V and CTRL-Y to read), and then CTRL-X to exit. Note again that ^ in front of a key means control, so e.g. ^K means “type CTRL+K” (which will delete the current line). Note also that M- means “hit Escape and then the key after”, so pressing the “escape” key, letting go of it, and then hitting “g” will go to a line and column number. Try it out - type Escape, then g, then type 500,10 and hit enter. Why do CTRL and Escape work differently? You hold down CTRL and another key, but you press Escape and then type something. Why!? The answer is that CTRL and ALT are “modifier keys”, like SHIFT - they modify the character sent when you hold them down. Escape is its own character, however, so you’re first saying “here’s an escape character!” and then “here’s another character!” (We don’t make the rules, we just explain them - sorry!) 2.1.10 Challenges: Use the help screen to answer (and experiment with) the following challenges - remember, CTRL-G gets you into help, CTRL-V pages down, and CTRL-X exits help. CHALLENGE: How do you delete the character at the current cursor position? CHALLENGE 2: How do you move to the end of the file? You can do a lot in these but as soon as you’re dealing with really large files, or many files, we suggest other editors. That having been said, we teach nano because it’s a good “basic” editor that is usually available and can almost always be used if you don’t have access to your favorite editor. 2.2 Big Powerful Editors There are two fairly dominant editors that work at the command line. They’ve been around for decades, and they have many advocates who care for them with a near-religious fervor. We will demo them for you, and point you at learning resources for them, and then leave it up to you to pick one. (We’ll probably use nano for most of the work we do in these workshops.) 2.2.1 Big Powerful Editor 1: vi ‘vi’ stands for “visual editor” and it’s available on most systems. It’s incredibly powerful, and incredibly robust, and is correspondingly cryptic and hard to use. It involves a lot of remembering specific character commands, in particular. (We’re actually using ‘vim’, but never mind that - it’s compatible with vi. Read more here.) To run vi, type: vi 2cities/book.txt (You should see all your changes from before, right?) vi starts in “normal mode”, which allows you to navigate around the file. In normal mode, what you type does not change the file - instead, it lets you issue commands to vi. The first and most important (?) command - to exit, type: :q and if there are no changes, it will simply exit. Run vi again, and let’s edit – vi 2cities/book.txt and then type ‘i’ to go to “insert” mode, and type something. Then hit the escape key to go back to “normal” mode. Now try to exit with :q. It won’t work! You have to either save, or quit. To force-quit without saving, run :q!. Now let’s learn to save! Go back and edit (i, then type something, then escape). To save, type :wq. (You can also (mystifyingly) type ZZ to do the same thing. shrug.) The main thing that vi does is give you a “normal” mode (where you can navigate around - use CTRL-F and CTRL-V to page down and up, for example) and an edit mode (use ‘i’ for insert or ‘a’ for append) where what you type goes directly into the file. You use Escape to get out of edit mode. In normal mode, ‘x’ will delete the character you’re on, and ‘dd’ will delete the line you’re on (and put it in the cut buffer), and P will pull the line out of the cut buffer into the file at the current location. And that’s what you really need to know :). A few tips for normal mode - to get help, type :help to go to a specific line, type the line number followed by G. 500G As a side note, we’ve just taught you the single most asked question on the Internet about UNIX: how to exit vi!! 2.2.2 Big Powerful Editor 2: emacs The other editor to know about is emacs. (This is what Titus uses the most.) To run emacs, emacs 2cities/book.txt This is automatically in edit mode (there’s no normal mode) so it behaves kind of like nano. To exit emacs, type CTRL-X and then CTRL-C. If you’ve modified things (by typing something), it will ask you if you want to save. To page down, type CTRL-V. To page up, type Escape V. To go to the beginning of a line, type CTRL-A. End of line, CTRL-E. (I’m telling you these specific keys because they also work at the command line.) There’s a pretty nice interactive tutorial for via that you can access with CTRL-H t (CTRL-H, followed by a ‘t’). Emacs shines when editing multiple files, but it can do a lot more, too. Some people spend their entire computing lives in emacs… see, for example, org-mode. 2.2.3 An opinion You only need to learn nano, and be basically familiar with vi and emacs. Read on for why! 2.3 Remote vs local, and why editors? So we’ve just shown you a bunch of editors that work on the command line/in the Terminal window, but don’t support mouse and copy paste and multiple windows and other nice things. Why can’t you just always use a nice editor that supports mouse commands etc etc?? Well. There are a few reasons! First is that it’s always nice to have backup options. Even if you resolutely stick with something that runs on your laptop, every now and then you may find yourself in a situation where you’re using someone else’s computer to debug or demonstrate something. Second is that these are platform independent options, in some sense - if you are connected to a UNIX system, you can pretty much always use nano or vi or emacs, no matter how you are connecting or from what type of computer. Third, sometimes it’s just faster to fix something locally in the shell. And it’s nice to have the option. Fourth, and related, is that remote file editing from your laptop or desktop requires that certain things be available on the remote computer - ssh and authentication (see next two workshops, Workshops 3 and 4!). Unfortunately, these aren’t always available - for example, we can’t actually use the nice editors on this binder, for technical reasons; we’d have to use the RStudio editor (which is also nice, but is also not always available). Last and probably least, if you’re in the Matrix and you’re Trinity and you’re trying to hack through the machine firewall after breaking into a heavily guarded compound, you’re unlikely to want to take the time to install an editor on a laptop you bring with you. Better to be able to use what’s already on the system, eh? (Yes, this is a Matrix reference.) 2.4 Editors that run locally on your laptop/desktop That all having been said, there is no reason you can’t use nice friendly editors most of the time! I asked on twitter about what editors people liked, and several popped up - Visual Studio Code was a hands-down winner. It works on Windows, Mac OS X, and Linux, and is free. BBEdit was beloved by many. Runs on Mac OS X. Free, with pay option. NotePad++ is a nice free Windows editor that I’ve used in the distant past. Some people really liked Atom too, which is free and runs on Windows, Mac OS X, and Linux. Any or all of these will work for editing remote files, support a wide variety of languages nicely, and otherwise are excellent choices. Pick one! Thrive! (We can’t use these yet because we need to configure remote access in a particular way - that will come next week :).) 2.5 Thinking about editors as a means to an end At the end of the day, whatever editor you choose needs to be one that lets you achieve your end goal - which is to quickly and reliably edit text files. I personally switch between vi and emacs on a regular basis. Emacs is where I do long-form writing and editing (because I’ve got mine configured nicely for that), while vi is what I use for quick edits (because it’s fast to start, and I don’t need to configure it at all for it to be useful - so I can use it more places). Again, most people will probably end up using something like VScode, which got many rave reviews online and supports robust syntax highlighting and many different languages, as well as remote editing. But it really doesn’t matter. I think of an editor like a kitchen - you may customize your kitchen layout and tools differently from someone else, but at the end of the day, your goal is to cook something, and you (in this analogy) only really need to worry about another editor if you’re using an unfamiliar system, just like if you’re cooking in a strange kitchen. And then it will be maddening and infuriating but that’s ok :). 2.6 Other ways to create, edit, filter, and modify files So editing is pretty cool, but if you’re in a hurry, or want to make a small change without switching windows, or need to work with some pretty big files, there are other approaches you can use. Read on! 2.6.1 Redirection, appending, and piping. By default, many UNIX commands like cat send output to something called standard out, or “stdout”. This is a catch-all phrase for “the basic place we send regular output.” (There’s also standard error, or “stderr”, which is where errors are printed; and standard input, or “stdin”, which is where input comes from.) Much of the power of the UNIX command line comes from working with stdout output, and if you work with UNIX a lot, you’ll see characters like &gt; (redirect), &gt;&gt; (append) and | (pipe) thrown around. These are redirection commands that say, respectively, “send stdout to a new file”, “append stdout to an existing file”, and “send stdout from one program to another program’s stdin.” Let’s start by going to our home directory: cd ~/ 2.6.2 The simplest possible “editor” - echo You can create a file with echo and redirection, like so: echo this is some content &gt; file.txt which will put the words this is some content in the file named file.txt. CHALLENGE: how do you view the contents of file.txt? If you then run echo this is other content &gt; file.txt it will overwrite file.txt. (Note: if you don’t like this overwriting behavior, you can run set -o noclobber so that bash will complain.) Instead of overwriting, you can append by specifying &gt;&gt;, like so – echo more content &gt;&gt; file.txt This doesn’t just work with echo - you can do this with many UNIX commands, e.g. cat file.txt file.txt &gt; newfile.txt will create newfile.txt with two copies of file.txt in it, and you can add a third with cat file.txt &gt;&gt; newfile.txt You can also e.g. search for words with grep and then save the results – for example, this will search for the word “worst” in the Tale of Two Cities, and save the results to worst-lines.txt. grep worst 2cities/book.txt &gt; worst-lines.txt 2.6.3 Piping and filtering What if you wanted to count the number of lines in which the word worst shows up in the Tale of Two Cities? You could use the “wc” (wordcount) program - grep worst 2cities/book.txt &gt; worst-lines.txt wc -l worst-lines.txt (the answer is 18 :) but this creates an unnecessary intermediate file, worst-lines.txt. You can avoid creating this file by using piping: grep worst 2cities/book.txt | wc -l which says “send the output of grep to the input of wordcount”. You’ll see this a lot in UNIX, and below we’ll explore this on a new file type - CSV files! 2.7 Working with CSV files CSV files - Comma Separated Value files - are another very common type of text files, especially in data science. Let’s explore working with them! We’ve put a list of South Park TV show quotes under SouthParkData/All-seasons.csv.gz. Let’s change into that directory to work with the CSV file. cd ~/SouthParkData Let’s now uncompress the file – remember, you can use Tab completion here by typing gunzip A&lt;TAB&gt; – gunzip All-seasons.csv.gz and look at the first few rows of the result All-seasons.csv file – head All-seasons.csv It looks like there are four columns, and the quotes are multi-line quotes. (I’ve never seen this before, but it seems to work!) Suppose you want to see if the word ‘computer’ is in there anywhere. You can use grep to do that – grep computer All-seasons.csv – and you get a lot of results! First, let’s count them: grep computer All-seasons.csv | wc -l – this will count the number of lines the word ‘computer’ shows up on. (It’s 78.) If you browse through the file, you might release that ‘Computer’ is a character on the show, but it turns out that grep is really literal and doesn’t match ‘Computer’ when you search for ‘computer’ - you need to provide ‘grep’ with ‘-i’ to do case-insensitive search! Let’s try that – grep -i computer All-seasons.csv | wc -l – and now it’s 101. How would we get at just the lines spoken by the computer? Well, if you look at the header of the file, head All-seasons.csv you’ll see that the third column is the one with the character in it. You can use the cut command to pick out just the third column by specifying comma as a separator with -d and -f3 as the field number – cut -d, -f3 All-seasons.csv | grep Computer which will give you a manageable number of results - about 16. You might note that there is an inconsistency in the way the character is named - Computer vs Computer Voice (maybe these are different characters? I don’t watch enough South Park to know…) Let’s do some counting – cut -d, -f3 All-seasons.csv | grep Computer | sort | uniq -c This is hard to pull apart but let’s do so - first, cut out column 3 then, search for Computer then, sort them alphabetically then, count the number of times each character shows up There are lots of ways this can come in handy for digging into csv files and figuring out where values are wonky. 2.7.1 Use csvtk when working with CSV files, maybe. This section was mostly to show you other ways of interacting with generic text files with CSV as an example, but if you work a lot with CSV or TSV files, I wanted to suggest looking into the csvtk program – we’ll show you how to install it with conda in workshop 5, but we pre-installed it for you on this binder. With csvtk, you can run commands that make use of column headers - for example, csvtk cut -f Character All-seasons.csv | grep Computer | sort | uniq -c gives you the same output, but it uses the header name. csvtk is a really nice piece of software that I am starting to use heavily. Highly recommended when doing a lot of CSV/TSV work - definitely check out the manual. 2.8 A quick primer on compression. Make sure you’re in the SouthParkData directory and have uncompressed All-seasons.csv – cd ~/SouthParkData gunzip All-seasons.csv (it’s ok if you’ve already run these and they fail, just want to make sure!) Text files can be large, so often they are distributed in compressed version. 2.8.1 Gzip and .gz files. gzip is a common compression format that works with .gz files. It works with one file at a time, so gzip compresses that one file and makes a new .gz file. To compress All-seasons.csv with gzip, you can use: gzip All-seasons.csv If you try to run it again, you’ll get an error message; try it! gzip All-seasons.csv …because the file no longer exists - it’s been compressed into a new file, All-seasons.csv.gz! If you run gunzip, it will uncompress the file and delete the old one. Sometimes this isn’t what you want – you can use output redirection to uncompress it and make a new copy: gunzip -c All-seasons.csv.gz &gt; All-seasons.csv but, then, if you try to run gzip All-seasons.csv it will tell you that the .gz file already exists. Say ‘n’ or use CTRL-C to exit. 2.8.2 zip and compressing multiple files. The big downside to gzip is that it works one file at a time. What if you wanted to bundle up multiple files AND compress them? Our recommended approach is to use zip to build a zip bundle or archive. This will both compress files, and store multiple files (even a directory hierarchy!) First, create the archive - cd ~/ zip -r south-park.zip SouthParkData/ (the -r is needed on some versions of zip to package up directories.) Then make a copy in a new place (just to demonstrate that it all works :) - mkdir new-place/ cd new-place/ unzip ../south-park.zip ls -R and you will see a complete new copy under ~/new-place/south-park/SouthParkData. This is handy for making quick backup copies of things and downloading them (see Workshop 4!) as well as sending people collections of files. We’ll show you a different way, using version control, in Workshop 8. Note, you can use unzip -v to see what’s in a zip archive, unzip -v ../south-park.zip and selectively unzip specific files by specifying them on the command line like so: unzip ../south-park.zip SouthParkData/README.md 2.9 Concluding thoughts What we’ve shown you is a whole plethora of hopefully not-too-confusing options for editing and working with text files. In terms of editing, the only thing you really need to do is (1) bookmark this page when you need to figure out how to exit vi, and (2) remember to use nano! The redirection and compression stuff is really useful, but again, you just need to know it exists and that there’s this tutorial on it. Taking a step back, these first two workshops have been about introductory skills that you will use every day when you use a UNIX computer. These skills may seem confusing, but they will become second nature if you use them regularly. And we’ll be doing that through the next 9 workshops! "],["connecting-to-remote-computers-with-ssh.html", "3 Connecting to remote computers with ssh 3.1 SSH and Clients 3.2 Mac OS X: Using the Terminal program 3.3 Windows: Connecting to remote computers with MobaXterm 3.4 Logging out and logging back in. 3.5 You’re logged on to a remote computer. Now what? 3.6 Copying files to and from your local computer. 3.7 Some commands are available! Others are not. 3.8 Summing up file transfer - a challenge! 3.9 Summing things up", " 3 Connecting to remote computers with ssh This two hour workshop will show attendees how to connect to remote computers using ssh software, which is the most common way to do so. We will discuss usernames and passwords, introduce ssh software clients, and work through the most common challenges attendees will face in connecting to remote computers. 3.1 SSH and Clients We’re going to be using SSH, the Secure Shell protocol, to connect to a remote computer - in this case, the ‘farm’ computer at UC Davis. We’ll use it for the next 7 workshops, and then in workshop 11, we’ll use ssh to connect to a computer that we rent from Amazon instead. ssh is a standard way to connect to remote computers, both to run commands and to retrieve files. It uses an encrypted connection so nothing you type can be seen by anyone else, which protects your passwords as well as any other data you send. 3.1.1 Some security thoughts A few points on the security front - you still need to use a secure authentication method (password, or private key - we’ll show you that in workshop 4!) people can still look over your shoulder, and if they have access to your computer they can do things like install keyloggers. ssh doesn’t hide the fact that you’re communicating with a particular remote computer, which is sometimes what snoopers care about (e.g. nation-states) But, by and large, security for researchers is not about stealing your data, it’s about breaking into the remote computers for other reasons. ssh is a pretty good protection against network eavesdroppers and so on. 3.1.2 ssh as a protocol - many clients! ssh operates as a network “protocol”, which means that the sender (your local computer, in this case) and the receiver (the farm computer) can be running any software that “speaks” ssh, and they can communicate just fine. In particular, this means you can use many different software packages that speak ssh - known as ssh “clients” - and we link to some below. For our lessons, we’re going to use two specific ssh clients, one for Mac OS X that’s just called “ssh”, and one for Windows that’s called MobaXterm. We’ve chosen these because ssh comes with Mac OS X, so we don’t need to install it, and we have a lot of experience with MobaXterm. Unfortunately they’re (mostly) quite different in appearance, so we’re going to run through them separately the first time through. There are many alternatives - for example, for Mac OS X there are many different free SSH clients, and here is a list of 10 ssh clients for Windows. They will all look and feel somewhat different, but they will all get you the same place! Windows users: While we’re working through the Mac OS X ssh connection, please go to the Windows instructions and start downloading MobaXterm - thanks!. 3.2 Mac OS X: Using the Terminal program Find and open the Terminal program using Spotlight - it’s under Applications. It will look and feel a lot like the things you saw in Workshops 1 and 2 :). (Congratulations! You have unlocked a secret of Mac OS X - it’s got a command line underneath, because it’s actually a UNIX operating system!) Now type ssh datalab-XX@farm.cse.ucdavis.edu where you replace XX with your user number (between 10 and 60). (You should received an e-mail from “Titus Brown” with the subject “Farm account name (remote computing 2021 workshop series)”. Ask a TA for help if you can’t find this e-mail.) You will be confronted with a “password:” prompt. Copy and paste in the password from your farm account e-mail. Ask a TA for help if you need it! Note that the password does not display, so it will look like nothing is being entered when you paste.) And voila, you are now logged into farm! You should be at a prompt that looks like this: datalab-09@farm:~$ 3.3 Windows: Connecting to remote computers with MobaXterm Steps: Go to the MobaXterm download page. Select “Home Edition”. Select “Portable edition”. It’s 25 MB and will take about a minute to download. Find the downloaded Zip file in your Downloads folder (should be named “MobaXterm_Personal_21.2”), and double click on it. In the MobaXterm_Personal_21.2 folder, run the MobaXterm 21.2 Application. Note: You may need to “allow access on all networks for this application” if Windows asks. Click on Session… (upper left). In the new window, click on SSH (upper left). Under “Basic SSH settings”, set “Remote host” to “farm.cse.ucdavis.edu”. This is the computer name you are connecting to. Click on “specify username”, and enter the username you received in the e-mail from “Titus Brown” with the subject “Farm account name (remote computing 2021 workshop series)”. (Ask a TA for help if you can’t find this e-mail.) Then select OK. It will now open up a terminal-looking window that will ask for your password. Select your password from your e-mail and copy it (ask a TA for help if you can’t find your password). Then use right-click to paste it. (It may open a pop-up window asking what you want right-click to do. Select the default.) Hit enter, and it should log you in! Congratulations! It will probably ask you if you want to store the password in your password store, and then ask you for a master password. You can then use this master password to “unlock” all your ssh passwords for MobaXterm to use. You can pick something short and simple to remember since (at least for now) you’ll only be using it to log into the temporary account at farm, but if you end up using MobaXterm a lot you may want to change it. 3.4 Logging out and logging back in. OK. Now that you’re in, …log out and log back in! To log out, type ‘logout’. Then go back through the above to make sure you’ve got it all right. A few notes - for Mac OS X, you can use the up arrow to go to the previous command and run it. You’ll need to type your password in again, though. for MobaXterm, you’ll be able to do use the saved password so you won’t need to type your password in again; see the screenshot below. 3.5 You’re logged on to a remote computer. Now what? The magic of UNIX and the command line is that once you’re logged onto a remote computer, …everything works the same. Yes, you will have access to different files, and maybe different software, and different compute resources (more disk space, maybe more CPUs or more memory) but the command line basically works the same whether you’re logged in to your laptop, a workstation next door, or an HPC across the world. Let’s start by reprising some of the basics from workshop 1 (the command line) and workshop 2 (editing text files). 3.5.1 Welcome to your account! Start by running: pwd and you will see something like /home/ctbrown, although it will vary with the account name you used. This is because we’re all using different accounts with different default home directories. 3.5.2 Loading some files into your account Before we go any further, we need some files! You’ll note that if you do an ls, there’s nothing in your home directory. That’s because most UNIX accounts start out empty. (Sometimes there will be generic files like “Desktop” and so on in there - it depends on the system.) Well, actually, it’s not quite empty. Try: ls -la and you’ll see a few configuration files and directories. All of these are created automatically for you and you don’t need to worry about them for now. So, basically, your account is empty of user files. So let’s get some files! There are actually many ways to download files, and we’ll show you a few over the next few workshops. We’ll start by mimicking the setup of the binders on days 1 and 2 by copying a bunch of files from GitHub into your account. The following command will take the set of files here and make them appear in your account: git clone https://github.com/ngs-docs/2021-remote-computing-binder/ – note that git and GitHub are something we’ll cover more thoroughly in week 8. For now, just accept it as one way to go out and get files :). Now if you do ls you’ll see a directory 2021-remote-computing-binder/. Let’s cd into it - cd 2021-re&lt;TAB&gt; if you hit the TAB key where it says &lt;TAB&gt;, you’ll get command-line completion to work. If you type ls -F you should see some familiar sights (at least if you attended workshops 1 and 2) - 2cities/ binder/ data/ README.md SouthParkData/ – yep, these are the files we worked with on those two days! 3.5.3 Revisiting file and path manipulation If you cd data/ and do ls you’ll see the following files: MiSeq Slide1.jpg hello.sh nano1.png README.md gvng.jpg nano2.png and with ls -a we will see the following: . MiSeq Slide1.jpg hello.sh nano1.png .. README.md gvng.jpg .hidden nano2.png Now, if you navigate into the tmp1 directory located in the .hidden directory, cd .hidden/tmp1 you will be in a different absolute directory than you were on in the binder - now it’ll be something like /home/ctbrown/2021-remote-computing-binder/data/.hidden/tmp1, rather than /home/jovyan/data/.hidden/tmp1. That’s because we’re on a different system, with a different user account than before, and (unlike with the binder) we are going to be doing more things than just exploring the contents of the binder, so we’ve put things in the folder underneath 2021-remote-computing/ to contain data for today and workshop 4. This is an example of home directory organization and project management, which we’ll be talking about in workshop 6 (project organization) - how to organize your account so that you can figure out what the files in it probably mean. At this point, you could do the rest of workshop 1’s lesson, but rather than do that, let’s just note that all of the relative path navigation you did will continue to work, even though you’re on a different computer in a different account than you were using for workshops 1 and 2. For example, you can copy files between directories using the same relative path as before, cp thisinnotit.txt ../tmp2 and we navigate to the tmp2 directory and list the files that are in it we will see the thisinnotit.txt file has been copied to the tmp2 directory. cd ../tmp2 ls -l – but the difference is that this directory is now under /home/ACCOUNT/2021-remote-computing-binder/data/ rather than /home/jovyan/data/. Try running pwd and you’ll see that: pwd 3.5.4 Revisiting file editing Now go back to the 2021-remote-computing binder directory – cd ~/2021-re&lt;TAB&gt; Here, the ~/ refers to the absolute path to your home directory, whatever your username is - it’s different for everyone in the class! - and then the 2021-remote-computing-binder/ is a directory underneath it. We can use the file command as in workshop 2 to look at the file type of 2cities/book.txt.gz – file 2cities/book.txt.gz – and then uncompress it, gunzip 2cities/book.txt.gz which will produce the uncompressed file 2cities/book.txt from the compressed file 2cities/book.txt.gz. If we run head on the .txt file, we’ll see the first 10 lines of the file: head 2cities/book.txt Conveniently, all three editors that we showed you in workshop 2 are available here - let’s use nano (or an editor of your choice) to edit the book.txt file. If you’re using nano, run nano 2cities/book.txt and use the arrow key to go down 9 lines to a blank line, and type kilroy was here! or something else silly and identifiable. Now save, using CTRL-X, then ‘y’, then ENTER. Now, if you run head 2cities/book.txt you should see that your edits are there. A difference from what we did in workshops 1 and 2 is that these changes are now persistent. Unlike binder, the files on farm don’t go away when you log out! 3.6 Copying files to and from your local computer. So, we’ve just edited files on the ‘farm’ computer, which is a remote computer system (located on the UC Davis campus). Suppose that we want to get a copy of that file locally. How do we do that? The method varies depending on which ssh client you’re using. 3.6.1 Mac OS X: Copying files using ssh. If you’re on Mac OS X, log out of farm by typing logout and now you will be still in the Terminal program, but your shell prompt will be running on your local computer instead of farm. Note: it’s important that you’re no longer at the farm prompt! You should not see ‘farm’ in the command-line prompt! Now run the following command, replacing ‘-XX’ with your datalab account number: scp datalab-XX@farm.cse.ucdavis.edu:2021-remote-computing-binder/2cities/book.txt /tmp and you should see output that looks something like this: book.txt 100% 788KB 2.6MB/s 00:00 The scp command stands for “secure copy” and it mimics the syntax of the cp command: scp &lt;from&gt; &lt;to&gt;, where either &lt;from&gt; or &lt;to&gt; can be a remote location of the form login@computer:path/to/location. Now open your /tmp folder - you can do that with open /tmp - and you should see ‘book.txt’ there! 3.6.2 Windows: Copying files using MobaXterm. This is actually pretty easy :). Go to the file pane in your MobaXterm window, and select 2021-remote-computing-binder. Then select the 2cities folder. Then select ‘book.txt’, and click the ‘download’ button. (See screenshot below.) It will ask you where to put it; just put it somewhere you can find it, like your Desktop. 3.6.3 View and change the file you just downloaded Go ahead and open the file you just downloaded on your local system. You should see the changes you made with the nano editor on the remote system. Congratulations!! Now, edit it using whatever editor you like and change something recognizable. Be sure to save it! 3.6.4 Copy the file back to farm. On Windows with MobaXterm, you can use the “upload” button (next to the download button :) to upload book.txt from your Desktop back to farm. On Mac OS X, you need to run the command: scp /tmp/book.txt datalab-XX@farm.cse.ucdavis.edu:2021-remote-computing-binder/2cities/ CHALLENGE: Now verify that your book.txt file on farm contains the changes you made on your local computer, by: if you’re on Mac OS X, logging into farm changing to the 2021-remote-computing-binder/2cities/ directory using head, less, or an editor to look at the book.txt file. 3.6.5 Digression: why do you need to log into/log out of farm on Mac OS X? You may have noticed that, on Mac OS X, we’re logging out of farm to run commands on the local machine. That’s because scp is a shell command that’s running on your local computer. That brings up two questions. First, why aren’t we running it on farm? And second, is there any way that we can avoid logging out, the way the Windows folk can avoid logging out? The answer to the first question is that 3.7 Some commands are available! Others are not. You may remember looking at the South Park CSV data set in lesson 2 - cd ~/2021-remote-computing-binder/SouthParkData/ gunzip All-seasons.csv.gz head All-seasons.csv and those commands are all standard UNIX commands. You can also use cut, grep, sort, and uniq just fine - for example, let’s calculate how many times a character in South Park (in column 3) has “computer” in its name – cut -d, -f3 All-seasons.csv | grep Computer | sort | uniq -c So those commands all work. But csvtk doesn’t – this command fails, csvtk cut -f Character All-seasons.csv | grep Computer | sort | uniq -c because csvtk isn’t installed. And that’s what we’ll be showing you how to do in workshop 5 - install software like csvtk using conda. 3.8 Summing up file transfer - a challenge! Let’s all do the following: Download the file All-seasons.csv from the directory 2021-remote-computing-binder/SouthParkData/ on farm to your local computer. Open it in a spreadsheet program and edit the first line. Export it to a CSV file with the name All-seasons-changed.csv Upload the file back to farm, to the directory 2021-remote-computing-binder/SouthParkData/ Log into farm and confirm it’s there and that the changes are present, using head All-seasons-changed.csv. 3.9 Summing things up Today, you learned how to log into remote systems, execute commands, and transfer files to and from your local computer. In our next workshop we will talk more about using shared systems to do work. "],["running-programs-on-remote-computers-and-retrieving-the-results.html", "4 Running programs on remote computers and retrieving the results 4.1 Using SSH private/public key pairs 4.2 Mac OS X and Linux: Using ssh private keys to log in 4.3 Windows/MobaXterm: Using ssh private keys to log in 4.4 Some tips on your private key 4.5 Working on farm 4.6 Using multiple terminals 4.7 File systems, directories, and shared systems 4.8 Disk space, file size, and temporary files 4.9 Summing things up", " 4 Running programs on remote computers and retrieving the results This two hour workshop will show attendees how to use remote computers to run their analyses, work with the output files, and copy the results back to their laptop and desktop computers. We will discuss input and output formats, where files are usually read from and written to, and how to use the ssh software to copy files to and from remote computers. In workshop 4, we will do more with running remote commands, getting files onto your remote system, file permissions, and actually working effectively on remote systems. We will also talk a bit about processes and other aspects of multiuser systems. 4.1 Using SSH private/public key pairs Today we’re going to start by using a different way to log in - ssh key pairs. Key pairs are a different way to provide access, and they rely on some mathematical magic called asymmetric cryptography or public-key cryptography (see wikipedia). (The details are far beyond the scope of this lesson, but it’s a fascinating read!) There are two parts to key pairs - the private part, which you keep private; and the public part, which you can post publicly. Anyone with the public key can challenge you to verify that you have the private key, and only the person with the private key can verify, so it’s a way to “prove” your identity and access. (The same idea can be used to sign e-mails!) Key pairs solve some of the problems with passwords. In brief, they are (much!) harder to guess than passwords. key pairs enable programs to do things without you having to type in your password. the private part of a key pair is NEVER shared, unlike with passwords where you have to type the password in. but the public part of pair can be shared widely. Because of these features, some systems demand that you use them. Farm is (usually) one of them; we have a special exception for the datalab-XX accounts, because key pairs are a confusing concept to teach right off the bat. 4.2 Mac OS X and Linux: Using ssh private keys to log in Your private key for your datalab-XX account is kept in .ssh/id_rsa. We need to copy it locally to make use of it. Run the following command in your Terminal window: cd ~/ scp datalab-XX@farm.cse.ucdavis.edu:.ssh/id_rsa datalab.pem and then chmod og-rwx datalab.pem (we’ll explain the second command below!) datalab.pem is your private key pair! Now, to log into farm using the key pair, run ssh -i datalab.pem datalab-XX@farm.cse.ucdavis.edu and voila, you are in! (VICTORY!) You’ll need to keep track of your datalab.pem file. I recommend keeping it in your home directory for now, which is where we downloaded it. 4.3 Windows/MobaXterm: Using ssh private keys to log in For MobaXterm, connect as you did in workshop 3 and download .ssh/id_rsa to some location on your computer, named datalab.pem. Now, create a new session and go to “Advanced SSH options” and select it the private key pair (see screenshot). Now connect. Voila! No password needed! VICTORY!! Note that if you change the location of your private key file, you’ll need to go find it again :). 4.4 Some tips on your private key NEVER SHARE YOUR PRIVATE KEY. We’ll talk more about private key management in the future, but the basic idea is that you should create a new private key for each computer you are using, and only share the public key from that computer. 4.5 Working on farm So farm is a shared computer with persistent storage (which is typical of a remote workstation or campus compute cluster (HPC). This means a few different things! Let’s start by logging back into farm. (You got this!) 4.5.1 First, download some files: Let’s make sure you have the right set of files from the last workshop – this will take the set of files here and make them appear in your farm account: cd ~/ git clone https://github.com/ngs-docs/2021-remote-computing-binder/ (If you’ve already done this, you can run this again and it will just fail, and that’s fine.) 4.5.2 Configuring your account on login One thing you can do is configure your account on login the way you want. This typically involves configuring your login shell. Edit the file ~/.bashrc, e.g. with nano: nano ~/.bashrc and type echo Hello and welcome to farm at the top of the file. If using nano, save with CTRL-X, say “yes” to save, hit ENTER. Now log out and log back in. You should now see ‘Hello and welcome to farm’ every time you log in! (You can easily delete it too, if you find it annoying :) The commands in .bashrc are run every time you run bash, which is the default login shell on farm. (There are other command-line shells, too, and they would be configured using different files.) There’s also a file called .profile that is run for every login and we may touch on that difference later - here’s a help article on it if you’re interested. Perhaps more usefully than ‘echo’, you can add commands like alias lf=&#39;ls -FC&#39; in your .bashrc if you want to configure your account that way; we’ll cover more configuration commands in workshop 6 and beyond. To see the changes without having to log out and log back in, run source ~/.bashrc and now lf will automatically run ls with your favorite options. For another example, here you could make rm ask you for confirmation when deleting files: alias rm=&#39;rm -i&#39; CHALLENGE QUESTION: Create an alias of hellow that prints out hello, world and add it to your .bashrc; verify that it works on login! 4.6 Using multiple terminals You don’t have to be logged in just once. On Mac OS X, you can use Command-N to open a new Terminal window, and then ssh into farm from that window too. On Windows, you can open a new connection from MobaXterm simply by double clicking your current session under “User sessions.” What you’ll end up with are different command-line prompts on the same underlying system. They share: directory and file access (filesystem) access to run the same programs, potentially at the same time They do not have the same: current working directory (pwd) running programs, and stdin and stdout (e.g. ls in one will not go to the other) These are essentially different sessions on the same computer, much like you might have multiple folders or applications open on your Mac or Windows machine. You can log out of one independently of the other, as well. And you can have as many terminal connections as you want! You just have to figure out how to manage them :). CHALLENGE: Open two terminals logged into farm simultaneously - let’s call them A and B. In A, create a file named ~/hello.txt, and add some text to it. (You can use an editor like nano, or you can use echo with a redirect, for example. If you use an editor, remember to save and exit!) In B, view the contents of ~/hello.txt. (You can use cat or less or an editor to do so.) A tricky thing here is that B does not necessarily have a way to know that you’re editing a file in A. So you have to be sure to save what you’re doing in one window, before trying to work with it in the other. We’ll cover more of how to work in multiple shell sessions in workshop 7 and later. 4.6.1 Who am I and where am I running!? If you start using remote computers frequently, you may end up logging into several different computers and have several different sessions open at the same time. This can get …confusing! (We’ll show you a particularly neat way to confuse yourself in workshop 7!) There are several ways to help track where you are and what you’re doing. One is via the command prompt. You’ll notice that on farm, the command prompt contains three pieces of information by default: your username, the machine name (‘farm’), and your current working directory! This is precisely so that you can look at a terminal window and have some idea of where you’re running. You might also find the following commands useful: This command will give you your current username: whoami and this command will give you the name of the machine you’re logged into: hostname These can be useful when you get confused about where you are and who you’re logged in as :) 4.6.2 Looking at what’s running You can use the ps command to see what your account, and other accounts, are running: ps -u datalab-09 This lists all of the different programs being run by that user, across all their shell sessions. The key column here is the last one, which tells you what program is running under that process. You can also get a sort of “leaderboard” for what’s going on on the shared computer by running top (use ‘q’ to exit). This gives a lot of information about running processes, sorted by who is using the most CPU time. If the system is really slow, it may be because one or more people are running a lot of things, and top will help you figure out if that’s the problem. (Another problem could be if a lot of people are downloading things simultaneously, like we did in workshop 3; and yet another problem that is much harder to diagnose could be that one or more people are writing a lot to the disk.) This is one of the consequences of having a shared system. You have access to extra compute, disk, and software that’s managed by professionals (yay!), but you also have to deal with other users (boo!) who may be competing with you for resources. We’ll talk more about this when we come to workshop 10, where we talk about bigger analyses and the SLURM system for making use of compute clusters by reserving or scheduling compute. If performance problems persist for more than a few minutes, it can be a good idea to e-mail the systems administrators, so that they are alerted to the problem. How to do so is individual on each computer system. On that note – 4.6.3 E-mailing the systems administrators When sending an e-mail to support about problems you’re having with a system, it’s really helpful if you include: your username and the system you’re working on the program or command you’re trying to use, together with as much information about it as possible (version, command line, etc.) what you’re trying to do and what’s going wrong (“I’m trying to log in from my laptop to farm on the account datalab-06, and it’s saying ‘connection closed’.”) a screenshot or copy/paste of the confusing behavior a thank you This information is all useful because they deal with dozens of users a day, and may be managing many systems, and may not be directly familiar with the software you’re using. So the more information you can provide the better! 4.7 File systems, directories, and shared systems One of the other consequences of working on a shared system is that you’re often sharing file systems with other people. That means you need to make sure they don’t have access to things they shouldn’t have access to. 4.7.1 Read and write permissions into other directories Try running this: ls ~datalab-09/ what do you see? That’s right, that’s my account, and my files. Now run it again: ls ~datalab-09/ By default, home directories on many systems are readable by everyone. However, they’re never writable unless you enable that intentionally for a directory. To see that, try creating a file in my home directory: echo hi &gt; ~datalab-09/test.txt and you will see Permission denied. 4.7.2 Listing directory and file permissions Let’s look at your home directory: ls -lad ~/ you should see something like: drwx------ 3 datalab-08 datalab-08 3 Aug 5 18:32 /home/datalab-08 and compare that to what you get if you look at my home directory: ls -lad ~datalab-09/ where you will see: drwxr-xr-x 8 datalab-09 datalab-09 10 Aug 11 17:59 /home/datalab-09 what does this all mean? In order, you have: ‘d’ means directory the first ‘rwx’ means ‘readable, writable, executable by owner’ the second ‘r-x’ means ‘readable, not writable, executable by group’ the third ‘r-x’ means ‘readable, not writable, executable by others’ the first ‘datalab-09’ is the owner of the directory the second ’datalab In the context of directories, the “x” means “can change into it.” If a directory is not +x for a particular user, that means they cannot change into it or into any directory underneath it. (We’ll talk more about what “executable” means in workshop 7, when we talk about scripting.) If you go back and look at your own home directory, you can see that by default (the way these accounts were set up), only you have drwx------ 3 datalab-08 datalab-08 3 Aug 5 18:32 /home/datalab-08 Now let’s modify it so that other members of group datalab-08 can access it – chmod g+rx ~/ ls -lad ~/ and you should see: drwxr-x--- 3 datalab-08 datalab-08 3 Aug 5 18:32 /home/datalab-08 Likewise, you could make it group writable with g+w, and you could make it world readable with o+rx - for example, ~ctbrown is world readable. You can set user, group, and ‘other’ permissions all at once with ‘a’ - so, for example, chmod a+rx ~/ would make your home directory readable/executable by the user, the group, and everyone else. One particularly useful thing you can do is make files read only for yourself! This prevents you from changing or deleting them by accident. For example, echo do not change me &gt; important-file.txt chmod a-w important-file.txt echo new information &gt; important-file.txt and you should see ‘permission denied.’ 4.7.3 Files have the same permission options So far we’ve been talking about directories, but files have the same permission settings. Try running ls -la ~/ and you’ll see the same kind of output for files. Here you can set +r or -r for read, +w or -w for write, etc. 4.7.4 How do groups work? You might be puzzled to note that your files belong to a group with the same name as your username. What’s up with that? On many systems (farm included) users are set up with a default group that only they belong to. Then users are added to additional groups as needed. This gives them the option of using groups for sharing files via group permissions, but decreases the likelihood that files get shared by accident. For this reason, all of the datalab-XX users belong to multiple groups: one group that is uniquely yours, and one group that is shared by all of the datalab-XX users. You can see what groups you (and other users) are members of like so: groups datalab-09 where you will see that I am a member of two groups, datalab-09 and datalab. If you are a member of a group, you can use the chgrp command to change the owning group of a file to that group: echo test &gt; test-file.txt chgrp datalab test-file.txt ls -lad test-file.txt CHALLENGE: What commands would you run to change the permissions on test-file.txt so that all the datalab-XX users have read and execute (but not write!) access to it? Note that all datalab-XX users belong to the datalab group. (You may not want to run these commands, but it won’t hurt if you do. Plus you can always change them back.) 4.7.5 How can you use this? I rarely use group permissions in my home directory, because I usually default to having my files be a+r But sometimes on farm there are large files that people in my research group want to share with each other but not with others, and you can use group permissions to manage access to things like that. Good practice (or at least practice that I recommend) is to do the following: put research-private files under directories that are g+rx and o-rwx. if you have a directory where you want people to be able to add new files but not change old ones, you can make the directory itself g+rwx but keep the files g+r and g-w (which is usually the default). That having been said, when setting something like this up for the first time, it’s worth writing down what you want the access permissions to be, then setting them up with chmod, and then checking with someone experienced (like the systems administrators) that you’ve got the right permissions for the policies you want to enforce. Note also that UNIX file permissions are kind of a blunt instrument, so I recommend keeping it as simple as possible. Generally you want to be using a separate system for tracking raw data and making sure that it’s backed up, etc. - there are various archival systems that we can recommend, depending on your file sizes and your research needs. 4.7.6 Things that regular users cannot do There are basically no exceptions to the permissions rules above for regular users. Linux has (by default) only two “tiers” of users - a regular user, and a “superuser”, usually referred to as “root”. Only root can do things like change the ownership of files, access files with restrictive permissions, etc. One situation where this can be important is when someone leaves a research group and you need access to their files but they no longer have access to the system themselves because their account is disabled. In this case, you might have to have a supervisor or the researcher themselves e-mail the farm systems administrators to fix the access problem, because they are the only people besides the owner of the file(s) who can change the permissions. It’s also a good reminder that on shared systems, other people will have access to your files - that’s completely legal and correct (because they’re the people running the system!) But this is why you need to be careful about what systems you use to store sensitive information, and why words like “HIPAA compliant” become important - it ensures that certain security and access policies are in place to protect sensitive data. 4.8 Disk space, file size, and temporary files You can see how much free disk space you have in the current directory with this command: df -h . You can see how much disk space a directory is using with du: du -sh ~/ I highly recommend using /tmp for small temporary files. For bigger files that you might want to persist but only need on one particular system, there is often a location called /scratch where you can make a directory for yourself and store things. We’ll talk more about that in workshop 10. Finally, the command free will show you how much system memory is available and being used. This command: cat /proc/cpuinfo will give you far too much information about what processors are available. Again, we’ll talk more about this in workshop 10 :). 4.9 Summing things up In this workshop, we talked a fair bit about working on shared systems, setting permissions on files, transferring files around, and otherwise being effective with using remote computers to do things. In workshop 5, we’ll show you how to customize your software environment so you can do the specific work you want to do. We’ll use CSV files, R, and some bioinformatics tools as examples. "],["installing-software-on-remote-computers-with-conda.html", "5 Installing software on remote computers with conda 5.1 Why is software installation hard? 5.2 Getting started with conda 5.3 Installing more software in your current environment 5.4 Using the ‘bioconda’ and ‘conda-forge’ channels 5.5 Conda and data science: R and Python 5.6 Tricky things to think about with conda 5.7 Reference list of Conda Commands 5.8 More Reading on Conda 5.9 Discussion items: 5.10 In summary", " 5 Installing software on remote computers with conda This two hour workshop will show attendees how to install and manage software using the conda installation system. We will give examples of installing Python and R software, and managing conda environments on remote systems. This lesson was adopted from a lesson co-authored by Shannon Joslin for GGG 298 at UC Davis. There is also a really nice binder-based tutorial on the NIH CFDE training Web site! Learning objectives: learn the basics of software installation, software dependencies, and isolation environments learn about conda and how to use it learn about conda-forge and bioconda and how to install software from them learn to use conda to manage R and Python installations Other references: ANGUS 2019 lesson Why you need Python Environments and How to Manage Them with Conda 5.1 Why is software installation hard? It’s a confusing ecosystem of operating systems (Mac OS X, many versions of Linux, Windows) Many software has many dependencies (e.g. just consider base language – C++, Java, Python, R, and their different versions) Caption: Software has a lot of dependencies This leads to confusing situations where different versions of underlying software are need to run two different programs – what if you wanted to run Macs14 and sourmash both, but one wanted ‘python’ to mean python2 and the other wanted ‘python’ to mean python3? Caption: sometimes different software packages can’t coexist Decoupling user-focused software from underlying operating systems is a Big Deal - imagine, otherwise you’d have to rebuild software for every OS! (This is kind of what conda does for you, actually - it’s just centralized!) Also, lot of software installation currently requires (or at least is much easier with) sysadmin privileges, which is inherently dangerous. Why do you need isolated software install environments? Some specific reasons: your work relies on a bunch of specific versions (perhaps old versions?) working with a collaborator who really likes a particular feature! experiment with new packages without messing up current workflow (reproducibility!) publication (“here’s what I used for software”, repeatability!) sometimes workflows rely on incompatible software packages! see my twitter question Conda tries to solve all of these problems, and (in my experience) largely succeeds. That’s what we’ll explore today. Conda is a solution that seems to work pretty well, and can be used by any user. Downsides are that it can get big to have everyone install their own software system, but it’s not that big… (The farm admins like it, too!) Caption: conda environments and packages Note that conda emerged from the Python world but is now much broader and works for many more software packages, including R! 5.2 Getting started with conda 5.2.1 Installing conda We installed conda in your account already for you. In case you want to install it on another computer, or in another account, the miniconda installation instructions are pretty good! Note that we also added the following software sources to your installation, conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge and we’ll talk about this later. But if you install miniconda on your own, you’ll need to run these commands in your new installation to set up the software sources correctly. 5.2.2 Log into farm As per the instructions in workshop 3 and workshop 4, log into farm.cse.ucdavis.edu using your datalab-XX account. When you log in, your prompt should look like this: (base) datalab-09@farm:~$ If it doesn’t, please alert a TA and we will help you out! The ‘base’ part of the prompt is new and it indicates that conda has been activated in your account and that you are in the base environment. Read on! 5.2.3 Creating your first environment &amp; installing csvtk! 5.2.3.1 What is a conda environment? A conda environment is a specific collection of packages/tools that you have installed. For example, you may have one environment with Python 2.7 and its dependencies, and another environment with Python 3.4, both for legacy testing. And then you might have a Python 3.9 environment for the latest version of Python. Environments are isolated from each other: if you change one environment, your other environments are not affected. You can easily activate new environments, which is how you switch between them. 5.2.4 Installation! Let’s install some software! We’ll start with csvtk, which we introduced in workshop 2, working with text files. Here’s the command to create a new conda environment, named ‘csv’, with csvtk installed. conda create --name csv -y csvtk Here, we are installing the csvtk package into an environment named csv. The -y indicates that we don’t want conda to ask us for confirmation - it should just go ahead and create the environment. Now, activate the new environment: conda activate csv Your prompt should change to have (csv) at the beginning. Let’s run csvtk on some real data now! We’ll use the files from workshop 2 (see these instructions for getting them) – make sure this works: cd ~/2021-remote-computing-binder/SouthParkData gunzip All-seasons.csv.gz ls -l All-seasons.csv (FYI, you may get an error in the gunzip command if you’ve already run that - it’s fine!) You should now be able to run: csvtk cut -f Character All-seasons.csv | grep Computer | sort | uniq -c and you should see: 13 Computer 3 Computer Voice 5.2.5 csvtk in a bit more detail Let’s explore csvtk a bit - what’s going on here? csvtk is a cross-platform library for working with CSV and TSV files. It’s written and maintained by Wei Shen, and it’s free and open source. It’s relatively new - I found out about it only a year or two ago - and while it doesn’t do anything I couldn’t do with other commands, it packages a bunch of really nice functionality together into one tool. For example, I can find the headers in a csv file like so, csvtk headers All-seasons.csv and I can then cut out one particular column and summarize things – e.g. csvtk cut -f Character All-seasons.csv | sort | uniq -c | sort -n | tail (which tells me that Cartman is by far the most quoted character in this file). What I’m doing here is using piping and filtering (from workshop 2) to: take the contents of the Character column, produced by csvtk sorting the contents (sort) counting the number of distinct elements (uniq -c) sorting the counts from least to most by number (sort -n) and then showing only the last 10 (tail) (It looks complicated, but as you start using these commands more and more, it will become second nature!) CHALLENGE: How would I find the least quoted characters in this file? The key thing about csvtk for this lesson is that it’s new software, and it’s mostly useful for data scientists, so it’s not “built into” UNIX operating systems yet (and may never be). So when you try running it in the base environment, conda activate base csvtk you’ll get “command not found” because it’s not installed. But conda lets us install it from bioconda, a community repository of software! And then we can use it! Yay! If you go back to your csv environment, you’ll see that you can run it again: conda activate csv csvtk 5.2.6 Where is the software coming from!? When we run conda create to install csvtk, where is conda looking for and finding the software? The short answer is “the Internet”, and more specifically, pre-prepared conda software repositories, or “channels”. Bioconda is one such channel, as is conda-forge. We’ll talk more about channels below. 5.2.7 Digression: there are many ways to install software! If you’re at all familiar with UNIX, you might be saying “wait! but I already use homebrew! or apt! or yum! or containers! or …!” Or you may point your sysadmins at this tutorial and they’ll say “well you should just use the modules system on your HPC.” Yes, there are many ways to install and use software! Why conda? We like conda, because it’s straightforward, flexible, cross-platform, supports version pinning, and does not require systems administrator access. In the past 12 years of teaching bioinformatics and data science, conda has come closest to being the thing that works for everyone, on every platform. As they say, “your mileage may vary” (YMMV). But we like conda :). 5.3 Installing more software in your current environment Once you’re in an environment, you can install new software with conda install -y &lt;software_name&gt; like so: conda install -y fastqc and that should work too! You’ll be able to run the fastqc command now. Here, FastQC is a completely separate application that we use in bioinformatics for looking at FASTQ files. We talk about that more in some other workshops we run, but: the main point is that it’s just some more “non-standard” software that you can install! Generally you want to avoid installing too many packages in one environment, as it starts to get slow to figure out whether or not something can be installed. We’ll talk more about this below. You can list software with conda list: conda list which is less useful than you might think, given how many packages we’ve installed… in the future you will be able to list only the user-requested packages and not the dependencies needed for them, but for now, there’s no good way to do that. (Why are there so many? Because most software is built on top of lots of other software… and so you need to install all of that other software, too!) 5.3.1 Finding and specifying versions To see what version of a particular piece of software you have installed, run: conda list csvtk and look at the second column. (Note that conda list doesn’t need an exact match, so e.g. you can find all packages with ‘csv’ in the name by doing conda list csv). As of Aug 2021, conda installs csvtk version 0.23.0. You can force conda to install exactly this version in the future like so, conda install csvtk==0.23.0 Unfortunately there’s no good way to know if a new version of a software package is “significant” or not, unless they use semantic versioning… Generally if there’s a big number update (1.0 -&gt; 2.0) the software will work quite differently, but there’s no guarantee on that. For example, our software sourmash 1.0 was very different from 2.0, while sourmash 3.0 was virtually identical to 2.0 in usage (but not in implementation). The next version, sourmash 4.0, broke things. (The lesson is, don’t trust software projects to be consistent in their versioning!) CHALLENGE: Use the conda create command to create a new environment and install the latest version of sourmash in it. Then activate that environment and verify that you can run ‘sourmash’. Make sure to switch back to your csv environment when you’re all done: conda activate csv 5.3.2 Making and using environment files What if you want to specify collections of software that you use together? And/or send collaborators or colleagues the set of software they need, all in one nice file? conda has a nice human-readable format for that, called an ‘environment file’. These are supposed to be reasonably portable files that you can ship around to different computers and have them automatically install the right stuff. You can see the one for the binder for workshops 1 and 2 here, for example. (These are YAML files, which are in a format that’s structured for computers to read but not super difficult for humans to write. You’ll see these used occasionally in configuration files for bioinformatics software, too.) If you have a complicated environment that you want to save, you can make an environment file from your current environment like so: conda env export &gt; export.yml To look at the environment files we’ve just created, do: cat export.yml and you can create a new environment from this file like so: conda env create -n csv2 -f export.yml This would create a new environment called csv2 that has all the same things installed in it as csv does (because that’s where we ran conda env export!) 5.3.3 Updating, removing, etc software You can update software with conda update, and remove software with conda remove. Generally there’s not too much need for these commands tho, as we recommend just creating new environments with a pinned version, OR the latest version. Then, when you want to update your software, you create a new, clean environment. 5.3.4 Creating multiple environments As you can probably infer from the above, you can have multiple environments with different (potentially incompatible) things installed in each one. The default is base. Other environments will have their own set of packages. Environments do not include packages from other environments; you’ll need to install each package in every environment that needs it. You can list environments with conda env list: conda env list It will list all of the available environments as well as denote the environment you are currently in with an *. Switch environments with conda activate &lt;environment_name&gt;, and remove environments with conda env remove -n &lt;environment_name&gt;. Note that switching environments doesn’t switch your directory, it just switches the software you’re running.. Whoa… 5.3.5 Tech interlude: what is conda doing? What conda does when it switches environments is change the location where it searches for software – the PATH (and other environment variables) – so that the software packages are searched for in different places. Try changing environments and each time doing echo $PATH. You should see that the first element in the PATH changes each time you switch environments! (You can also use type &lt;program&gt; or which &lt;program&gt; to see where a program is located, and which program you are running when you type &lt;program&gt;. 5.3.6 Challenges with using one big environment We generally recommend using a task-specific environments for each task (e.g. one environment for your R work, and another for your bioinformatics work), because this can dramatically simplify installation and speed up new conda installs. This is because conda needs to make sure that any new package (and all of its dependencies) you would like to install doesn’t conflict with all the existing packages. 5.3.7 How Titus uses conda That having been said, my experience is that I rely on a core set of packages for my daily work, and so I usually have my “daily work” environment where I have a bunch of stuff installed, and then when I’m working on data analysis, I don’t want to constantly install new things or switch environments. So I usually have a default environment that I work in, and when I use non-standard software (stuff I use infrequently or for specific tasks) I create software-specific environments to work in. snakemake (which we’ll be talking about in workshop 9) helps with this by letting you use analysis-specific environments. 5.3.8 Finding packages within conda To search for all available versions of a particular package from the command line, do: conda search &lt;software&gt; 5.4 Using the ‘bioconda’ and ‘conda-forge’ channels Conda-forge and Bioconda are “software channels” for conda, that provide collections of conda-packaged software. In particular, conda-forge contains an awful lot of general community packages as well as many Python and R libraries, while bioconda is more focused on biology/bioinformatics tools specifically. You can install stuff directly from these channels by specifying the bioconda channel explicitly: conda install -c bioconda .... Or, you can add it to your “default” set of channels to search, as we did above: (You don’t need to run these, but you can:) conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge this sets up your .condarc file – take a look, cat ~/.condarc This will automatically make conda install search for packages in bioconda. Note: if you get the error PackagesNotFoundError: The following packages are not available from current channels: but you’re pretty sure the package exists, you probably need to configure your channels properly :) 5.4.1 Mac OS X and Linux, but not Windows Note conda itself works on Windows, OS X and Linux! But unfortunately many conda-forge and bioconda packages are only available for OS X and Linux, and not Windows :(. This is because they are built for Linux or a UNIX-like OS. 5.4.2 How to engage with conda-forge and bioconda Both conda-forge and bioconda are community-driven libraries of software. They rely upon people (like you!) to package software; this involves writing a recipe, like the sourmash recipe. Some tips and tricks for using conda-forge and bioconda: Both are community maintained, please be respectful and friendly - no one is working for you, and no one is making money doing this. Help advocate for your favorite software to be added, and/or do it yourself! (e.g. gtdbtk) - we can help! File bug reports, but be respectful and helpful. See a sterling example :). 5.5 Conda and data science: R and Python Conda emerged from the Python community initially, but it now has some pretty good support for R as well, through the conda-forge channel. Why use Conda for R and Python? What does it give you? A few things - with conda, you can manage multiple R and Python installations, each containing different (and incompatible!) versions of packages. where libraries are available via conda, you can install them without compiling them. This can be (much) faster than using the standard install.packages or pip install supported by R and Python. where libraries are not available via conda, you can still install them, and you don’t need to do anything special or tricky - just run the standard installation commands. 5.5.1 Conda and R conda-forge now has some pretty good support for R - see this environment file for RNAseq, for example. It installs both tidyverse packages and bioconductor packages! Let’s install R with RMarkdown support. First, let’s figure out what the right package name is. We could go to the conda-forge page and search for rmarkdown, but it turns out that google is often your best bet :). Google ‘install rmarkdown with conda’ and you’ll see that the first hit is r-rmarkdown. Let’s try it! Side note: we’re going to switch to using the mamba command, because it’s faster. More on that below. mamba create -n rmd -y r-rmarkdown This will go ahead and install R itself, as well as all of the packages needed to compile RMarkdown documents (like this Web site, in fact). Activate the environment: conda activate rmd Now, try type R to see where R is installed - under your own account. Yay! If you want, you can run R and then library(rmarkdown) to verify that it’s installed. I’ve found that the majority of R packages I use in bioinformatics are readily available via conda-forge, which is nice. Again, your mileage may vary… CHALLENGE: What would be the command to install the dplyr library for R in either the existing rmd environment, or in a new environment? (You can run it if you like, but it might take a few minutes.) 5.5.2 Conda and Python Python is heavily used in data science as well, and it is also well supported by conda. Conveniently, you can install different versions quite easily: mamba create -n py39 -y python==3.9 will install Python v3.9. Then conda activate py39 type python python -V will show you that Python is v3.9 in this environment. Conveniently, as with R, you have full installation privileges - so you could run the python package installer, pip, to install stuff. For example, run: pip install screed to install the screed library in your very own Python environment. (screed is another library my lab built for reading sequence data files; here we’re just using it as an example of something that you can install with conda :). 5.5.3 Supporting interactive packages (RStudio and JupyterLab) Many (most?) people now use R and Python packages via RStudio and JupyterLab, and it’s totally possible to use conda installs with that. Unfortunately, showing you how to run RStudio Server and JupyterLab on farm is a bit out of scope for this workshop series, but please drop us a note at datalab-training@ucdavis.edu if you’re interested. 5.6 Tricky things to think about with conda 5.6.1 It can take a long time to install lots of software This is because conda needs to make sure to resolve version incompatibilities before proceeding. Solution: use isolated environments to install single packages, instead. Another solution: use the mamba command, a drop-in replacement for conda. All of the commands above will work with mamba instead of conda, except only conda activate which must be done using conda. 5.6.2 Explicit package listing You can grab an explicit list of version pinned software that is OS specific like so - conda list --explicit &gt; package-files.txt conda create --name myenv --file spec-file.txt conda install --name myenv --file spec-file.txt this will guarantee identical environments. See the conda docs for more information. 5.7 Reference list of Conda Commands Conda commands action conda install &lt;package_name&gt; install a package conda list list installed packages conda search &lt;package_name&gt; search for a package––this can be the full or partial name of a package conda info list of information about the currently active environment conda list list out all the installed packages in the currently active environment conda remove &lt;package_name&gt; remove a conda package conda config --get channels list out the active channels and their priorities conda update update all the installed packages conda config --remove channels &lt;channel&gt; remove unwanted channel conda env list list the different environments you have set up conda activate &lt;new_environment_name&gt; activate the a new conda environment conda (this also works for activating our base environment conda info --envs list the locations of conda directories 5.8 More Reading on Conda Conda Documentation Drawing credit: Gergely Szerovay. Read original article here 5.9 Discussion items: What happens if something isn’t conda installable? You can install it as normal, and it will be usable as normal. However, it won’t be “managed” via conda (or snakemake) Disk, vs environment, vs login shell especially note that multiple terminals all look at the same disk 5.10 In summary Conda is one way you can install software on computers, including most especially HPC and cloud computers. Conda lets you create separate “environments” containing collections of software that are isolated from other collections. Conda supports a pretty normal data science set of tools, and also provides tools to support computational reproducibility via environment files and version pinning. Go conda!! "],["structuring-your-projects-for-current-and-future-you.html", "6 Structuring your projects for current and future you 6.1 Learning Objectives 6.2 Transferring files around efficiently 6.3 Retrieving remote files from Web sites 6.4 Dealing with files: some recommendations 6.5 Farm vs cloud 6.6 Thinking about data science projects! 6.7 One example: a rough bioinformatics workflow 6.8 Sending and Receiving Data 6.9 Storing data 6.10 Where do I work with large amounts of data? 6.11 Setting up your project 6.12 Naming files 6.13 Looking forward to the next few workshops: techniques for doing data science on remote computers. 6.14 Additional resources", " 6 Structuring your projects for current and future you In this two hour workshop, we will discuss folder structures for organizing your projects so that you can track inputs, outputs, and processing scripts over time, and keep yourself organized as your projects evolve. This lesson was adopted from a lesson co-authored by Shannon Joslin for GGG 298 at UC Davis. 6.1 Learning Objectives By the end of this lesson, students will: know how to transfer files around efficiently understand how to set up a data-driven project for tracking and proper iteration know how to store and work with mass quantities of data understand why to setup projects in a specific manner begin to plan file and directory names for their own workflows 6.1.1 Lesson requirements This lesson builds on workshop 3 and workshop 4. Before continuing, please: be sure that you can log into farm.cse.ucdavis.edu with your datalab-XX account do pre-load some files into your account 6.2 Transferring files around efficiently We’ve spent some time transferring single files around with ‘scp’, and hopefully you feel comfortable with that now. But many people (myself included) find ‘scp’ kind of annoying because you have to already know the path to the filename, and it’s kind of inconvenient for transferring multiple files around. For this, we’re going to make the MobaXterm folk use a shell window: OK, so what options are there when using ‘scp’? 6.2.1 recursive scp with -r First, you can copy entire directories with scp by using -r – from your laptop, try doing scp -r datalab-XX@farm.cse.ucdavis.edu:2021-remote-computing-binder/2cities 2cities this will transfer the entire directory to your local computer. 6.2.2 sftp You can use a command line within a command line with sftp. Briefly, sftp datalab-XX@farm.cse.ucdavis.edu will open up a “secure file transfer” shell, that has its own set of commands. Try typing: cd 2021-remote-computing-binder cd 2cities ls get README.md – I use this quite a bit when I want to be able to use ‘cd’ and ‘ls’ to find the right file to download. Use quit to exit out of SFTP. 6.2.3 zip -r to create collections of files Zip files work on pretty much all systems, and are handy ways to transport collections of files. To create a zip file: cd ~/2021-remote-computing-binder zip -r 2cities.zip 2cities The file ~/2021-remote-computing-binder/2cities.zip is now a file you could copy or transfer around, and it will unpack to the entire contents of the directory 2cities/. To view the contents of a zip file: unzip -v 2cities.zip and to unpack it: cd ~/ unzip 2021-remote-computing-binder/2cities.zip which will create a new directory ~/2cities/. 6.2.4 Working with .tar.gz files While I strongly recommend using zip files, you will inevitably run into .tar.gz files on UNIX. You can think of these as “collections of files that have been glommed into one file and then gzipped”, and here are the two commands you need to know: tar tzvf ~ctbrown/2cities.tar.gz will show you the contents of a .tar.gz file, and tar xzvf ~ctbrown/2cities.tar.gz will unpack it into your current directory. I don’t recommend creating .tar.gz files in general so I’m not going to show you how to create them :). tar stands for ‘tape archive’ and is notoriously confusing to use, so I’m not going to say any more about it. (Obligatory xkcd on using the tar command.) 6.2.5 Probably the most useful advice: use a transfer directory To me, it’s really annoying to find and remember directory paths and filenames when transferring files around, and I’ve been doing it for 30 years and am really practiced at it. So what I sometimes do is use a ‘transfer’ directory. On farm, mkdir ~/transfer cp ~/2021-remote-computing/2cities/README.md ~/transfer/ and now (on your laptop) you can just do things like scp datalab-XX@farm.cse.ucdavis.edu:transfer/README.md . and you don’t have to remember the full path. This is handy because you can use your current working directory and things like tab completion on the remote system to copy files into your transfer directory, and then remember only a short path to actually transferring files around. I use this when I’m working in complicated or annoying directory structures. 6.3 Retrieving remote files from Web sites Often you want to grab files from Web sites - CSV data sets, text files, or what have you. This involves finding the URL, and then using a program like wget or curl to get them. We’re going to use curl today, but wget does pretty much the same thing. Let’s find a URL for the Tale of Two Cities book that we’ve been using, over on Project Gutenberg. Go to this Web site in a browser: https://www.gutenberg.org/ebooks/98 and find the link that says “Plain text”. Right click on it, and “copy link”. Now go back to your terminal window where you’re logged into farm, and run cd ~/ curl -L -O https://www.gutenberg.org/files/98/98-0.txt and this will create a file 98-0.txt in your current directory. You can run head on this file: head 98-0.txt and see that it contains the right text. The trick is often to find the right URL to get the raw text link. For example, if you go to a GitHub link, like https://github.com/ngs-docs/2021-remote-computing-binder/blob/latest/2cities/README.md and you run curl on this you will get the formatted Web file, which isn’t generally what you want. What you want for GitHub files is the ‘Raw’ link - try clicking on that. This is now the text file, and you can use curl on it – curl -L -O https://github.com/ngs-docs/2021-remote-computing-binder/blob/latest/2cities/README.md Note here that the -L tells curl to follow Web redirects, which can be important; and -O says save the file under the name at the end of the URL. You can omit -O and it will send the file to stdout: curl -L https://github.com/ngs-docs/2021-remote-computing-binder/blob/latest/2cities/README.md &gt; new-file.txt cat new-file.txt The really nice thing about this is that for big files, the file will transfer directly between the hosting site and the remote computer. This is really handy for e.g. large sequencing data files that are located at sequencing facility Websites - you don’t have to download 100 GB files to your laptop and then transfer them from there to farm, or into the cloud, for example! (This is a big reason why cloud computing is really interesting for the NIH - less data transfer between distant computers for really big data sets!) Other than finding and copying the right URL, the other tricky thing that doesn’t generalize is permission-restricted files. Briefly, since you’re copying the URL from your browser (where you may be logged in) over to farm (where you may not be logged in), farm may not have access to download it. There is no one-size-fits-all solution to this, unfortunately. And, again, we’ll be covering retrieving files from github in other ways during workshop 8. CHALLENGE: Pick any book from https://www.gutenberg.org/ Find the URL for the plain text Copy the URL Use curl to download it to farm Look at it with head 6.4 Dealing with files: some recommendations Some short-hand rules I recommend for working with files on remote systems. download big files directly to remote computer system if possible, so that they don’t go via your home/work Internet connection or laptop (which is often more limited than the remote computer’s connection!); get used to transferring files to/from your laptop (see below); consider using a transfer/ directory for simplicity; for managing small files that you create on a remote system, use version control (workshop 8) you can also configure Dropbox on Linux systems, but it burdens the system and it’s also not a great idea to copy files that are probably private over to a shared system. I’ve also had some bad experiences with deleting my entire Dropbox by mistake… UNIX makes it a little too easy to operate on lots of files! 6.5 Farm vs cloud One of the main differences that you’ll see over time is that there are “remote, shared” systems like farm, and “remove, private” systems like binder and Amazon/Google cloud computers. The cloud computers often guarantee you resources and some level of privacy along with superuser privileges, but also charge you for it (and it’s often inefficient use of compute!) By contrast, “remote, shared” systems like HPCs and shared workstations can provide larger resources at the cost of sometimes having to worry about what other users are doing. The “ride share” vs “personal car” analogy is fairly apt here, actually :). In shared cars, you have to worry about where other people want to go and accomodate them at least sometimes, but you can share purchase and maintenance costs. With personal cars, you bear the entire cost burden, but you don’t have to coordinate with other people much. There are no simple answers as to what system to use, but I feel confident in asserting that if you have access to a shared compute cluster, you should start there and only consider expanding into the cloud once you know your needs. 6.6 Thinking about data science projects! The next sections all talk about configuring your data and directories so that you can track your work. This is because, fundamentally, we use computers to first explore data sets, before refining and producing. Figure 1 from Stoudt et al., 2021 So we need to organize our projects for iteration, dead-ends, and tracking processes over time - potentially months, or years. And that’s what the next few sections will be about. For more discussion and details, I highly recommend Principles for data analysis workflows, Stoudt et al., 2021, which is the best exploration of real data science practice I’ve yet seen. (Conflict alert: I was a reviewer :) 6.7 One example: a rough bioinformatics workflow a bioinformatics workflow 6.8 Sending and Receiving Data Here at UC Davis most researchers sequence at the UC Davis DNA Technologies Core. You can find their sample submission form here. When they’ve sequenced your samples they will hold your sequencing data on either SLIMS lab information management system for three months after the sequencing run finishes or bioshare for an undetermined amount of time. Do yourself a favor and download &amp; back your new sequencing data up to a hard disk IMMEDIATELY. 6.8.1 Downloading data - is it correct? We’ll be grabbing some from OSF, an open science framework that can host small amounts of data (5Gb limit). Let’s setup a directory to work from and download some data: mkdir -p ~/seqdata cd ~/seqdata curl -L https://osf.io/srdhb/download -o mini_Alca-torda.tar.gz tar -xvf mini_Alca-torda.tar.gz cd mini_A-torda ls -alh And we should see a list of Alca torda chromosomes. We got some data! However, the data could have been changed or corrupted in the process of downloading it from the Internet. (This CAN happen, and WILL happen if you do this for long enough!) We can address this problem by using the md5sum command. md5sum, in short, is a command that spits out a string of characters to represent a file’s fingerprint. If a file with the same name has characters that are different the md5sum will show the difference, so we don’t have to find the difference. This is exceptionally valuable when we have files that contain gigabytes of data. Let’s see what an md5sum looks like: md5sum mini-chr1.fna.gz you should see something exactly like: d34e2c570ef79195dfb56b8c258490f8 mini-chr1.fna.gz Here you can see a string of characters that represent the mini-chr1.fna.gz md5sum. The first string of characters is the file’s “fingerprint”. The second is the name of the file–this is useful when we md5sum multiple files. If you change even a single byte of the file, the md5sum fingerprint will change completely. So, We can check to make sure the data downloaded successfully by seeing if the string of characters generated by md5sum matches the ones in the @mini_A-torda.md5 file. First, take a look at the file: less \\@mini_A-torda.md5 (press Q to exit) We can check a list of md5sums contained in a file with the -c flag. The command will automatically look for files contained in the list and check the md5sum against the ones printed in the file. md5sum -c \\@mini_A-torda.md5 But if we navigate into the data/ directory we can see there is a mini-chr1.fna.gz file there too. Are they the same? CHALLENGE: Determine if the two mini-chr1.fna.gz files are the same. Anytime you download large data files you should check to make sure it has downloaded successfully. In bioinformatics, this includes raw sequence data from a sequencing center, data from collaborators host on a shared space, sequence files from NCBI, transferring data between devices, downloading data from clusters and so on and so forth. (Most sequencing centers will automatically provide md5sums with your data. If they do not, email to request them!) This holds true for other files as well - always be alert to the possibility of data corruption! Sometimes this can be as innocuous as “all the line endings in my CSV file are changed because someone opened and saved it in a different text editor”; sometimes it can be much worse, as when Excel started automagically changing gene names like “Oct-8” into dates… We can also make a list of md5sums for a group of files: cd ~/seqdata/mini_A-torda md5sum mini-chr[1-9]*.fna.gz &gt;&gt; autosomes.md5 Here, we’ve redirected the output of the md5sum command to the autosomes.md5 file. We can then host/send our data with the md5 file for others to ensure the recipients know they’ve received the same data. 6.9 Storing data In any data analysis, we’re going to end up producing a lot of intermediate files. Which ones do we need to save? And how big are they? The answers will vary depending on workflows. For bioinformatics (and potentially a range of other data types - your mileage may vary!) we suggest the following: 6.9.1 Bioinformatics: What do I back up? raw data – YES – you only get them once! results files – YES – they are so useful intermediate files – maybe – they can be used in many different ways That having been said, a lot of what we’ll show you over the next few workshops is how to regenerate your intermediate files when you need them, and/or keep them up to date. 6.9.2 Bioinformatics: How big should I expect the files to be? Raw data will probably be the biggest Results files depends on what you are doing Metadata are usually pretty small Intermediate files are usually smaller than your raw data files but there will be a lot of them Always, always back up raw data. Once the sequencing center deletes the data, it is gone forever! As such, make sure you’ve have your data backed up. As soon as you download onto the cluster back it up to another device (or two!). After you’ve finished your project you’ll probably have some pretty valuable results files. Back these up! It is likely that these files won’t be nearly as large as our original files and they can often be put on osf.io or downloaded to a laptop. Examples: file type size # of files Total Space Raw Genome Assembly sequencing files 44Gb 2 88Gb Intermediate files 12Gb 2 24Gb Assembled Genome ~550Mb 1 55Mb Raw RAD seq data 92Gb 2 184Gb Intermediate files from RADseq ~25Mb 20,925 741Gb Results of PopGen expt 9.3Mb 2 ~20Mb Data files will come in a variety of sizes. In general, the largest single files will be the raw data files you receive from the sequencing center. Individual files in each subsequent step will be smaller (but there might be more of them!) because each step throws out or compresses data. However, new files are generated at most steps in most projects. Because of this, the sheer number of intermediate files can take up a lot of space! 6.9.3 How often should I backup my data? Scripts and code: This varies person to person. I suggest posting your scripts to GitHub and to push to GitHub at least everyday you modify your scripts. (This is what we’ll teach in workshop 8!) GitHub will give you free private repositories as an academic, so you don’t need to make things public. Raw Data: Luckily, you’ll only need to backup your raw data once. Or maybe twice. Just don’t delete it! Results: Again, these only need to be backed up once. However, if you reanalyze your data make sure you save the new results. Or at least the workflow you used to get there. 6.9.4 Where do I back up my data? There are a number of places you can backup to, for better or for worse. Small amounts of data (&lt; 100 MB) that you would like version controlled, such as code, can be backed up to GitHub. Note that GitHub is not archival and cannot be used as the permanent post-publication place for your scripts, but there are solutions for that (search “github zenodo”). Small amounts of data (100 Mb - 5Gb) can be stored on a project by project basis at the Open Science Framework. Each project is allocated 5Gb of space and there is no limit on number of projects. OSF also supports private and collaborative projects, and can be referred to in publications, so you can use it as your “raw results” location too. Large amounts of data (&gt; 5Gb) The most reasonable solution to this is to back data up to a (or multiple) hard drives! Data can be stored via your Google Drive account. To store data using storage space that UC Davis has allocated to you, see this Meet &amp; Analyze Data tutorial. 6.10 Where do I work with large amounts of data? 6.10.1 High Performance Computing Clusters aka clusters (or HPCs). UC Davis has a number of clusters available to students, depending on your department or where your lab has purchased resources: farm crick barbera Pros: maintained on campus help desk for troubleshooting Cons: requires an initial buy in to get a significant amount of space &amp; compute 6.10.2 Amazon Web Service These are cloud computing services provided by Amazon (what don’t they provide!?) We’ll be showing you how to use Amazon for workshop 11. Pros: Only pay for what you use Can have temporary access to supercomputers which could end up costing less than consistent access on a cluster great for short-term massive compute needs, if you have the $$ Cons: no long term storage…unless you want to pay for that too must build everything yourself no help desk have to pay for downloading (“egress charges”) All platforms have pluses and minuses. The one that will work best for you depends on your lab. I prefer the farm, for ease of use &amp; consistency, if your lab can support you. 6.11 Setting up your project QUESTION Imagine you are at the end of a project, about to write up the manuscript. Looking back over carrying out your experiment(s), what were the top 3-5 most useful things to getting to the point of writing up? And how can you communicate those to future you (and others)? 6.11.1 Things to think about multiple projects inputs and outputs directory structure testing analyses In bioinformatics, organization is everything. It allows us to be efficient in setting up experiments, maintain consistency between experiments and, in some cases, repeat a previous analysis. Here we can see how organization allows for a tidy workspace: project paths The list of files up top isn’t too cringe worthy if you only have one project. But multiple projects or experiments in one directory results in an organizational disaster… paths I like to have the following directory structure: 6.12 Naming files Having files with consistent names allows us to do things to them en masse and can also allow us to know what has been done to them. Everyone has a different strategy in what they like their file names to contain and how they go about naming them but here a few suggestions. 6.12.0.1 Prefixes Use the beginning of your file to your advantage. If you’ve sequenced individuals yourself, you can name these files according any or all of the following: which individual they are from what well in the plate they came from the barcode/tag/library they have/were a part of the date of sampling which cohort they are a member of An example of this might be something like Ht_1997_A08_21_ACTGTT.fastq where: Ht = species ID (Hypomesus transpacificus) 1997 = birth year A08 = well number 21 = plate number ACTGTT = associated barcode Having some metadata in the file name can allow for efficient groupings when running quality controls (such as testing for batch effects). If we keep our names consistent between plates/runs could run an analysis on all individuals that were from any of these variables. And, as a reminder, with tab completion, long and ugly file names are not really a problem to type! 6.12.0.2 File endings The ends of files can be immensely helpful as well. Remember, at the command line, a file’s ending (e.g. .txt or .csv) exists to inform a human what kind of file/format to expect. You’ll see some examples of using this in automated scripts in workshop 7 and workshop 9. 6.13 Looking forward to the next few workshops: techniques for doing data science on remote computers. The discussion below motivates the next few workshops :). In workshop 7, we’ll talk about basic scripting and working with many files and how to conduct long-running analyses. In workshop 8, we’ll show you git and how to track changes to text files. In workshop 9, we’ll talk about using the snakemake workflow system to efficiently and effectively automate larger scale analyses. In workshop 10, we’ll discuss how to execute jobs using the SLURM system to tap into farm’s compute cluster. And, finally, in workshop 11 we will demonstrate the use of cloud compute resources as another source of remote computing. 6.14 Additional resources Bioinformatics Data Skills by Vince Buffalo Best Practices for Sci Comp (Wilson et al. 2014) Bioinformatics Curriculum Guidelines (Welch et al. 2014) "],["automating-your-analyses-and-executing-long-running-analyses-on-remote-computers.html", "7 Automating your analyses and executing long-running analyses on remote computers 7.1 What is a script? 7.2 Getting started 7.3 Automating commands by putting them in a text file 7.4 for Loops 7.5 Troubleshooting scripts 7.6 If statements 7.7 Persistent sessions with screen and tmux 7.8 Concluding thoughts 7.9 Appendix: exercise answers", " 7 Automating your analyses and executing long-running analyses on remote computers This two hour workshop will show attendees how to automate their analyses using shell scripts, as well as run and manage software that takes minutes, hours, or days to execute. We’ll also show you how to disconnect from and resume running processes using the ‘screen’ and ‘tmux’ commands. This lesson was adapted from a lesson co-authored by Shannon Joslin for GGG 298 at UC Davis. Learning objectives: Commands for, basename, echo, if How to write and execute shell scripts Learning how to use multiple sessions with screen for long-running analyses 7.1 What is a script? A script is like a recipe of commands for the computer to execute. We’re teaching you how to make shell scripts today, but scripts can be in any programming language (R, python, etc.). Why and when would we want to use scripts vs. typing commands directly at the terminal? Automate: don’t have to remember all the commands and type then one at a time Scale up: can use same script for multiple samples, multiple processes Reproduce &amp; share: easier to reproduce or share analyses because it’s all written down Version control: stay tuned for workshop 8! Note that scripts are especially helpful for processing many files with the same commands - but sometimes it’s not always worth the time/effort for an uncommon task. See xkcd comic - is it worth the time? :) 7.2 Getting started As per the instructions in workshop 3 and workshop 4, log into farm.cse.ucdavis.edu using your datalab-XX account. When you log in, your prompt should look like this: (base) datalab-09@farm:~$ If it doesn’t, please alert a TA and we will help you out! 7.3 Automating commands by putting them in a text file 7.3.1 Running scripts with bash At the terminal, we can type: echo Hello, this is the terminal! In a script, we can do the same thing - (we covered how to create and edit files with nano from Workshop 2!): Create a script file with nano - The file extension for shell scripts is ‘.sh’. nano first_script.sh Add the following 3 lines to the script: #!/bin/bash echo Hello, this is a script! echo I am on the next line! The #!/bin/bash header (this is known as a “sha-bang” or hashbang) tells the shell how to interpret the script file. It will be used later! Execute the script bash first_script.sh Note that commands are executed in the order that they appear in the script 7.4 for Loops Scripts can do far more than print echo statements! We’re gonna take a detour to learn about for loops and then run loops in scripts. In workshop 6, we showed you a way to create a list of the md5sum numbers for the autosome files: cd ~/seqdata/mini_A-torda md5sum mini-chr[1-9]*.fna.gz &gt;&gt; autosomes.md5 This approach uses wildcards to tell the shell to grab the md5sum for all files starting with mini-chr, with a number from 1 to 9, and ending with ‘.fna.gz’. Another way to do this, and include the Z chromosome file as well, is to write a for loop that runs the md5sum command for each ‘.fna.gz’ file in the directory: (To type this in the terminal, type ‘enter’ or ‘return’ after each line) for i in *.fna.gz do echo md5sum $i done QUESTION: why did we put echo here? Another way to enter the for loop code into the terminal uses ;: for i in *.fna.gz; do echo md5sum $i; done for loop structure: we set the counter for the thing we want to iterate (“loop”) through with the for i in *.fna.gz. In this case, we are running the same command for each file in our current directory that ends in ‘.fna.gz’. The i represents the ith file in our loop and we refer to it with the $ notation (more on variables later!) - also, “i” is an arbitrary name; it could be “potato” :) starts with do and ends with done loop components are separated by ; or new lines. We have used indentation to make it easier to read the for loops in the notes, but the shell does NOT need indentation to interpret the loop! Note that other programming languages like Python do require indentation! Now, let’s append those md5sum numbers to a text file for i in *.fna.gz; do md5sum $i &gt;&gt; my_md5sum_list.txt; done Reminder: The &gt;&gt; are appending the md5sum values to 1 text file. Check out the list (exit by pressing q) less my_md5sum_list.txt Now, let’s practice for loops by renaming MiSeq sequence file names - we’re going to build a for loop step by step. Go to this directory: ~/2021-remote-computing-binder/data/MiSeq: cd ~/2021-remote-computing-binder/data/MiSeq and then type for i in *.fastq do echo $i done This is running the command echo for every value of the variable i, which is set (one by one) to all the values in the expression ’*.fastq’. If we want to get rid of the extension ‘.fastq’, we can use the basename command: for i in *.fastq do basename $i .fastq done Now, this doesn’t actually rename the files - it just prints out the name, with the suffix ‘.fastq’ removed. To rename the files, we need to capture the new name in a variable: for i in *.fastq do newname=$(basename $i .fastq).fq echo $newname done What $( ... ) does is run the command in the middle, and then replace the $( ) with the output of running the command. This output is assigned to the variable “newname”. Side note: you may see backticks used instead of $(...). It does the same thing but the syntax is trickier to get right, so we teach $(...) instead of `...`. Note that $( ... ) can be nested, to, so you can do $( command $( command2 )) which is occasionally handy. Now we have the old name ($i) and the new name ($newname) and we’re ready to write the rename command – for i in *.fastq do newname=$(basename $i .fastq).fq echo mv $i $newname done CHALLENGE: Run the above loop in a shell script called rename_file.sh. Now that we’re pretty sure it all looks good, let’s run it for realz - the shell script should look like this: #!/bin/bash for i in *.fastq do newname=$(basename $i .fastq).fq mv $i $newname done 7.4.1 Subsetting Let’s do something quite useful - subset a bunch of FASTQ files. If you look at one of the FASTQ files with head, head F3D0_S188_L001_R1_001.fq you’ll see that it’s full of FASTQ sequencing records. Often I want to run a bioinformatics pipeline on some small set of records first, before running it on the full set, just to make sure all the syntax for all the commands work (“data forensics”). So I’d like to subset all of these files without modifying the originals. First, let’s make sure the originals are read-only chmod u-w *.fq Now, let’s make a ‘subset’ directory mkdir subset Now, to subset each file, we want to run a ‘head’ with an argument that is the total number of lines we want to take. In this case, it should be a multiple of 4, because FASTQ records have 4 lines each. Let’s take the first 100 records of each file by using head -400. The for loop will now look something like: for i in *.fq do echo &quot;head -400 $i &gt; subset/$i&quot; done QUESTION: We need to use \" \" for the echo statement above - why? If that command looks right, run it for realz: for i in *.fq do head -400 $i &gt; subset/$i done and voila, you have your subsets! We can check the number of lines for all the subset files: wc -l ./subset/* (This is incredibly useful. You have no idea :) CHALLENGE: Can you rename all of your files in subset/ to have ‘subset.fq’ at the end? 7.4.2 Variables Let’s backtrack a bit - what are variables? We’ve seen 2 examples of variables so far - $i and $newname: for i in *.fastq do newname=$(basename $i .fastq).fq mv $i $newname done You can use either $varname or ${varname}. The latter is useful when you want to construct a new filename, e.g. MY${varname}SUBSET would expand ${varname} and then put “MY” and “SUBSET” on either end, while MY$varnameSUBSET would try to put “MY” in front of $varnameSUBSET which won’t work - unknown/uncreated variables are evaluated to empty by default, so this would just expand to MY. We recommend always using ${name} instead of $name, because it always works the way you expect, unlike $name, which can be confusing when constructing new filenames as above. NOTE: ${varname} is quite different from $(expression)! The former is replaced by the value assigned to varname; the latter is replaced by the result of running expression. So, both replace but they do different things. Think of $ here as meaning, “replace me with something”. 7.5 Troubleshooting scripts As we’ve seen above, the echo statements help to make sure the commands look correct before running for real. There are several set options that are useful to determine what happens to your script on failure. We recommend: Always put set -e at the top. Sometimes put set -x at the top. 7.5.1 Practicing set -e in bash scripts We’re going to use the MiSeq .fq files again. Create an output report directory cd ~/2021-remote-computing-binder/data mkdir ./MiSeq/fastqc_reports Create and activate a conda environment that has fastqc installed in it (see workshop 5 notes on conda): mamba create -n fqc -y fastqc conda activate fqc Write a for loop that runs fastqc on each .fq files with a shell script. Create a bash script using nano text editor (save and exit with CTRL-O, enter, CTRL-X on keyboard) nano set_e.sh Create a bash script with the following commands, this version includes set -e: #!/bin/bash set -e OUTDIR=&#39;fastqc_reports&#39; for i in ./MiSeq/*.fq do echo $i fastqc $i -o $OUTDIR done Reminder: Another way to type bash for loops is with the ;, for example this syntax does the same thing as above: for i in ./MiSeq/*.fq; do echo $i; fastqc $i -o $OUTDIR; done This command runs the script: bash set_e.sh CHALLENGE What happens when you run the bash script above with and without the set -e option? There is an error in the bash script. How would you fix the script? (Bonus: try adding set -x to your bash script) 7.6 If statements If statements act on things conditionally. For example, you might want to do something if a file exists and a different thing if the file doesn’t. In other words, if statements evaluate outputs as True or False and use the output to decide what action to take - it’s like a decision tree. (Note that conditional operators in Unix are not all the same as in other programming languages) if statement structure: starts with if and ends with fi loop components are separated by ; or indentation Here, we’re wrapping if statements in a for loop: cd ~/ nano if-for.sh Put this loop in the if-for.sh script file: for i in * do if [ -f $i ]; then echo $i is a file elif [ -d $i ]; then echo $i is a directory fi done (the version of above loop that uses the ; separators) for i in *; do if [ -f $i ]; then echo $i is a file; elif [ -d $i ]; then echo $i is a directory; fi; done but what the heck is this [ ] notation? That’s actually running the ‘test’ command; try help test | less to see the docs. This is a weird syntax that lets you do all sorts of useful things with files – I usually use it to get rid of empty files. touch emptyfile.txt to create an empty file, and then: for i in * do if [ \\! -s $i ]; then echo rm $i fi done …and as you can see here, I’m using ‘!’ to say ‘not’. (Why do I need to put a backslash in front of it, though??) (-s tests if a file exists and is not empty) 7.6.1 Running scripts in a loop We can run loops in scripts AND scripts in loops! Say we have an ifs.sh script that compares 2 numbers with an if statement: #!/bin/bash a=40 b=20 if [ $a != $b ] then echo &#39;a is not equal to b!&#39; else echo &#39;a is equal to b!&#39; fi Run the script: bash ifs.sh QUESTION: What does the != conditional operator mean? Now, let’s edit this script to give it arguments. Instead of editing the values for “a” and “b” in the script, we’ll create “a” and “b” arguments so we can change them when executing the script. Here’s how we change the script - $ and the number assigns the argument a position in the line of code. #!/bin/bash a=$1 b=$2 if [ $a != $b ] then echo &#39;a is not equal to b!&#39; else echo &#39;a is equal to b!&#39; fi After the bash &lt;script name&gt;, the syntax now assigns the 1st element ($1) to 40 and the 2nd element ($2) to 20. This means you can enter different numbers when executing the script, without needing to edit the script file at all! bash ifs.sh 40 20 Note, you can add echo statements to the script to remind you what the arguments are. This is often helpful for troubleshooting and building the script, for example: #!/bin/bash echo running $0 echo a will be $1 echo b will be $2 a=$1 b=$2 if [ $a != $b ] then echo &#39;a is not equal to b!&#39; else echo &#39;a is equal to b!&#39; fi CHALLENGE: How might you use this script in a for loop to compare a range of numbers to one number? For example, suppose you wanted to check the $2 parameter against the numbers 20 30 40 50 60 70 to see if it matched one of them? 7.7 Persistent sessions with screen and tmux What if you want to run multiple scripts at once, or you want to put your computer to sleep to check later without stopping analyses that take a long time to complete? There are 2 programs, screen and tmux, that allow you to create separate terminal screens that can continue to run in the background (as long as you don’t turn your computer off!). If you’re running these programs on the Farm, you can logout and even turn your computer off. :) We’ll get back to these details in workshop 10. These are a bit tricky to get used too, so we’ll do a demo. Basic commands for screen and tmux below. They both have keyboard shortcuts as well (screen cheat sheet). Description screen tmux start a screen session screen -S &lt;session name&gt; tmux new -s &lt;session name&gt; close a session screen -d &lt;session name&gt; tmux detach list existing sessions screen -ls tmux ls go to existing session screen -r &lt;session name&gt; tmux attach -t &lt;session name&gt; end session exit tmux kill-session -t &lt;session name&gt; Like text editors, both programs basically do the same thing - choose the one you’re most comfortable using! There are several reasons to use screen or tmux – they keep output from long-running commands, including ones that are running interactively and need input; they provide a way to “detach” from a particular shell prompt with a particular configuration, and resume it later; they let you switch between terminal windows that are running on two different computers. 7.8 Concluding thoughts Break the task down into multiple commands Put commands in shell scripts, run in serial Automate and scale up using for loops and conditional statements Use echo and set -e to debug! We’ll return to the concept of using scripts to execute analysis workflows in workshops 9 (Snakemake) and 10 (Using SLURM on HPC). 7.9 Appendix: exercise answers Answers for questions Why do we use echo in for loops? echo prints out the command without running it; this is a good way to double-check the for loop is doing what you expect! Why did we need ” ” in the subset echo statement for loop? In this case, the shell will evaluate the echo statement as everything in the double-quotes. Without the quotes, the echo statement will send “head -400 $i” to a file in the subset directory; it will not run the subset command properly. What does the != conditional operator mean? This means “not equal to”. The “!” means “not”. Whereas, “==” means “equal to”. Answer for subset exercise for i in *.fq; do echo &quot;head -400 $i &gt; subset/$i&quot;; newname=$(basename $i .fq)subset.fq; echo mv subset/$i subset/$newname; done Answers for set -e exercises Fails on 1st iteration with set -e, fails each iteration of the loop without set -e Output with set -e: (base) ~$ bash set_e.sh ./MiSeq/F3D0_S188_L001_R1_001.fq Specified output directory &#39;fastqc_reports&#39; does not exist Output without set -e: (base) ~$ bash set_e.sh ./MiSeq/F3D0_S188_L001_R1_001.fq Specified output directory &#39;fastqc_reports&#39; does not exist ./MiSeq/F3D0_S188_L001_R2_001.fq Specified output directory &#39;fastqc_reports&#39; does not exist ./MiSeq/F3D141_S207_L001_R1_001.fq Specified output directory &#39;fastqc_reports&#39; does not exist ./MiSeq/F3D141_S207_L001_R2_001.fq Specified output directory &#39;fastqc_reports&#39; does not exist ... Add set -x option to print out the commands computer is running. There’s an error in the path to save FastQC output reports. # wrong path OUTDIR=&#39;fastqc_reports&#39; # correct path OUTDIR=&#39;./data/fastqc_reports&#39; Answer for ifs.sh in a loop exercise This is one approach to compare 20 30 40 50 60 70 to the $2 argument: for i in 20 30 40 50 60 70 do bash ifs.sh $i 40 done This output (including the helpful echo statements), looks like this: a will be 20 b will be 40 a is not equal to b! a will be 30 b will be 40 a is not equal to b! a will be 40 b will be 40 a is equal to b! a will be 50 b will be 40 a is not equal to b! a will be 60 b will be 40 a is not equal to b! a will be 70 b will be 40 a is not equal to b! This is one approach to compare a range of numbers to the $2 argument: for i in {1..5} do bash ifs.sh $i 5 done The output (including the helpful echo statements) will look like this: a will be 1 b will be 5 a is not equal to b! a will be 2 b will be 5 a is not equal to b! a will be 3 b will be 5 a is not equal to b! a will be 4 b will be 5 a is not equal to b! a will be 5 b will be 5 a is equal to b "],["keeping-track-of-your-files-with-version-control.html", "8 Keeping Track of Your Files with Version Control 8.1 Learning Objectives 8.2 What is git? 8.3 What is GitHub? 8.4 Using git 8.5 Challenge Question 1 8.6 Revisiting the Workflow 8.7 Undoing Changes 8.8 Challenge Question 2 8.9 Working Collaboratively 8.10 Challenge Question 3 8.11 Odds and Ends 8.12 Additional Resources", " 8 Keeping Track of Your Files with Version Control This two hour workshop will show attendees how to use the git version control system to track changes to files, back up files to GitHub, and sync files between your laptop/desktop and a remote server. We will also discuss ways to collaborate with a team. This lesson was adapted from several UC Davis sources: Shannon Joslin’s lesson from GGG 298 DataLab’s workshops on git and git for teams Nick Ulle’s lesson from STA 141B 8.1 Learning Objectives By the end of this lesson, students will be able to: setup git repositories (on GitHub or a local machine) add and track changes to files with git add save changes by committing them with git commit undo changes with git revert and git restore resolve merge conflicts 8.2 What is git? git is a version control system. A version control system tracks the changes you make to files, so that you can go back to a previous version at any time. It’s like Microsoft Word’s change tracking on steroids. Besides version control, git can also be used for collaborating with others. git works in terms of repositories and commits. A repository is a directory (including its subdirectories) that’s managed by git. A commit is a checkpoint or savepoint for the files in a repository. git keeps track of every commit you make, so you can go back to older commits whenever you want. This diagram shows the overall workflow for git: All git commands begin with git followed by the name of a subcommand. The arrows on the diagram represent several of the subcommands we’ll learn about in this workshop, and we’ll revisit the diagram after learning a few. In git jargon, the local computer is the computer where you’re running git commands. The local computer will usually be your laptop or desktop, but for this workshop, the local computer will be Farm. A remote computer (or just “remote”) is any other computer online. The remote computer will usually be a server like GitHub, which we’ll learn more about in the next section. git takes practice to learn, like anything else. This lesson will teach you everything you need to know to get started with using git to track changes to your own files. Collaborating with others is harder, and this workshop will only cover the basics. Keep an eye out for future DataLab workshops to learn more :) 8.3 What is GitHub? We’re going to introduce you to git through GitHub, a website used to store, share, and collaborate on git repositories. You don’t have to use GitHub or its competitors (such as GitLab and BitBucket) in order to use git, but doing so provides a convenient way to: back up your repo look at your changesets share your software with others (including both future you and your lab/advisor) 8.3.1 Create a GitHub Account Please go to GitHub and create a free account. You’ll need to choose a username (this is public, and kind of like a social media handle, so choose wisely :) and a password (something you can remember and type!). As of writing this, GitHub’s free accounts allow unlimited private repositories with up to three collaborators, and unlimited public repositories. You can apply for an academic account or upgrade your account if you want more collaborators on private repositories. 8.3.2 Create a New Repository Let’s use GitHub to create a new repository. The steps are: Navigate to GitHub. Click on the plus sign in the upper right of the screen. Select “New repository” from the drop-down menu. On the “Create a new repository” page, choose a name for your repository. For this workshop, name the repository 2021-my-first-repo. A repository can have any valid directory name, but putting the year at the beginning is a good practice because it makes it clear when the repo was created. Check the box “Add a README file”. Click the green “Create repository” button at the bottom of the page. Here’s a screenshot of the “Create new repository” page: After a few seconds, you should be redirected to a web page with a URL like https://github.com/USERNAME/2021-my-first-repo, where USERNAME is replaced by your GitHub username. This is your new repository’s URL. Unless you selected “Private” on the “Create a new repository” page, your new repository is public. That means anyone can see and copy files you put in the repository (but only you and people you grant permission to can edit the repository). 8.4 Using git 8.4.1 Set up git on Farm Now, log in to Farm, or start up a Binder and go to the terminal. The git config command changes git’s configuration. The first time you use git on a computer (such as Farm), it’s a good idea to configure git with your name and email address. Make sure to use the same email address you used to sign up for GitHub, so that GitHub will recognize your work. The commands to set your name and email address are: git config --global user.name &quot;Your Name&quot; git config --global user.email you@example.com If you decide to use git on your laptop or desktop, or get a new Farm account after this workshop, you’ll need to run these commands again. 8.4.2 Optional: Set up a Password Helper You’ll have to type in your password each time you want to make changes, unless you do this: git config --global credential.helper &#39;cache --timeout=7200&#39; By default git will cache your password for 15 minutes. Here, the timeout parameter increases this to two hours. 8.4.3 Clone the Repository In order to work with your new repository, you first need to download it to Farm. The git clone command downloads a repository from a remote server for the first time. Go to the GitHub repository you created, click the green “Code” button in the top right corner, and copy the HTTPS URL shown. Then change to your home directory on Farm (cd ~), type git clone, and paste the URL you copied. The command should look like this: git clone https://github.com/USERNAME/2021-my-first-repo Press enter to run the command. This will create a directory 2021-my-first-repo in your home directory. Change into the new directory and take a look around: cd 2021-my-first-repo ls -a You’ll notice two files: A hidden .git subdirectory, which git uses to keep track of changes you make to the repo A README.md file, which GitHub created when you checked the “Add a README file” box. This file is also displayed on the GitHub page for your repository. 8.4.4 Edit a File Let’s edit the README.md file. You can use nano on Farm or RStudio on binder: nano README.md Add a new line like Hello, git!, then save the file and exit. Now let’s see if git recognizes the changes. The git status command checks the status of a git repo. In the terminal, enter: git status You should see the following message: On branch main Your branch is up to date with &#39;origin/main&#39;. Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: README.md no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) This is telling you a few things: README.md has been modified. This is the most important part! Your copy of the repository is up to date with origin/main, which is what git calls GitHub’s copy of the repository. We’ll revisit what origin/main means later. Commands to add changes you plan to keep (“commit”), or to undo changes you decided don’t want to keep. You’ll learn more about these commands later. Before you can make a commit, you have to tell git which changes you want to include in the commit. You do this by adding changes to git’s staging area (or index) with the git add command. You can think of the staging area as a box you’re packing up to get ready to store. Add your changes to README.md to the staging area by entering this command: git add README.md Now check the status: git status The status should now say that your changes to README.md are “staged for commit”. They are now in the staging area, but haven’t been committed yet. Before making a commit, it’s a good habit to review the changes in the staging area. This is especially important if you’re working with multiple files. The git diff command shows differences between your working directory and previous commits. You can use git diff with the argument --staged to compare the staging area to the most recent commit (GitHub made a commit when you created the repo). Run this command: git diff --staged You should see something like: diff --git a/README.md b/README.md index 8d2d4d8..0a92250 100644 --- a/README.md +++ b/README.md @@ -1 +1,4 @@ -# 2021-remotecompute-test \\ No newline at end of file +# 2021-my-first-repo + +Hello, git! + For each file you changed, the git diff command shows lines you added prefixed with + (in green) and lines you removed prefixed with - (in red). So the message above is telling you that you changed one file (README.md), and that you changed three lines. Two are blank lines and one is the text you added to the file. If you edited the file differently, you’ll see a different message, but you should see the changes you made. 8.4.5 Commit a File Now let’s commit the changes. The git commit command creates a commit. When you create a commit, git will ask you to write a one-sentence commit message to describe your changes. It’s a good habit to write a concise, informative commit message to remind yourself and your collaborators of what changed. If you run git commit in the terminal without any other arguments, git will open an editor (usually vi) for you to enter and save your commit message. You can use the -m argument to enter your commit message directly into the terminal (without opening an editor). Make sure to surround the message in quotes. Enter this in the terminal: git commit -m &quot;Added a line to README.&quot; git should reply with something like this: [main 9bf6695] added info to README 1 file changed, 3 insertions(+), 1 deletion(-) Here 9bf6695 is the commit’s ID. Your commit will have a different ID, and we’ll learn more about how to use the ID later. What if you run git status now? git status You should see: On branch main Your branch is ahead of &#39;origin/main&#39; by 1 commit. (use &quot;git push&quot; to publish your local commits) nothing to commit, working tree clean This tells you there are no new, uncommitted changes in your repo (because you just committed the changes). It also tells you that your repo is now out of sync with GitHub (origin/main) by 1 commit. In other words, GitHub does not yet have the commit you just made. Let’s send the commit to GitHub. The command to send a commit from the local repo to a remote repo is git push. Run the command now: git push git will ask for a username and a password. Enter your GitHub username and password. You’ll see this error message: remote: Support for password authentication was removed on August 13, 2021. Please use a personal access token instead. fatal: unable to access &#39;https://github.com/s-canchi/2021-remotecompute-workshop8.git/&#39;: The requested URL returned error: 403 Most remote git repos accept a username and password, but as of August 13, 2021, GitHub does not. 8.4.5.1 Set up a Personal Access Token Github disabled passwords in favor of personal access tokens (PAT) and SSH keys. Follow these steps on GitHub to generate a PAT: Navigate to GitHub Click on your user icon in the top right corner Select “Settings” from the drop-down menu On the left panel of the page that opens, click on “Developer settings” On the left panel of the page that opens, click on “Personal access tokens” Click the “Generate new token” button. On the “New personal access token” page, give your token a name in the “Note” field to help you remember when and why you created the token. Check the “repo” scope so the token can push to repos. Click the green “Generate token” button at the bottom of the page. On the “Personal access tokens” page, copy the new token (in the green box). The token will no longer be viewable once you navigate away from this page. Save the token somewhere safe (for example, a password manager). If you want to learn more about GitHub’s PAT authentication, read our detailed tutorial. Now let’s try pushing the commit again. Run this command in the terminal: git push When prompted, enter your GitHub username and the PAT token as your password. Pushing commits to a remote repository can take a moment. If the command finishes running successfully, you should see something like this: To https://github.com/s-canchi/2021-my-first-repo.git ad7e8de..9bf6695 main -&gt; main This tells you that git pushed all of your commits up to and including commit 9bf6695 to your GitHub repo. Now if you go to the GitHub page for your repo, you should see your changes to the README.md file. 8.4.6 View the Repository History on GitHub On the GitHub page for your repo, click on the “2 commits” link on the right side of the blue box. You’ll see two commit messages: one “initial commit” from when you created the repository, and one with the commit message you just wrote. They’ll be ordered from most to least recent. Click on the commit message you just wrote. You’ll see a colored diff that shows what you changed. If you click on the ... in the right corner, you can view the version of the file that was saved in this commit. This is one way to view old versions of files in your repository (you can do this for any commit). 8.5 Challenge Question 1 For this challenge: Go back to the terminal on Farm. Make some more changes to the README.md file. Use git add to add your changes. Inspect your changes with git status and git diff. Once you’re happy with your changes, commit them with git commit. Verify the changes were committed with git status and git diff. Push your commit to GitHub with git push. Open the GitHub page for your repo and verify that the changes have been made. Take a look at the diff for the change. Voila! 8.6 Revisiting the Workflow Let’s revisit the git workflow diagram from earlier: A typical git workflow is: Clone a repo from a server (like GitHub) with git clone. This downloads from remote to local. Make some changes to your local copy of the repo. Stage your changes with git add. Commit your changes to the repo’s history with git commit. Push the changes on your computer back to the server with git push. This uploads from local to remote. Repeat 2-5 as many times as you like until finished. There are lots of steps in this process, so there are lots of places where it can go wrong. Pay attention to error messages and search online if you get stuck. Lots of people use git, and your question has probably been asked and answered :) Which files should you add to git? In general, add text, scripts, metadata to git, but not the results produced by the scripts. If your repository follows best practices for reproducibility, it should be possible to regenerate the results anytime they’re needed. Of course, it’s okay to keep uncommitted result files in your local repo as you work. Also avoid committing any large data files (&gt; 1 MB), as these will slow down git (and GitHub has limits on total storage for free users). How often should you make commits? This varies from person to person and team to team. If you have collaborators that already use git, ask them about their workflow. Otherwise, it’s good practice to make a commit every time you complete a small task (a natural checkpoint in your work) or decide to stop working for the day. The remainder of this workshop is about how to undo changes (including commits) and the bare minimum you need to know use git collaboratively. 8.7 Undoing Changes Confusingly, git has four different subcommands for undoing things: restore revert reset checkout The two you’re most likely to use in day to day work are git restore and git revert. We’ll cover those two in detail, and briefly touch on how they relate to the other two. 8.7.1 Restoring a File The git restore command is useful if you want to undo changes you haven’t committed yet. Open up the README.md file in a text editor (such as nano) and make some changes. For instance, let’s add the line This is a mistake to the end of the file. Then save and close the file. Next, check the repository status: git status As expected, git tells us there are changes to the README.md file: On branch main Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: README.md no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) What if now you decide the changes were a mistake and you don’t want to keep them? The git restore command restores a file in the working directory to the version of the file in the most recent commit. Let’s try out restoring the README.md file: git restore README.md Now check the status again: git status The status shows that there are no longer any new changes to the README.md file: On branch main nothing to commit, working tree clean There is no way to undo git restore, so be careful when you’re using it. In older versions of git, the command to restore a file in the working directory was git checkout -- FILENAME. You may occasionally see people mention this command online, and it still works in recent versions of git. However, the git restore command is preferable because it disambiguates what you’re trying to do (git checkout is also used for other things unrelated to restoring files). 8.7.1.1 Restoring the Staging Area With a different set of arguments, you can also use the git restore command to remove changes from the staging area. Open up the README.md file in a text editor again and make some more changes. This time let’s add the line Git is tough to the end of the file. As usual, save and close the file. Next, add the changes to the staging area: git add README.md Open the file in a text editor one more time, and add the line This is a great sentence to the end of the file. Save and close the file. Then check the repository status with git status: Changes to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: README.md Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: README.md git tells us that some of the changes to README.md are staged (from when we ran git add earlier) and some of them aren’t. Now suppose you decide you want to unstage the changes to README.md. This removes the changes from the staging area, but not from the working directory. Unstaging changes is especially useful when you’re working with multiple files and accidentally add a file you don’t want to commit yet. To unstage a file, use git restore with the --staged argument: git restore --staged README.md Then check the status again: On branch main Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: README.md no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) The changes to README.md are still in the working directory, but no longer in the staging area. In older versions of git, the command to restore a file in the staging area was git reset FILENAME. You may occasionally see people mention this command online, and it still works in recent versions of git. However, the git restore command is preferable because it disambiguates what you’re trying to do (git reset is also used for other things unrelated to restoring the staging area). 8.7.2 Reverting a Commit The git revert command is useful if you want to undo changes that you’ve already committed. Once a commit is in your repository’s history, it generally stays there forever. There is a way to delete commits, but you should really only delete commits if you accidentally commit sensitive data). Instead, the idiomatic way to undo a commit is to create a new commit that reverses the changes. Let’s start by editing the README.md file again and making a commit. Add the line This is a big mistake. to the end of the file. Then run: git add README.md git commit -m &quot;This commit is a mistake.&quot; Now suppose you decide the commit is a mistake, and want to undo it. First, you need to find the ID of the commit you want to undo. The git log command opens a scrolling list of all commits in the repository’s history and their IDs. Run the command: git log You can exit the log by typing q. In the log, locate the the mistaken commit, and copy or remember the first 5 digits of its ID. git is smart enough that it can generally recognize a commit from the first few digits of its ID, and will tell you if it needs more digits for disambiguation. In my repo, the ID of the commit starts with e01d1. In your repo, the commit will likely have a different ID. Next, run this command, replacing the ID with the ID you copied: git revert e01d1 --no-edit The --no-edit flag tells git revert to generate the commit message for the new commit automatically. Without the flag, git revert will prompt you to enter a commit message. Now inspect the file with nano or cat. You should see that the changes from your bad commit are gone. If you look in the log with git log, you’ll also see a new commit to revert the changes of a previous commit. 8.8 Challenge Question 2 The goal of this challenge is to make a bad commit and then revert it. Work through these steps: Change a file in your repository. add and commit the changes. Find the ID of the commit from step 2 in the repository history. revert the commit from step 2. 8.9 Working Collaboratively 8.9.1 Editing on GitHub You can edit on GitHub directly! This is a great way to fix little typos and use a friendly editor, but it’s a bit clunky for day to day work. To edit a file on GitHub via the Web: Go to your file on GitHub. Click the little edit button. Add some text. Commit changes. Yay, the changes are now in the history! Unfortunately, the changes aren’t in your local repository yet. You can download the changes by running: git pull Et voila! Hopefully this will be easy to remember: push sends commits from your local repo to a remote, and pull retrieves commits from a remote to your local repo. The git pull command can also retrieve commits made by someone else (such as a collaborator) as long as the commits are in the repository on GitHub. 8.9.2 Merge Conflicts When you work on a GitHub repository with other people, they might change a file, commit the changes, and then push their commit to GitHub. Your local copy of the file won’t change unless you pull the new commit from GitHub. In other words, your local repository can easily get out of sync with the remote repository on GitHub. If you then change your local copy of the file and commit the changes, you create a conflict. If you try to push the conflicting commit to GitHub, you’ll see an error message: git push To github.com:USERNAME/REPOSITORY.git ! [rejected] main -&gt; main (fetch first) error: failed to push some refs to &#39;git@github.com:USERNAME/REPOSITORY.git&#39; hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: (e.g., &#39;git pull ...&#39;) before pushing again. hint: See the &#39;Note about fast-forwards&#39; in &#39;git push --help&#39; for details. When you see an error, don’t panic! The error message hints that you should try pulling commits from GitHub before pushing your commit. If you pull commits from GitHub, you might see another error message: git pull remote: Counting objects: 3, done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), done. From github.com:USERNAME/REPOSITORY 6fe289c..48e44d3 main -&gt; origin/main Auto-merging README.md CONFLICT (content): Merge conflict in README.md Automatic merge failed; fix conflicts and then commit the result. This is okay! Git tried to automatically fix the conflict by merging your commit with the other person’s commit, but couldn’t figure out how because both commits changed the same file (README.md in the example). An automatic merge will only succeed if the commits being merged changed different files. Otherwise, it’s up to you to resolve the conflict manually. If you open the file causing the conflict in a text editor, you’ll see something like this: # Our README.md &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Here are the changes you made. ======= Here are the changes the other person made. &gt;&gt;&gt;&gt;&gt;&gt;&gt; 48e44d3a60af614f3a0da794a1701d040221d40f Here&#39;s some text that was added to the file in an earlier commit. Git automatically marked which parts of the file conflict. Changes from your commit are shown between &lt;&lt;&lt;&lt;&lt;&lt;&lt; and =======. Changes from the other person’s commit are shown between ======= and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. All you need to do is edit the file to look the way you want. If you wanted to keep your changes and the other person’s changes (the polite thing to do), you could edit the file to look like this: # Our README.md Here are the changes you made. Here are the changes the other person made. Here&#39;s some text that was added to the file in an earlier commit. When you’re done editing, save and then commit the file. This is called a merge commit. Git will automatically provide a commit message indicating that you merged your commit with the other person’s commit: [main 9594c15] Merge branch main of github.com:USERNAME/REPOSITORY Finally, you can push your commit along with the merge commit to GitHub: git push Counting objects: 6, done. Delta compression using up to 4 threads. Compressing objects: 100% (2/2), done. Writing objects: 100% (6/6), 602 bytes | 0 bytes/s, done. Total 6 (delta 0), reused 0 (delta 0) To github.com:USERNAME/REPOSITORY.git 48e44d3..9594c15 main -&gt; main Note that if you pull and git asks you to merge a file, but you’d like to undo the pull and make more changes before merging, you can use the command git merge --abort. Git will remind you about “unmerged paths” in the git status message when it’s waiting for you to merge a file: git status On branch main Your branch and &#39;origin/main&#39; have diverged, and have 1 and 1 different commits each, respectively. (use &quot;git pull&quot; to merge the remote branch into yours) You have unmerged paths. (fix conflicts and run &quot;git commit&quot;) (use &quot;git merge --abort&quot; to abort the merge) Unmerged paths: (use &quot;git add &lt;file&gt;...&quot; to mark resolution) both modified: README.md no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) You can generally avoid merge conflicts by communicating with your collaborators, but it’s useful to know how to resolve them when they happen. 8.10 Challenge Question 3 For this final challenge question, you’re going to create and then fix a merge conflict. Here are the steps to work through: Edit the README.md file on GitHub and make a commit. Don’t pull the commit to your local repo yet. Edit the README.md file on your local repo and make a commit. Try pushing your commit (you’ll get an error message). Run git pull and resolve the merge conflict by editing README.md. Run git commit to save the resolved changes. Push the merge commit back to GitHub and check that it appears in the history there. 8.11 Odds and Ends 8.11.1 Ignoring Files with .gitignore Sometimes a repository contains files you never want to commit. For instance, you should generally commit text, code, and other files that generate outputs, but not the actual outputs. As another example, you generally shouldn’t commit large data sets. Over time, these files you never commit will start to clutter up your git status output. You can tell git to ignore specific files by creating a new file called .gitignore at the top level of the repository. The .gitignore file is a list of files to ignore, with one line per file. You can use the * character as a wildcard. For instance, suppose you want git to ignore all PDF files in your repository. Open .gitignore with a text editor and add the line: *.pdf After you set up your .gitignore file, add and commit it. Remember you can always change .gitignore as time goes on by editing it and making a new commit. It’s good practice to keep git status clean, showing only the stuff that you’re interested in tracking. 8.11.2 Setting up a Repository without GitHub So far you have learned a git workflow that relies on GitHub for creating new repositories. What if you have an existing directory with files that you now want to track using git? This section will show you how to set that up. First, make a directory for the new repository: mkdir workshop_scripts cd workshop_scripts Next, initialize the directory as a git repository: git init Now the repository is set up. You can add and commit files as usual. 8.11.2.1 Pushing to GitHub If you later decide you want to use the repository with GitHub, here’s how to do it. First, set up a remote for the repository on GitHub. Follow the steps in the Create a New Repository section of this reader, but leave the box to “Add a README file” unchecked. You can name the new repository anything you want, but it’s idiomatic to use the same name as the directory that contains the repository. Next, use the git remote command to tell git to use the new GitHub repository as a remote: git remote add origin https://github.com/USERNAME/workshop_scripts.git Now your repository is aware of the remote on GitHub. The first time you push commits, you’ll also have to link your local repository’s main branch to the remote repository’s main branch: git push --set-upstream origin main 8.12 Additional Resources Check out Daniel Standage’s blog post on using github to collaborate with yourself for inspiration! Handy git cheatsheet from GitHub Mark Lodato’s visual reference on git Roger Dudler’s simple guide to git Git-it which provides step by step tutorial with challenge exercises "],["automating-your-analyses-with-the-snakemake-workflow-system.html", "9 Automating your analyses with the snakemake workflow system 9.1 What is a workflow and why use one? 9.2 Snakemake: A workflow management system 9.3 Getting started - logging into farm! 9.4 Installing snakemake 9.5 More setup 9.6 RNA-Seq workflow we will automate 9.7 First step: quality control with FASTQC 9.8 Some features of workflows 9.9 Making the rules more generic 9.10 Wildcards 9.11 Adding more rules 9.12 Titus’ version of the final snakefile as created during the workshop 9.13 Random aside: --dry-run or -n 9.14 Advanced features 9.15 Practical advice: How to build your workflow 9.16 Summary of what we did today. 9.17 More Snakemake resources 9.18 A quick checklist:", " 9 Automating your analyses with the snakemake workflow system This ~2.5 hour workshop will introduce you to the snakemake workflow system, for executing large-scale automated analyses. By the end of this lecture, we will: know how to make basic workflows in snakemake understand variable substitution in snakemake rules understand wildcard matching in snakemake rules be introduced to reasons why workflow systems can help you do your computing more easily 9.1 What is a workflow and why use one? A workflow is a series of sequential tasks that need to be completed in order to reach a goal. Workflows are ubiquitous! Making pizza is a workflow! Many things in bioinformatics are workflows. Every bioinformatics workflow consists of multiple steps, taking previous outputs (data or information) in and executing upon them and outputting something. Raw data goes in, results come out! 9.2 Snakemake: A workflow management system Snakemake is a commonly used workflow system created by Johannes Koester and others (see 2012 publication). Many other workflow systems exist. E.g. nextflow, Common Workflow Language (CWL), and Workflow Definition Language (WDL). Are all good! Each workflow system comes with its own syntax and set of advantages. They are all here to make computational methods reproducible and shareable. (If you work in R a lot, you might be especially interested in drake – which is now called Targets.) Today we’re going to talk about ways of automating workflows using snakemake. 9.2.1 Fun fact The name ‘snakemake’ comes from the fact that it’s written in (and can be extended by) the Python programming language. (Confusingly, Python is actually named after Monty Python, not the reptiles.) 9.2.2 The Snakefile Snakemake works by looking at a file called the Snakefile. Snakefile contains recipes (or rules) for running tasks and creating files. Each rule is defined as a step in the workflow. Snakemake uses the rules and command line options to figure out how the rules relate to each other so it can manage the workflow steps. 9.3 Getting started - logging into farm! As per the instructions in workshop 3 and workshop 4, log into farm.cse.ucdavis.edu using your datalab-XX account. When you log in, your prompt should look like this: (base) datalab-01@farm:~$ If it doesn’t, please alert a TA and we will help you out! 9.4 Installing snakemake We will use conda to install snakemake-minimal. You have conda pre-installed from workshop 4. We will install snakemake inside a conda environment called “snakemake” conda create -y --name snakemake snakemake-minimal This command makes a new environment called “snakemake” and installs snakemake in it! Here, snakemake-minimal is just the stuff needed to run snakemake, without some extra bells and whistles. Activate the environment with this command: conda activate snakemake Check the version of snakemake with snakemake --version As of August 2021, the snakemake version is 6.7.0; yours should be that version or later. Next, add two bioinformatics software to the snakemake environment: fastqc and salmon conda install -y fastqc salmon These are two packages that we will use for bioinformatics work. 9.5 More setup 9.5.1 Create a working directory Create a working directory called snakemake_lesson mkdir -p ~/snakemake_lesson cd ~/snakemake_lesson 9.5.2 Download some data curl -L https://osf.io/5daup/download -o ERR458493.fastq.gz curl -L https://osf.io/8rvh5/download -o ERR458494.fastq.gz curl -L https://osf.io/xju4a/download -o ERR458500.fastq.gz curl -L https://osf.io/nmqe6/download -o ERR458501.fastq.gz You should now have four files in your current directory, representing four sequencing experiments. Now we’re all set! 9.6 RNA-Seq workflow we will automate 9.7 First step: quality control with FASTQC ERR458493.fastq.gz is a fastq file that contains RNA-Seq data from one sample called ERR458493. We can do some quality control to see how good the sequence is by running it through a program called fastqc, like this: fastqc ERR458493.fastq.gz This command should produce two output files: ERR458493_fastqc.html and ERR458493_fastqc.zip So this is a pretty simple bioinformatics task, but let’s use this task as the start of our snakemake workflow! Remove output: rm ERR458493_fastqc.zip rm ERR458493_fastqc.html 9.7.1 Create a Snakefile Create a new file and call it “Snakefile” nano Snakefile Copy and paste this text into the Snakefile: rule all: input: &quot;ERR458493_fastqc.html&quot;, &quot;ERR458493_fastqc.zip&quot; rule make_fastqc: input: &quot;ERR458493.fastq.gz&quot; output: &quot;ERR458493_fastqc.html&quot;, &quot;ERR458493_fastqc.zip&quot; shell: &quot;fastqc ERR458493.fastq.gz&quot; Save and close. To run the snakefile, type: snakemake -p -j 1 It worked! There is a html file and a zip file. Let’s explore the logic of what happened: Each rule tells Snakemake how to do something. The first rule (in this case called “all”) is the rule run by default, so we are asking snakemake to create the two target files ERR458493_fastqc.html and ERR458493_fastqc.zip. Snakemake first looks at the directory to see if the target files are there. (They’re not!) Snakemake then looks at the rest of the rules one at a time (in this case, there’s only one!) to see if it can figure out how to make the target files. The make_fastqc rule says, “if this input exists, you can run the provided shell command to make that output”. So snakemake complies! Here, the “input:” in the rule all has to match the “output” in the rule make_fastqc or else Snakefile wouldn’t know what to make. Meta-notes: Snakefile contains a snakemake workflow definition The rules specify steps in the workflow You can “decorate” the rules to tell snakemake how they depend on each other. Rules can be in any order, but put “default goals” as first Information within rules such as input and output (and other things) can be in any order, as long as they are before shell. These are just shell commands, with a bit of “decoration”. You could run them yourself if you wanted! Rule names are arbitrary (letters, numbers, _) You can specify a subset of outputs, e.g. just the .html file, and snakemake will run the rule even if it only needs one of the files. It goes all red if it fails! (try breaking one command :) It’s all case sensitive. Tabs and spacing matter! You could use the -ET4 flag in nano to make the editor treat tabs as 4 spaces. If you see syntax error messages, always check your tabs first. Replacing tabs with spaces could fix the problem! You can make lists for multiple input or output files by separating filenames with a comma. 9.8 Some features of workflows If you run snakemake -p -j 1 again, it won’t do anything. That’s because all of the input files for the first rule already exist! However, if you remove a file and run snakemake rm ERR458493_fastqc.html snakemake -p -j 1 Then snakemake will run fastqc again, because now you don’t have one of the files you’re asking it to make! This ability to selectively figure out whether or not to run a command is one of the most convenient features of snakemake. 9.8.1 What are these flags (-p, -j)? -p or --printshellcmd: Print out the shell commands that will be executed. -j or --jobs: Use at most N CPU cluster/cloud jobs in parallel. 9.8.2 When you run snakemake, by default, it runs the first rule. How can we run a different rule? Specifying the rule name, tells snakemake to run that specific rule: snakemake -p -j 1 make_fastqc Specifying the output file you want, tells snakemake to run the rule that produces the desired output file: snakemake -p -j 1 ERR458493_fastqc.html 9.9 Making the rules more generic Let’s make the make_fastqc rule a little more generic. Edit the file and make the rule look like this: rule make_fastqc: input: &quot;ERR458493.fastq.gz&quot; output: &quot;ERR458493_fastqc.html&quot;, &quot;ERR458493_fastqc.zip&quot; shell: &quot;fastqc {input}&quot; It replaces the {input} with whatever is in the “input:” line, above. CHALLENGE: Add a new rule, called make_fastqc2, that runs fastqc on ERR458501.fastq.gz Does it run? Reminder: add the desired output file to the “all” rule as an input, too! 9.10 Wildcards You should now have two rules: 1) make_fastqc and 2) make_fastqc2, They have the same shell command but they have different inputs and outputs: one has “ERR458493.fastq.gz” as an input, and “ERR458493_fastqc.html” and “ERR458493_fastqc.zip” as outputs, while the other has “ERR458501.fastq.gz” as an input, and “ERR458501_fastqc.html” and “ERR458501_fastqc.zip” as outputs. If you line these up, you’ll notice something interesting: ERR458493.fastq.gz ERR458493_fastqc.html ERR458493_fastqc.zip ^^^^^^^^^ ERR458501.fastq.gz ERR458501_fastqc.html ERR458501_fastqc.zip ^^^^^^^^^ We can make use of this commonality by adding a wild card! We will tell snakemake that any time it is asked for a file that ends with _fastqc.html or _fastqc.zip, it should look for a similarly named file that ends with .fastq.gz. If it finds one, it can run fastqc on that file to produce those outputs. Change the make_fastqc rule: the input: is “{sample}.fastq.gz” the output is “{sample}_fastqc.html”, “{sample}_fastqc.zip” delete the make_fastqc2 rule! Your complete Snakefile should look like this: rule all: input: &quot;ERR458493_fastqc.html&quot;, &quot;ERR458493_fastqc.zip&quot;, &quot;ERR458501_fastqc.html&quot;, &quot;ERR458501_fastqc.zip&quot; rule make_fastqc: input: &quot;{sample}.fastq.gz&quot; output: &quot;{sample}_fastqc.html&quot;, &quot;{sample}_fastqc.zip&quot; shell: &quot;fastqc {input}&quot; Let’s try it! rm *.html snakemake -p -j 1 Please note: wildcards operate within a single rule, not across rules. CHALLENGE: Update the Snakefile so that it runs fastqc on “ERR458494.fastq.gz” and “ERR458500.fastq.gz” too. 9.11 Adding more rules Now let’s add some more rules at the bottom. For our desired workflow, we need to do three things: 1) Download a reference transcriptome 2) Index the reference transcriptome 3) Quantify the reference genes based on the reads (using salmon) 9.11.1 Downloading the reference genome The download_reference shell command is: curl -L -O https://downloads.yeastgenome.org/sequence/S288C_reference/orf_dna/orf_coding.fasta.gz and it creates a local file orf_coding.fasta.gz. (Note that you can always run the command at the prompt if you want to make sure that it works, and to find out what the output filename is!) Add the appropriate rule to the Snakefile - it should look like this: rule all: input: &quot;ERR458493_fastqc.html&quot;, &quot;ERR458493_fastqc.zip&quot;, &quot;ERR458501_fastqc.html&quot;, &quot;ERR458501_fastqc.zip&quot;, &quot;orf_coding.fasta.gz&quot; rule make_fastqc: input: &quot;{sample}.fastq.gz&quot;, output: &quot;{sample}_fastqc.html&quot;, &quot;{sample}_fastqc.zip&quot; shell: &quot;fastqc {input}&quot; rule download_reference: output: &quot;orf_coding.fasta.gz&quot; shell: &quot;curl -L -O https://downloads.yeastgenome.org/sequence/S288C_reference/orf_dna/orf_coding.fasta.gz&quot; 9.11.2 Add the index genome command: The index reference shell command is: salmon index --index yeast_orfs --transcripts orf_coding.fasta.gz This is what the snakefile should look like: rule all: input: &quot;ERR458493_fastqc.html&quot;, &quot;ERR458493_fastqc.zip&quot;, &quot;ERR458501_fastqc.html&quot;, &quot;ERR458501_fastqc.zip&quot;, &quot;orf_coding.fasta.gz&quot;, directory(&quot;yeast_orfs&quot;) rule make_fastqc: input: &quot;{sample}.fastq.gz&quot;, output: &quot;{sample}_fastqc.html&quot;, &quot;{sample}_fastqc.zip&quot; shell: &quot;fastqc {input}&quot; rule download_reference: output: &quot;orf_coding.fasta.gz&quot; shell: &quot;curl -L -O https://downloads.yeastgenome.org/sequence/S288C_reference/orf_dna/orf_coding.fasta.gz&quot; rule index_reference: input: &quot;orf_coding.fasta.gz&quot; output: directory(&quot;yeast_orfs&quot;) shell: &quot;salmon index --index yeast_orfs --transcripts {input}&quot; BUT if you try to run snakemake -p -j 1 then it won’t run… we have to specify the rule to run: snakemake -p -j 1 download_reference snakemake -p -j 1 index_reference CHALLENGE: Modify the snakefile such that ALL the rules run when you type snakemake -p -j 1 9.11.3 Running Salmon quant The next command we want to snakemake-ify for all samples is this command: salmon quant -i yeast_orfs --libType U -r ERR458493.fastq.gz -o ERR458493.fastq.gz.quant --seqBias --gcBias Let’s put it in a snakemake rule called salmon_quant rule salmon_quant: shell: &quot;salmon quant -i yeast_orfs --libType U -r ERR458493.fastq.gz -o ERR458493.fastq.gz.quant --seqBias --gcBias&quot; and then let’s decorate with input and output: rule salmon_quant: input: &quot;ERR458493.fastq.gz&quot; output: directory(&quot;ERR458493.fastq.gz.quant&quot;) shell: &quot;salmon quant -i yeast_orfs --libType U -r ERR458493.fastq.gz -o ERR458493.fastq.gz.quant --seqBias --gcBias&quot; Next, replace the filename with wildcards: rule salmon_quant: input: &quot;{sample}.fastq.gz&quot; output: directory(&quot;{sample}.quant&quot;) shell: &quot;salmon quant -i yeast_orfs --libType U -r {input} -o {output} --seqBias --gcBias&quot; Snakemake doesn’t automatically look at all the files in the directory and figure out which ones it can apply rules to - you have to ask it more specifically, by asking for the specific files you want. CHALLENGE: make the command snakemake run with no target rules for all four salmon quant commands. 9.11.4 One version of the final Snakefile rule all: input: &quot;ERR458493_fastqc.html&quot;, &quot;ERR458493_fastqc.zip&quot;, &quot;ERR458501_fastqc.html&quot;, &quot;ERR458501_fastqc.zip&quot;, &quot;ERR458500_fastqc.html&quot;, &quot;ERR458500_fastqc.zip&quot;, &quot;ERR458494_fastqc.html&quot;, &quot;ERR458494_fastqc.zip&quot;, &quot;orf_coding.fasta.gz&quot;, &quot;yeast_orfs&quot;, &quot;ERR458493.fastq.gz.quant&quot;, &quot;ERR458501.fastq.gz.quant&quot;, &quot;ERR458494.fastq.gz.quant&quot;, &quot;ERR458500.fastq.gz.quant&quot; rule make_fastqc: input: &quot;{sample}.fastq.gz&quot;, output: &quot;{sample}_fastqc.html&quot;, &quot;{sample}_fastqc.zip&quot; shell: &quot;fastqc {input}&quot; rule download_reference: output: &quot;orf_coding.fasta.gz&quot; shell: &quot;curl -L -O https://downloads.yeastgenome.org/sequence/S288C_reference/orf_dna/orf_coding.fasta.gz&quot; rule index_reference: input: &quot;orf_coding.fasta.gz&quot; output: directory(&quot;yeast_orfs&quot;) shell: &quot;salmon index --index yeast_orfs --transcripts {input}&quot; rule salmon_quant: input: &quot;{sample}&quot; output: directory(&quot;{sample}.quant&quot;) shell: &quot;salmon quant -i yeast_orfs --libType U -r {input} -o {output} --seqBias --gcBias&quot; 9.12 Titus’ version of the final snakefile as created during the workshop SAMPLES=[&quot;ERR458493&quot;, &quot;ERR458501&quot;, &quot;ERR458494&quot;, &quot;ERR458500&quot;] print(&#39;samples are:&#39;, SAMPLES) rule all: input: expand(&quot;{sample}_fastqc.html&quot;, sample=SAMPLES), &quot;orf_coding.fasta.gz&quot;, &quot;yeast_orfs&quot;, expand(&quot;{sample}.quant&quot;, sample=SAMPLES), rule make_fastqc: input: &quot;{sample}.fastq.gz&quot;, output: &quot;{sample}_fastqc.html&quot;, &quot;{sample}_fastqc.zip&quot; shell: &quot;fastqc {input}&quot; rule download_reference: output: &quot;orf_coding.fasta.gz&quot; shell: &quot;curl -L -O https://downloads.yeastgenome.org/sequence/S288C_reference/orf_dna/orf_coding.fasta.gz&quot; rule index_reference: input: &quot;orf_coding.fasta.gz&quot; output: directory(&quot;yeast_orfs&quot;) shell: &quot;salmon index --index yeast_orfs --transcripts {input}&quot; rule salmon_quant: input: fastq = &quot;{sample}.fastq.gz&quot;, index = &quot;yeast_orfs&quot; output: directory(&quot;{sample}.quant&quot;) shell: &quot;salmon quant -i {input.index} --libType U -r {input.fastq} -o {output} --seqBias --gcBias&quot; 9.13 Random aside: --dry-run or -n If you give snakemake a --dry-run (-n) parameter, it will tell you what it thinks it should run but won’t actually run it. This is useful for situations where you don’t know what needs to be run and want to find out without actually running it. 9.14 Advanced features There are many advanced features to snakemake, and we’ll touch on a few of them here. 9.14.1 Rule-specific conda environments with conda: and --use-conda If you specify a conda environment file, in a conda: block in a rule, and run snakemake with --use-conda, it will always run that rule in that software environment. This is useful when you want to version-pin software a specific action, and/or have conflicting software in different rules. See Making and using environment files for more information on conda environment files! 9.14.2 parallelizing snakemake: -j You can tell snakemake to run things in parallel by doing snakemake -j 2 This will tell snakemake that it can run up to two jobs at a time. (It automatically figures out which jobs can be run at the same time by looking at the workflow graph.) 9.15 Practical advice: How to build your workflow General advice: Start small, grow your snakefile! DO copy and paste from this tutorial and others you find online! It rarely hurts to just re-run snakemake until it does nothing but error out, and then analyze that error :) 9.15.1 Approach 1: write down your shell commands Pick a small, linear workflow, and then: Make rules for each of your shell commands, and run them individually; Add input and output to each rule until you can “just” run the last rule and have it all work; start adding wildcards as you see fit! 9.15.2 Approach 2: automate one step that you run a lot Alternatively, if you have a complex workflow that would take a lot of time and energy to convert, pick a specific part that you would like to run on a lot of files! 9.16 Summary of what we did today. Snakefiles contain rules Snakemake uses those rules to figure out what files to build The basic idea is simple, but there are lots of tricks that we will teach you! 9.17 More Snakemake resources Google is your friend! CFDE Snakemake lesson The first three 201(b) class materials are a fairly gentle introduction a free book! – the Snakemake book ANGUS 2019 material – Workflow Management using Snakemake ### Dealing with complexity Workflows can get really complicated; here, for example, is one for our most recent paper. But it’s all just using the building blocks that I showed you above! If you want to see some good examples of how to build nice, clean, simple-looking workflows, check out this RNAseq example. Debugging Q: What if I get a workflow error? If you get the error WorkflowError: Target rules may not contain wildcards. Please specify concrete files or a rule without wildcards. then what’s happening is you’re trying to directly run a rule with a wildcard in it (i.e. an abstract rule), and snakemake can’t figure out what the wildcard should be; instead, ask snakemake to build a specific file. For example, with the rule immediately above that adds wildcards to salmon_quant, snakemake -p -j 1 salmon_quant will complain about the target rule containing wildcards. You should instead run snakemake ERR458493.fastq.gz.quant which snakemake can use to figure out what the wildcards should be. An alternative to specifying the file on the command line is to put it in the default rule, e.g. rule all: (see the section on default rules in GGG 201(b)) and then you can run snakemake. We can use the -n gives a dry run of the Snakefile. For example snakemake -p -j 1 -n 9.18 A quick checklist: Are you asking snakemake to create a specific file? either by: executing snakemake or by specifying a rule that has an input or an output without wildcards, or by providing a default rule for any rule that you expect to be executed automatically (because some other rule needs its output), have you specified output:? "],["executing-large-analyses-on-hpc-clusters-with-slurm.html", "10 Executing large analyses on HPC clusters with slurm 10.1 What is a cluster? 10.2 How do clusters work? 10.3 Some useful tips and tricks for happy slurm-ing 10.4 More on resources and queues and sharing 10.5 What we’ve shown you today. 10.6 Some final thoughts before departing farm and moving into the cloud.", " 10 Executing large analyses on HPC clusters with slurm This two hour workshop will introduce attendees to the slurm system for using, queuing and scheduling analyses on high performance compute clusters. We will also cover cluster computing concepts and talk about how to estimate the compute resources you need and measure how much you’ve used. This lesson was originally written by Shannon Joslin for GGG 298 at UC Davis (see original lesson). 10.1 What is a cluster? A cluster can be thought of as a group of computers which work together to allow you to perform memory intensive functions. Clusters are accessed by logging onto one computer (head node) and resources (other computers) are acquired by asking for resources from job schedulers. Image modified from vrlab 10.2 How do clusters work? 10.2.1 Job Schedulers In order to carry out commands that are memory intensive we need to use auxiliary computers that will not affect the login/head node, which is usually shared by many people. NOTE: sometimes merely copying large files is memory intensive enough that we will need to (or should) use computers other than the head node! To request resources to run our scripts we use job schedulers. Job schedulers handle how to allocate the compute cluster’s resources to batch job scripts submitted by users. There are a number of different flavors of job schedulers. The job scheduler you will be submitting jobs to is specific to the cluster you are using at your institution but they all have the following general structure: The job scheduler evaluates when resources will be dedicated to a job based on the: partition &amp; priority (-p) how much of the group’s resources are already being used requested wall time (-t) requested resources memory (--mem) CPUs (-c) The Slurm workload manager is an open source workload manager that is commonly used on compute clusters (both farm and barbera at UC Davis use Slurm). It handles allocating resources requested by batch scripts. There are two main ways you can request resources using Slurm: 10.2.2 EITHER: run an interactive session with srun Interactive sessions allow you to work on computers that aren’t the login/head node. Essentially you can do everything you’ve done at the command line interface on the compute cluster. This is really powerful for doing memory intensive commands that you may not need to keep track of. However, with this power comes a great danger! Why is it dangerous? This is an interactive session, so the commands you run will not be saved in scripts anywhere. So, if you wanted to go back and recreate an analysis, you won’t know what you’ve run, how you’ve run it or which versions of software you used. To request and launch a basic interactive session that will last for 10 minutes, use the following: srun --partition high2 --time=00:10:00 --pty /bin/bash If successful, you should see something like: srun: job 37969118 queued and waiting for resources srun: job 37969118 has been allocated resources You’ll also see that your prompt changes. (base) datalab-09@c6-92:~$ This is because you’re no longer on farm - you’re in a shell on one of the cluster nodes. (Here, c6-92 is the hostname of one of the cluster nodes!) OK, so what’s going on here? In order to handle jobs, Slurm needs to know where to run it, and the maximum amount of walltime your job will run. This is so that it can properly allocate a computer to handle your job and (try to) predict when those resources will be available for others. With the --partition flag, you’re telling srun to use a particular partition, or subset of nodes, that is for high priority jobs - we’ll talk more about this below! The --time flag specifies the walltime you’re asking for. Walltime is literally “the time shown on the clock on the wall”, and can be measured as the maximum expected amount of time from the start of your job to the end of the job. Pay close attention to the time you give to yourself here! Slurm will terminate the session immediately at the end of the allotted time. Sadly, it doesn’t care (or know) if you are 99.99% of the way through your analysis :/ The default resources that you’re allocated are: 2 GB of RAM 1 CPU and we’ll show you how to figure that out for yourself, below, using squeue. You can request more or different resources by using the following flags: --mem=&lt;number&gt;Gb = request a certain amount of memory -c &lt;number&gt; = request a certain number of CPUs Here, the -c flag is the same number you would use for snakemake -j to run many things in parallel; see Automating your analyses with the snakemake workflow system for the snakemake lesson. (We’ll show you how to run snakemake inside of slurm below!) If your prompt doesn’t have farm in it - that is, if you’re logged into a different computer in an srun session - you should type exit now to get back to the head node: exit This will exit the shell. If you are on farm head node already, this will just log you out of your ssh session and you should log back in ;). CHALLENGE: on the farm head node, set yourself up for a 5 second session using srun. What happens when the five seconds are up? 10.2.3 OR: Submit batch scripts with sbatch The alternative to srun is sbatch, which is my preferred way (and the suggested way!) to run batch job scripts, for several reasons. Batch job scripts (also known as job scripts) are scripts that contain #! /bin/bash at the beginning of each script and are submitted to the slurm workload manager by using sbatch. They are (mostly) just shell scripts, with a few exceptions. We can use the same commands that we would use at the command line within our sbatch scripts. (For more info on bash scripts, see Automating your analyses and executing long-running analyses on remote computers.) First, to try out sbatch let’s create a script called HelloWorld.sh. mkdir -p ~/slurm-lesson cd ~/slurm-lesson nano HelloWorld.sh Then copy and paste the following: #! /bin/bash echo Hello World sleep 15 date Then exit nano with Ctrl+X Try running it: bash HelloWorld.sh what does it do? We can submit this script to Slurm with the sbatch command. sbatch HelloWorld.sh but we receive an error message… sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified As with srun, above, we must tell Slurm how much time to allow our submitted script by using the -t/--time flag, and what queue to run it on with -p/--partition. Let’s tell Slurm that our job shouldn’t take longer than 5 minutes (note: the format is dd-hh:mm:ss, or days-hours:minutes:seconds), and to use the high2 partition of the cluster. sbatch -t 00-00:05:00 -p high2 HelloWorld.sh You will see your job was successfully submitted and will be given an associated Job ID number: Submitted batch job 15219016 but with a different number. This is a unique job ID that you can use to monitor (with squeue) and cancel your job (with scancel). See Monitoring your jobs with squeue, below. 10.2.4 Flags to use when submitting jobs with sbatch or srun We can use a number of different flags to specify resources we want from Slurm (we’ll cover how to measure these in Measuring your resource usage). the partition we would like to use for our job – this will also dictate the priority with which our job is run. This is heavily cluster and account dependent; on farm, your datalab-XX accounts have access to high2, med2, and low2, for example, which let you run on CPU-intensive nodes at high, medium, and low priority. See Partitions, below. the memory required to run our job. We can request a specified amount of memory with the following flag: --mem=&lt;number&gt;Gb we can have slurm e-mail us updates about our job, such as when it starts(BEGIN), ends(END), if it fails(FAIL) or all of the above (ALL). There are many other mail-type arguments: REQUEUE, ALL, TIME_LIMIT, TIME_LIMIT_90 (reached 90 percent of time limit), TIME_LIMIT_80 (reached 80 percent of time limit), TIME_LIMIT_50 (reached 50 percent of time limit) and ARRAY_TASKS. We can request slurm emails us with the following flags: --mail-user=&lt;your_email&gt; --mail-type=&lt;argument&gt; we can also give jobs specific names. To name your job use: -J &lt;job_name&gt; Be careful, as there is a limit to the number of characters your job name can be. slurm automatically records stdout output where all of the output from commands run from the script are printed to. These will take the form as slurm-12345.out where 12345 is an identifying number (the job ID, by default!) slurm assigns to the file. We can change this to any output file name we want. To specify the name of your output file use -o &lt;file_name&gt;.out slurm can record stderr output, where all of the errors from the script are printed to. We can ask slurm to create err files and name them with -e &lt;file_name&gt;.err If we were being mean to ourselves we would write these out at the command line each time we submitted a job to slurm with sbatch. It would look something like this: sbatch --time=01-02:03:04 -p high2 --mem=4Gb --mail-user=&lt;your_email&gt; --mail-type=ALL -J &lt;job_name&gt; -o &lt;file_name&gt;.out -e &lt;file_name&gt;.err (We would need to switch out all of the &lt;text&gt; with parameters specific to our preference, but hopefully you get the gist.) But we can make this much easier on ourselves! Typing all of the parameters out on the command line every time we want to submit a batch script is annoying and it also doesn’t allow us to record what parameters we used easily. We can instead put the parameters to run each job in the script we submit to slurm! This also has the advantage of supporting repeatability! 10.2.5 Repeatability through SBATCH variables in shell scripts One of the most important things in computational science is repeatability - can I run exactly the same thing today as I did yesterday? You’d think this would be straightforward, but it is exceptionally easy to run a series of command on data, leave the data for a few months (or years) and come back to the data and have no clue how you went from point A to point Z. (Just to be clear, this is bad. :) Let’s say we lost everything except our backed up raw data and we needed to recreate an analysis. In the worst case, where the commands used to carry out the experiment were not saved, we would have to figure out all of the commands with only a vague memory of the steps we took to get results. It is hard, if not impossible to recreate an analysis with exactly the same string of commands and parameters. So, we should think about documenting things as we go. In the best case (of this terrible scenario) we would have a script to recreate our analysis! So, we can make this easy for our future forgetful-selves and put all of the flags and commands we submit to Slurm INSIDE our batch scripts! We can do this by adding #SBATCH lines of code after the sha-bang line (#! /bin/bash) in our script. Let’s save the parameters we used to run HelloWorld.sh within the batch script instead of specifying them on the command line. Modify your HelloWorld.sh script to look like the following: #! /bin/bash # #SBATCH --mail-user=&lt;email&gt;@ucdavis.edu # YOUR EMAIL ADDRESS #SBATCH --mail-type=ALL # NOTIFICATIONS OF SLURM JOB STATUS - ALL, NONE, BEGIN, END, FAIL, REQUEUE #SBATCH -J HelloWorld # JOB ID #SBATCH -e HelloWorld.j%j.err # STANDARD ERROR FILE TO WRITE TO #SBATCH -o HelloWorld.j%j.out # STANDARD OUTPUT FILE TO WRITE TO #SBATCH -c 1 # NUMBER OF PROCESSORS PER TASK #SBATCH --mem=1Gb # MEMORY POOL TO ALL CORES #SBATCH --time=00-00:05:00 # REQUESTED WALL TIME #SBATCH -p high2 # PARTITION TO SUBMIT TO echo Hello World sleep 15 date (Make sure to replace your &lt;email&gt; with your UC Davis email address.) Then submit your script: sbatch HelloWorld.sh Once our scripts start running, we should see the following files in our current directory: HelloWorld.j#######.err and HelloWorld.j#######.out. These are the recorded outputs from running your batch job with slurm! 10.2.6 Reprise: running HelloWorld.sh via srun Of course, even with all the SBATCH stuff in the script, it’s still just a shell script that you can run at the command line; the only command that pays attention to the #SBATCH lines is sbatch itself! This is useful for debugging. Sometimes you’ll have to debug a script that isn’t working the way you want it to, and you’ll want to do this interactively. If you can use the head node for this, great - but sometimes the problem is that the script uses resources that aren’t available for the head node, OR (even worse) that the script runs fine on the head node, but NOT via sbatch. In this case, you can use srun to allocate yourself a node, and then go run the script yourself. CHALLENGE: Use srun to allocate a node, and then run HelloWorld.sh. Question: do you get the e-mails and the output files as with sbatch? (No, because when you’re running the script via bash inside an srun, slurm doesn’t know it’s a special script.) 10.2.7 Choosing between srun and sbatch I use srun when I want to explore or debug something, or need an interactive prompt. Sometimes I’ll use it when I want to run something at a high priority and don’t want to have to write a batch script, but I almost always regret it when I do this. (Be better than me!) I almost always prefer sbatch. There are a bunch of reasons - the script is a text file, so I can correct it if I get something wrong! the script specifies the resources at the top, so I can edit those easily! I can comment the script so I can understand it later. I can use version control to track changes to my sbatch scripts (see Keeping Track of Your Files with Version Control) I can run one (or a dozen) sbatch scripts at various priorities, and can be notified by e-mail when they’re done. This lets me walk away from the computer :) 10.2.8 A stock sbatch script that includes activating a conda environment Here’s the sbatch script I usually start with. It has a few nice features: it lists the parameters that I usually end up modifying (-c, -t, --mem) it supports conda environment activation (see Installing software on remote computers with conda) it prints out the resources I actually used at the end! (See Measuring your resource usage below) #!/bin/bash -login #SBATCH -p med2 # use &#39;med2&#39; partition for medium priority #SBATCH -J myjob # name for job #SBATCH -c 1 # 1 core #SBATCH -t 1:00:00 # ask for an hour, max #SBATCH --mem=2000 # memory (2000 mb = 2gb) #SBATCH --mail-type=ALL #SBATCH --mail-user=&lt;email&gt;@ucdavis.edu # initialize conda . ~/miniconda3/etc/profile.d/conda.sh # activate your desired conda environment conda activate base # fail on weird errors set -e set -x ### YOUR COMMANDS GO HERE ### # for example, sleep 15 ### YOUR COMMANDS GO HERE ### # Print out values of the current jobs SLURM environment variables env | grep SLURM # Print out final statistics about resource use before job exits scontrol show job ${SLURM_JOB_ID} sstat --format &#39;JobID,MaxRSS,AveCPU&#39; -P ${SLURM_JOB_ID}.batch We’ll talk a bit more about the choices made in this script, below, when we talk about choosing your CPU and memory. See Measuring your resource usage, below. But first, let’s cover… 10.3 Some useful tips and tricks for happy slurm-ing 10.3.1 Trick 1: running srun inside of a screen. Back in Automating your analyses and executing long-running analyses on remote computers, we introduced you to Persistent sessions with screen and tmux. If you are using srun to run commands, it is just like any other interactive shell - if you close your laptop, or your network is disconnected, you’ll terminate the shell. So I often use screen to make my srun sessions resilient to laptop closure and shell termination. There’s one key trick here: run screen first, then run srun. (I’ll show a demo, you don’t need to follow along - just know that this is a possibility.) 10.3.2 Trick 2: running snakemake inside of an sbatch script. In our previous workshop, we introduced you to Automating your analyses with the snakemake workflow system. You can use snakemake inside of an srun or sbatch script! CHALLENGE: Try using srun to run the following commands: conda activate snakemake cd ~/snakemake_lesson rm *.zip *.html snakemake -j 1 How would you run this with more CPUs? Hint: you need to modify BOTH your srun command AND your snakemake command. How would you modify the sbatch script in A stock sbatch script that includes activating a conda environment to run this in an sbatch environment? 10.3.3 Monitoring your jobs with squeue Oftentimes we submit jobs and would like to know certain things about them – if they’ve started, how long they’ve been running, if they are still running, etc, etc… We can look at the status of any job Slurm is handling by using squeue If we type squeue then we see many rows of jobs… JOBID PARTITION NAME USER ST TIME NODES CPU MIN_ME NODELIST(REASON) 15218450 bmh this_is_ keyu1996 CG 31:10 1 2 100G bm3 15219413 bmh pigz aminio CG 0:01 1 8 4G bm2 15108157_[34-4 bigmemm mapping gfburgue PD 0:00 1 8 200G (Resources) 14204771_[1182 med freebaye eoziolor PD 0:00 1 4 2000M (AssocGrpCpuLimit) 15217722_[7-23 bmm trim hansvgdu PD 0:00 1 2 10G (JobArrayTaskLimit) 15113687 bigmemm AA_ophiu jgillung PD 0:00 1 24 200G (Priority) 15144078 bigmemm NT_ophiu jgillung PD 0:00 1 24 200G (Priority) 15144205 bigmemm AA_plant jgillung PD 0:00 1 24 200G (Priority) 15144210 bigmemm NT_plant jgillung PD 0:00 1 24 200G (Priority) This is a list of ALL the jobs currently submitted to Slurm – which usually quite a few! And often we won’t be able to scroll through the list to find our job(s). So, in order to only see your own job(s) we can specify a username: If you don’t know your username, you can find it in a couple of ways: 1. the whoami command: whoami with $USER echo $USER (Note: there are subtle differences between the two. The whoami command displays the effective user id at the time the command is entered. The $USER is an environment variable that is set by the shell–it won’t work on all operating systems but it works great here!) We can use the output of this to see the status of the jobs associated with a particular username: squeue -u $USER JOBID PARTITION NAME USER ST TIME NODES CPU MIN_ME NODELIST(REASON) 15219530 bmh ggg298-a ggg298-4 R 0:28 1 8 10G bm14 Much better!! Not only can you check on your own job’s status but you can also check on the status of your slurm group; here, our slurm group is ctbrowngrp: squeue -A ctbrowngrp You can also check on the status of particular partitions: squeue -p high2 These will show you what resources are being used so you can figure out which are free, sort of. Note: squeue -u username is how I figured out what my NODES and CPU specs were for the srun, above. 10.3.4 Canceling your jobs with scancel To cancel a single job you can specify the JOBID scancel &lt;job_ID&gt; To cancel all of the jobs that belong to you, use the -uflag. scancel -u &lt;username&gt; CHALLENGE: Use srun to set yourself up with a 10 minute session on high2; then, in the session, use squeue -u to find the job ID of your session; then, scancel it. Alternatively, you can cancel ALL your jobs with scancel -u. 10.4 More on resources and queues and sharing 10.4.1 Measuring your resource usage There’s an old joke that you can tell to your kids if you want to teach them to distrust you. Do you know how they figure out what the weight limits are on bridges? They build the bridge, and then they run bigger and bigger trucks over it until the bridge fails. Then they record the weight of the last truck that succeeded in crossing the bridge, and use that as the weight limit for the bridge. It’s funny because it’s not true… …unless you’re doing big compute, in which case it’s basically exactly that. For Reasons, there is no one-size-fits-all 100% reliable way to estimate the resources needed. But you can measure the resources used when a job completes successfully! (For this reason, I suggest that the first time you run a job, you request more resources than you think you’ll need - more time, more memory - and then trim it back after measuring it.) There are two ways to estimate your resources. First, you can use /usr/bin/time -v to measure the time and memory needed to run a command. You’re looking for the following output: ... Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.03 ... Maximum resident set size (kbytes): 2516 ... The first is how much time it took, the second is the max amount of memory (in kb) needed. You can use this to measure the amount of time it takes to run a script. CHALLENGE: Use /usr/bin/time -v to run the HelloWorld.sh script. What resources does it need? The other way is to add the following command to the bottom of your HelloWorld.sh script: sstat –format ‘JobID,MaxRSS,AveCPU’ -P ${SLURM_JOB_ID}.batch which will put the following output in your .out file: JobID|MaxRSS|AveCPU 37971877.batch|952K|00:00.000 OPTIONAL CHALLENGE: Let’s do this to look at the snakemake workflow! Steps: create the sbatch script to run snakemake - see A stock sbatch script that includes activating a conda environment remove .zip and .html submit the script with sbatch …wait… inspect the output file. 10.4.2 Nodes vs CPUs vs tasks You will at some point see slurm docs making distinctions between nodes, tasks, and CPUs per task. My advice for people just beginning to work with slurm is to ignore all of this until you need it, and just use -c or --cpus-per-task, and leave -N and -n set at 1 (which is the default). That having been said… here’s what they mean: Node: A physical box that connects memory systems with extension cards and CPU cores. CPU Core: An independent computing unit that can access a certain number of CPU threads with all the threads having independent input streams but sharing the core’s total memory. tasks: A way to organize tasks that may have multiple CPUs per each task. The -c flag will adjust the number of CPUs per process. Alter this if the job is multithreaded and requires more than one CPU per task to perform optimally. If this option is specified without the -n flag, then as many tasks will be allocated to per node as possible. The -n flag will determine the number of tasks to run. The default Slurm setting is one CPU per task per node but is adjusted when using -c. 10.4.3 Partitions Partitions are the subsets of the cluster (or “partitions of the whole cluster”) that you have access to. I am not an expert on the details, but basically they specify (1) a set of computers and (2) a priority for running things on When you get your account on an HPC, you’ll get a listing of what you have partitions you have access to. Here’s what my group gets on farm. We have access to: low2 - low priority compute nodes med2 - medium priority compute nodes high2 - high priority compute nodes as well as bml - low priority big mem node (up to a TB) bmm - medium priority big mem node bmh - high priority big mem node The way priorities work on farm is as follows: bmh = get a dedicated computer that you purchased, within 1 minute bmm = get more than what you paid for, if there’s any free resources (which is usually), but your job might be suspended if another user asks for nodes on their computer in bmh. bml = get (way) more than you paid for (just like bmm) but your job might be killed and rescheduled. On bmh/bmm/bml, we have one bigmem node, so: 1TB of memory max 96 CPU On high2/med2/low2, we have two parallel nodes, so: 512 GB RAM total, 256 GB max per job 32 cores on one machine, 64 cores on the other machine These will all be different for your account and your compute node. See “What are HPC clusters’ access models?” at the HPC FAQ for how this works at UC Davis. 10.4.4 How to share within your group Here’s the guidance I give my group: Users in ctbrowngrp collectively share resources. Currently, this group has priority access to 1 TB of ram and 96 CPUs on one machine, and 256 GB RAM and up to 64 CPUs on another two machines. The big mem machine is accessed using the big mem partition, bm*, while the smaller-memory machines are accessed on high2/med2/low2. As of February 2020, there are 31 researches who share these resources. To manage and share these resources equitably, we have created a set of rules for resource usage. When submitting jobs, if you submit to a bm* partition, please follow these rules: bmh/high2: use for 1. small-ish interactive testing 2. single-core snakemake jobs that submit other jobs. 3. only if really needed: one job that uses a reasonable amount of resources of “things that I really need to not get bumped.” Things that fall into group 3 might be very long running jobs that would otherwise always be interrupted on bmm/med2 or bml/low2 (e.g. &gt; 5 days), or single jobs that need to be completed in time for a grant or presentation. If your single job on bmh/high2 will exceed 1/3 of the groups resources for either RAM or CPU, please notify the group prior to submitting this job. bmm/med2: don’t submit more than 1/3 of resources at once. This counts for cpu (96 total, so max 32) and ram (1TB total, so max 333 GB). bml/low2: free for all! Go hog wild! Submit to your hearts content! Note that the bmm/bml and med2/low2 queues have access to the full cluster, not just our machines; so if farm is not being highly utilized you may be able to run more jobs faster on those nodes than on bmh/high2. 10.4.5 How can you get an account on your HPC? To figure out what you have access to at UC Davis, see the HPC FAQ, “How do I request access to HPC clusters?” 10.5 What we’ve shown you today. We’ve discussed how a computer cluster differs from a single remote computer: you have far more resources at your fingertips, but you need to deal with the added complexity of slurm (or another scheduler). We’ve shown you how to reserve a node interactively (with srun) or for a script (with sbatch). We’ve given you some tips and tricks for using clusters effectively. We’ve shown you how to track and examine running jobs. And, finally, we’ve given you a few different ways to measure resource use, along with some suggestions on how to be a good person and share nicely. You may never need to use a really big compute cluster to run things. But if you do, you’re at least ready to get started :) 10.6 Some final thoughts before departing farm and moving into the cloud. This is the last time we’ll use farm. &lt;waves goodbye&gt; For our last and final workshop in this series, Making use of on-demand “cloud” computers from Amazon Web Services, we’ll show you how to rent computers from Amazon. In our view, this is a great fallback when you have a burst computing need, or don’t have access to specific resources that you need (like GPUs). Importantly, most of the stuff we’ve learned for remote computing will work just fine no matter what system we use. Shell, text editing, ssh, conda, project organization, shell scripting, version control, and snakemake will all work on any modern UNIX system. …and that’s why we taught them to you :). "],["making-use-of-on-demand-cloud-computers-from-amazon-web-services.html", "11 Making use of on-demand “cloud” computers from Amazon Web Services 11.1 Workshop structure and plan 11.2 Some background 11.3 Amazon, terminology, and logging in! 11.4 Let’s get started! 11.5 Using your computer “in the cloud” 11.6 Configuring your instance differently. 11.7 Shutting down instances 11.8 Exercise 11.9 Checklist of things you learned today! 11.10 FAQs 11.11 Concluding thoughts on the cloud", " 11 Making use of on-demand “cloud” computers from Amazon Web Services This two hour workshop will introduce attendees to AWS computer “instances” that let you rent compute time on large or specialized computers. We’ll create a small general-purpose Linux computer, connect to it, install some software, and explore the computing environment. This lesson is based on materials originally developed by Abhijna Parigi, Marisa Lim, and Saranya Canchi for the CFDE training site. Our goal for this workshop is to help you understand how you might make use of cloud computers for your work. 11.1 Workshop structure and plan Brief introduction to AWS and the cloud Set up an instance and connect to it Install and run things on the cloud computer View optional configuration settings for AWS instances 11.2 Some background What is cloud computing? Renting and use of IT services over the internet. No direct, active management of hardware by the user. Avoid or minimize up-front IT infrastructure costs. Amazon and Google, among others, rent compute resources over the internet for money. Why might you want to use a cloud computer? There are lots of reasons, but basically “you need a kind of compute or network access that you don’t have.” More memory or disk space than you have available otherwise An operating system you don’t have access to (Windows? Mac?) Installation privileges for software May not want to/be able to install specific software on your local computer Use commercial software without buying it Need access to Graphics Processing Units (GPUs) 11.2.1 Costs and payment Today, everything you do will be paid for by us. In the future, if you create your own AWS account, you’ll have to put your own credit card on it. We’d be happy to answer questions about options for paying for AWS administratively. Your free login credentials will work for the next 8 hours ;). 11.3 Amazon, terminology, and logging in! Amazon web services is one of the most broadly adopted cloud platforms It is a hosting provider that gives you a lot of services including cloud storage and cloud compute. Amazon’s main compute rental service is called Elastic Compute Cloud (or EC2) and that’s what we’ll be showing you today. Terminology: Instance - a computer that is running …somewhere…, i.e. in “the cloud”. The important thing is that someone else is worrying about the hardware etc, so you’re just renting what you need! Cloud computer - same as an “instance”. Image - the basic computer install from which an instance is constructed. The configuration of your instance at launch is a copy of the Amazon Machine Image (AMI) that you choose. For more on why EC2 is named the way it is, see Elasticity (cloud computing) 11.3.1 EC2 Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, re-sizable compute capacity in the cloud. Basically, you rent virtual computers that are configured according to your needs and run applications and analyses on that computer. Well suited for analyses that could crash your local computer. E.g. those that generate or use large output files or take too long HIPAA compliant/secure computing is available! 11.3.2 Some features of AWS Sign up process is relatively easy (you need a credit card and some patience to deal with delays in two-factor authentication) Simple billing Stable services with only 3-4 major outages that only lasted 2-3 hours and did not affect all customers (region-specific). A large team of employees who are on top of any problems that arise! Lots of people use it, so there are a ton of resources Spot instances (unused EC2 instances) - you can “bid” for a price based on current demand. It is cheaper, but your instances might be terminated abruptly if demand goes up too high. 11.4 Let’s get started! We will create a cloud computer - an “instance” - and then log in to it. 11.4.1 “Spinning up” instances We’re going do go through the following: Select a region: geographic area where AWS has data centers Pick the AMI (OS) Pick an instance (T2 micro free tier!) Launch 11.4.1.1 Step 1: log in Log in at: https://cfde-ctb.signin.aws.amazon.com/console Use your datalab-XX account (datalab-08) and the password datalab-cfde. Put up a hand on Zoom when you’ve successfully logged in with the workshop user credentials. 11.4.1.2 Step 2: Select region Select the AWS region of your remote machine that is closest to your current geographic location. It is displayed on the top right corner. Click on it and choose a location. In this tutorial, we have selected N.California because that’s where UC Davis is located and so our network connection to it is generally fast. AWS Dashboard A note regarding the “AWS Region”: The default region is automatically displayed in the AWS Dashboard. The choice of region has implications for cost, speed, and performance. 11.4.1.3 Step 3: Choose virtual machine Click on Services (upper left corner): AWS Services Click on EC2: EC2 A note regarding “Amazon EC2”: Amazon Elastic Cloud Computing (Amazon EC2) features virtual computing environments called instances. They have varying combinations of CPU, memory, storage, and networking capacity, and give you the flexibility to choose the appropriate mix of resources for your applications. Click on Launch Instance: Launch Instance 11.4.1.4 Step 4: Choose an Amazon Machine Image (AMI) An Amazon Machine Image provides the template for the cloud computer you’re renting - the base installed operating system and applications. Select AWS Marketplace on the left hand side tab: AWS Marketplace Type Ubuntu Pro in the search bar. Choose Ubuntu Pro 20.04 LTS by clicking Select: AMI Why “Ubuntu 20.04 AMI”? Ubuntu 20.04 was released in 2020 and is the latest version. This is a Long Term Support (LTS) release which means it will be supported with software updates and security fixes. Since it is a Pro version the support will last for ten years until 2030. Click Continue on the popup window: Ubuntu Focal 11.4.1.5 Step 5: Choose an instance type Amazon EC2 provides a wide selection of instance types optimized to fit different use cases. You can consider instances to be similar to the hardware that will run your OS and applications. Learn more about instance types and how they can meet your computing needs. For this tutorial we will select the row with t2.micro: t2.micro t2.micro is “Free Tier Eligible” - what does that mean? The “Free tier eligible” tag lets us know that this particular operating system is covered by the Free Tier program where you can use (limited) services without being charged. Limits are based on how much storage you allocate and/or how many hours of compute you perform in a one month. You can proceed to launch the instance with default configurations by clicking on Review and Launch. 11.4.1.6 Step 6: Review and launch instance The last tab in setup is Review which summarizes all the selected configurations for the instance. Click Launch after review. launch instance 11.4.1.7 Step 6a: SSH Key pair If you are launching an AWS instance for the first time, you will need to generate an ssh key pair. (See Using SSH private/public key pairs from workshop 4!) Choose the Create a new key pair option from the drop down menu. Type your account name under Key pair name, e.g. “datalab-08”. Click Download Key Pair to obtain the .pem file to your local machine. You can access the .pem file from the Downloads folder which is typically the default location for saving files. Next time you launch an instance, you can reuse the key pair you just generated, using the Choose an existing key pair option. **Warning: Do not select Proceed without a key pair option since you will not be able to connect to the instance.* Check the acknowledgement box, and click Launch Instances. pem key Why do I need a key pair? With the SSH protocol, public key authentication improves security as it frees users from remembering complicated passwords and allows automated logins as well. 11.4.1.8 Step 6b: Launch status You will be directed to the Launch Status page where the green colored box on top indicates a successful launch! Click on this first hyperlink, which is the instance ID. Your hyperlink will be different! SSH 11.4.1.9 Step 6c: Get your machine network address The instance console page shows you a list of all your active instances. If you followed the instructions above, you should have only one. Here, you should name your machine by clicking on the empty spot under “Name”. Please name it something like “datalab-XX first machine”. Continue on to the next section to connect to your AWS instance! 11.4.2 Connecting to instance ([The below instructions reprise much of what we did to connect to farm - see Using SSH private/public key pairs) Go back to your instance page, select it and click on “Connect”. The Public DNS information you need to connect to your instance via ssh can be found in the “SSH client” tab: 11.4.2.1 MobaXterm on Windows In MobaXterm, click on “Session” Click on “SSH” Enter the Public DNS as the “Remote host” (the part that looks like ec2-[..].us-west-1.compute.amazonaws.com) Check box next to “Specify username” and enter “ubuntu” as the username Click the “Advanced SSH settings” tab Check box by “Use private key” Use the document icon to navigate to where you saved the private key (e.g., “amazon.pem”) from AWS on your computer. It is likely on your Desktop or Downloads folder Click “OK” 11.4.2.2 MacOS Start Terminal Change the permissions on the .pem file for security purposes (removes read, write, and execute permissions for all users except the owner (you). cd ~/Downloads chmod og-rwx ~/Downloads/datalab-*.pem Go back to your instance page, select it and click on “Connect”. The information you need to connect to your instance via ssh can be found in the “SSH client” tab: 11.5 Using your computer “in the cloud” At this point, we can use most of the command we learned in the previous workshops! 11.5.1 Inspecting your computer See how much disk space you have in your home directory: cd ~/ df -h . See how much memory you have access to: free Look at your available CPUs: cat /proc/cpuinfo 11.5.2 You can do all the UNIX things Let’s start by taking a look at some of our friendly old data: cd ~/ git clone https://github.com/ngs-docs/2021-remote-computing-binder/ and now you run grep, gunzip, cut, and head as usual – cd ~/2021-remote-computing-binder/SouthParkData/ gunzip All-seasons.csv.gz head All-seasons.csv cut -d, -f3 All-seasons.csv | grep Computer | sort | uniq -c As before, you can’t run csvtk, though, because that’s not a commonly installed UNIX command - csvtk cut -f Character All-seasons.csv | grep Computer | sort | uniq -c Also, note that the nano and vi editors are installed by default, but not emacs. 11.5.3 Install conda Let’s install conda! Go to the miniconda download page and copy the URL for Python 3.9, Miniconda3 Linux 64-bit. Then, run: cd ~/ curl -O https://repo.anaconda.com/miniconda/Miniconda3-py39_4.10.3-Linux-x86_64.sh Now, following the instructions for Linux install, do: bash ~/Miniconda3-py39_4.10.3-Linux-x86_64.sh and answer “yes” to accept the license, and “yes” to have the installer initialize Miniconda. Then, reload your .bashrc: source ~/.bashrc and you should now be at a prompt that includes (base), e.g. (base) ubuntu@ip-172-30-2-92:~$. We also need to add some channels (see Installing conda) - conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge and then let’s install mamba, a faster version of the ‘conda’ command. conda install -y mamba 11.5.4 Run a snakemake workflow Let’s reprise Automating your analyses with the snakemake workflow system. First, let’s check out a git repository that contains our snakemake workflow: cd ~/ git clone https://github.com/ngs-docs/2021-remote-computing-snakemake Then, create a conda environment with the necessary software: mamba create -n snakemake -y snakemake-minimal fastqc salmon conda activate snakemake Change our working directory to the git repo: cd ~/2021-remote-computing-snakemake/ Run a shell script to download the raw data: ./download.sh …and finally, run snakemake! snakemake -j 1 Tada! You’ll have all your output files etc. (Note that you could perfectly well have run this inside of a screen session - see Persistent sessions with screen and tmux.) 11.5.5 Summing things up, round 1 This highlights many of the things that we’ve taught you in this workshop series: logging into a remote computer - in this case, an amazon instance downloading files directly to a remote computer installing software with conda cloning a git repository containing text files using a shell script to automate some tasks using snakemake to orchestrate a workflow and it’s worth noting that essentially everything we did works equally well on a local Linux laptop, a remote HPC, and a remote cloud computer. In this case, this is a private computer that only you have access to, so you don’t need to worry about file permissions or using a queue to run workflows. 11.6 Configuring your instance differently. There are several optional set up configurations. Let’s explore them! Go back to your instance page and let’s set up a new computer and explore some of the options that are available to you. Click launch and then: Pick the AMI (OS) - Ubuntu Pro 20.04 LTS Pick an instance (T2 micro free tier!) and now click Next: Configure Instance Details on the AWS page. Configure the instance to suit your requirements. You can: change number of instances to launch select the subnet to use modify Stop or Terminate behaviors control if you would like the instance to update with any patches when in use request Spot Instances A note on “Spot Instance”: A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. “Configure Storage” Your instance comes with a in built storage called instance store and is useful for temporary data storage. The default root volume on a t2.micro is 8 GB. For data you might want to retain longer or use across multiple instances or encrypt it is best to use the Amazon Elastic Block Store volumes (Amazon EBS). Attaching EBS volumes to an instance are similar to using external hard drives connected to a computer. Click on Add New Volume for additional storage. You can get up to 30 GB of EBS general purpose (SSD) or Magnetic storage when using Free tier instances. “Add Tags” Tags are useful to categorize your AWS resources when you have many of them. A tag consists of a case-sensitive key-value pair. Some examples: GTEx-RNAseq, General-GWAS, KF-GWAS. Learn more about tagging your Amazon EC2 resources. “Configure Security Group” Similar to setting up a firewall through which we would modify connection of external world and the EC2 instance. Blocks/allow connections based on port number and IP. You can create a new security group or select from an existing one. Learn more about Security groups for EC2 instances. 11.7 Shutting down instances When you shut down your instance, any data that is on a non-persistent disk goes away permanently. But you also stop being charged for any compute and data, too! Stopping vs hibernation vs termination Stopping: saves data to your disk (the EBS root volume ) only EBS data storage charges apply No data transfer charges or instance usage charges RAM contents not stored Hibernation: charged for storage of any EBS volumes stores the RAM contents it’s like closing the lid of your laptop Termination: complete shutdown separate disks are detached data stored in EBS root volume is lost forever instance cannot be re-launched To enable Hibernation, click the box in the Configure Instance step of the setup. 11.8 Exercise Launch a t2.nano, Ubuntu 20.04 LTS - Focal instance in the the East US (Ohio) region. Change the root storage volume to 16 GiB and add an additional EBS volume (8 GiB). Bonus points: Your added volume will persist after you have terminated your instance. Where can you find it? Hints: Go to Amazon Market place and search for the “Ubuntu 20.04 LTS - Focal”. Should be the first result. Look in tab 4 called “Add Storage” to add additional storage volumes. 11.9 Checklist of things you learned today! A little bit about AWS and cloud computing How to launch an instance How to connect to the instance How to install and run a software program on the instance How to terminate your instance Reminder: Terminate all your instances! 11.9.1 Additional Resources Useful tips: https://wblinks.com/notes/aws-tips-i-wish-id-known-before-i-started/ Consolidated billing: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html 11.10 FAQs 11.10.1 What are my data transfer costs? AWS and other cloud provides typically charge for data transfer from their network to the external Internet (e.g. your home computer). Costs are highly dependent on the region. For example, for S3 buckets located in the US West (Oregon) region, the first GB/month is free and the next 9.999 TB/month cost $0.09 per GB. However, if the S3 buckets are located in the South America (São Paolo) region, the first GB/month is still free, but the next 9.999 TB/month cost $0.25 per GB. More info here: https://www.apptio.com/blog/aws-data-transfer-costs/ 11.10.2 What are data storage costs? See https://aws.amazon.com/ebs/pricing/ 11.10.3 What are the advantages of using AWS over an academic HPC? (See Executing large analyses on HPC clusters with slurm from Workshop 10 for working with HPCs) Some universities don’t have a HPC No queues! No waiting! Can set up as many instances as you want (as long as you are willing to pay for them) Can install anything without needing admin permissions Almost no scheduled or unscheduled outages Easier to set up Easier to learn and get help on the internet Costs more over time, but someone is paying for the HPC too! But if you have a good HPC, it is often cheaper. 11.10.4 Can you set up multiple instances at once Yes! There is a limit per account but it is a very large number and won’t apply to most people. (The limit is there to keep you from spending a lot of money by accident.) 11.10.5 Can you launch more than one instance with the same configurations? Yes, there is an option to do this on the instance set up page. Look in the second tab! 11.10.6 Can you copy an instance or share an instance with collaborators? Yes, but this is not as straightforward as it seems. The way to clone an instance is via snapshots Check out our AWS discussion board for FAQs and discussion. We encourage you to post questions there! 11.11 Concluding thoughts on the cloud If your laptop can run your analysis, there’s no need to use the cloud. Binder is a free, configurable option that runs in the cloud (remember binder, from Introduction to the UNIX Command Line? :) Sometimes labs have workstations, and you can use those, too! If you have an HPC account, you can try using that. Cloud computing is most useful when you don’t have an investment in an existing computer, and you have a sudden need to do a bunch of compute. It can also be a way to briefly expand your compute options. Last but by no means least, cloud computers often have fast and cheap access to VERY large data sets; this is one reason why the NIH is so interested in data reuse via cloud computing. AWS and GCP are commercial cloud options. If you prefer to write mini grant applications, NSF XSEDE will give you AWS-like computers via Jetstream, and HPC-like computers via PSC Blacklight and others. (Ask us for more information on these!) The key thing is that everything we showed you works almost equally well independent of where you’re computing. The only differences are when you start needing to cooperate with others, via e.g. Slurm queueing. "],["appendix.html", "Appendix Previous offerings Workshop Protocol", " Appendix Previous offerings Videos from August, 2021 The inaugural offering of this material was in August 2021, co-hosted by the UC Davis DataLab and the NIH Common Fund Data Ecosystem. The material from that workshop is hosted here, and all the sessions were recorded; videos below! Video: Introduction to the UNIX Command Line Video: Creating and modifying text files on remote computers Video: Connecting to remote computers with ssh Video: Running programs on remote computers and retrieving the results Video: Installing software on remote computers with conda Video: Structuring your projects for current and future you Video: Automating your analyses and executing long-running analyses on remote computers Video: Keeping track of your files with version control Video: Automating your analyses with the snakemake workflow system Video: Executing large analyses on HPC clusters with slurm Video: Making use of on-demand “cloud” computers from Amazon Web Services Workshop Protocol Before workshop Instructor(s) create workshop notes on GitHub as an R markdown file Instructor(s) create/test computing environment (i.e., binder, Farm HPC) for workshop material Training coordinator creates pre- and post-workshop assessment surveys, with input from instructor(s) Training coordinator emails notes, pre-workshop survey, and Zoom link to participants the day before the workshop During workshop Moderator begins by asking everyone to fill out the pre-workshop survey if they haven’t already, and to put up a raised hand Zoom reaction when they’ve completed the survey. Then, describe ways to ask for help during the workshop: 1) type in Zoom chat to everyone or as a direct message to moderator/helpers (introduce helpers), 2) unmute and ask; say that we prefer participants don’t use the raised hand reaction to ask a question, since that’s used for checking in. At this point, there should be a majority of raised hands up for the survey question. Turn over the mic to the instructor. Moderator will keep track of chat questions, time (including a 5-minute break ~10:15 or 10:30am), and help share workshop resource links (i.e., section of workshop notes, exercises, post-workshop survey) Helpers aide in tracking chat questions and participation during check ins (i.e., direct message participants who have not raised their hands to make sure they’re still following along) Training team writes down typos, suggestions, etc. for the workshop and notes in the associated GitHub issue After workshop Training team updates workshop notes as needed Training coordinator emails notes, workshop recording, and post-workshop survey link to participants some time after the workshop "]]
